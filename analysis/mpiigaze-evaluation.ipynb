{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPIIGaze Dataset Evaluation\n",
    "\n",
    "This notebook evaluates the existing eye-tracking model on the MPIIGaze dataset with error output in degrees of visual angle for direct comparison with research literature.\n",
    "\n",
    "## Dataset Overview\n",
    "- **MPIIGaze**: 213,659 images from 15 participants during natural laptop use\n",
    "- **Evaluation**: Leave-one-person-out cross-validation\n",
    "- **Metric**: Mean angular error in degrees\n",
    "- **Benchmarks**: State-of-the-art ranges from 4.3° to 10.8° depending on evaluation protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and Extract MPIIGaze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "data_dir = Path(\"./mpiigaze_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download URL\n",
    "dataset_url = \"http://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz\"\n",
    "dataset_file = data_dir / \"MPIIGaze.tar.gz\"\n",
    "extract_dir = data_dir / \"MPIIGaze\"\n",
    "\n",
    "print(f\"Downloading MPIIGaze dataset from {dataset_url}\")\n",
    "print(f\"File size: ~2.1 GB - this may take a few minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset if not already present\n",
    "if not dataset_file.exists():\n",
    "    print(\"Downloading MPIIGaze dataset...\")\n",
    "    urllib.request.urlretrieve(dataset_url, dataset_file)\n",
    "    print(f\"Download complete: {dataset_file}\")\n",
    "else:\n",
    "    print(f\"Dataset already downloaded: {dataset_file}\")\n",
    "\n",
    "# Extract the dataset if not already extracted\n",
    "if not extract_dir.exists():\n",
    "    print(\"Extracting dataset...\")\n",
    "    with tarfile.open(dataset_file, 'r:gz') as tar:\n",
    "        tar.extractall(data_dir)\n",
    "    print(f\"Extraction complete: {extract_dir}\")\n",
    "else:\n",
    "    print(f\"Dataset already extracted: {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "for root, dirs, files in os.walk(extract_dir):\n",
    "    level = root.replace(str(extract_dir), '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Limit to first 5 files per directory\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{subindent}... and {len(files) - 5} more files\")\n",
    "    if level > 3:  # Limit depth for readability\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore MPIIGaze Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for evaluation subset and data structure\n",
    "evaluation_dir = extract_dir / \"Evaluation Subset\"\n",
    "if evaluation_dir.exists():\n",
    "    print(f\"Found evaluation subset at: {evaluation_dir}\")\n",
    "    \n",
    "    # List subjects in evaluation subset\n",
    "    subjects = [d.name for d in evaluation_dir.iterdir() if d.is_dir()]\n",
    "    subjects.sort()\n",
    "    print(f\"Found {len(subjects)} subjects: {subjects}\")\n",
    "else:\n",
    "    print(\"Evaluation subset not found, looking for main data directory...\")\n",
    "    # Check for alternative structure\n",
    "    for item in extract_dir.iterdir():\n",
    "        print(f\"Found: {item.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a single subject's data structure\n",
    "if evaluation_dir.exists() and subjects:\n",
    "    sample_subject = subjects[0]\n",
    "    subject_dir = evaluation_dir / sample_subject\n",
    "    \n",
    "    print(f\"Examining subject {sample_subject} structure:\")\n",
    "    for item in subject_dir.iterdir():\n",
    "        print(f\"  {item.name}\")\n",
    "        if item.is_dir():\n",
    "            # Look inside subdirectories\n",
    "            files = list(item.iterdir())\n",
    "            print(f\"    Contains {len(files)} files\")\n",
    "            if files:\n",
    "                print(f\"    Sample files: {[f.name for f in files[:3]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine sample data files\n",
    "def explore_subject_data(subject_id):\n",
    "    \"\"\"\n",
    "    Explore the data structure for a given subject\n",
    "    \"\"\"\n",
    "    subject_dir = evaluation_dir / subject_id\n",
    "    \n",
    "    # Look for different data types\n",
    "    data_types = {}\n",
    "    \n",
    "    for item in subject_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            files = list(item.iterdir())\n",
    "            data_types[item.name] = len(files)\n",
    "        elif item.suffix in ['.mat', '.json', '.txt']:\n",
    "            data_types[item.name] = \"metadata\"\n",
    "    \n",
    "    return data_types\n",
    "\n",
    "if evaluation_dir.exists() and subjects:\n",
    "    sample_data = explore_subject_data(subjects[0])\n",
    "    print(f\"Data structure for {subjects[0]}:\")\n",
    "    for key, value in sample_data.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Sample Images and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load MPIIGaze data for a subject\n",
    "def load_subject_data(subject_id, data_type=\"left\"):\n",
    "    \"\"\"\n",
    "    Load data for a specific subject\n",
    "    data_type: 'left', 'right', or 'normalized' (if available)\n",
    "    \"\"\"\n",
    "    subject_dir = evaluation_dir / subject_id\n",
    "    \n",
    "    # Try to find image data directory\n",
    "    possible_dirs = [\"left\", \"right\", \"normalized\", \"images\", \"data\"]\n",
    "    image_dir = None\n",
    "    \n",
    "    for dirname in possible_dirs:\n",
    "        candidate_dir = subject_dir / dirname\n",
    "        if candidate_dir.exists():\n",
    "            image_dir = candidate_dir\n",
    "            print(f\"Found image directory: {dirname}\")\n",
    "            break\n",
    "    \n",
    "    if image_dir is None:\n",
    "        print(f\"Available directories in {subject_dir}:\")\n",
    "        for item in subject_dir.iterdir():\n",
    "            print(f\"  {item.name}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load images\n",
    "    image_files = sorted([f for f in image_dir.iterdir() if f.suffix in ['.png', '.jpg', '.jpeg']])\n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "    \n",
    "    # Look for annotation files\n",
    "    annotation_files = [f for f in subject_dir.iterdir() if f.suffix in ['.mat', '.json', '.txt']]\n",
    "    print(f\"Found annotation files: {[f.name for f in annotation_files]}\")\n",
    "    \n",
    "    return image_files, annotation_files\n",
    "\n",
    "# Test with first subject\n",
    "if evaluation_dir.exists() and subjects:\n",
    "    sample_images, sample_annotations = load_subject_data(subjects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display sample images\n",
    "if sample_images:\n",
    "    # Load first few images\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_path in enumerate(sample_images[:6]):\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"{img_path.name}\\nShape: {img.shape}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Sample images from {subjects[0]}\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Image shape: {img.shape}\")\n",
    "    print(f\"Image dtype: {img.dtype}\")\n",
    "    print(f\"Image range: {img.min()} to {img.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine annotation data\n",
    "def load_annotations(subject_id):\n",
    "    \"\"\"\n",
    "    Load annotation data for a subject\n",
    "    \"\"\"\n",
    "    subject_dir = evaluation_dir / subject_id\n",
    "    \n",
    "    # Look for .mat files (common format for MPIIGaze)\n",
    "    mat_files = [f for f in subject_dir.iterdir() if f.suffix == '.mat']\n",
    "    \n",
    "    annotations = {}\n",
    "    for mat_file in mat_files:\n",
    "        try:\n",
    "            data = loadmat(str(mat_file))\n",
    "            # Filter out MATLAB metadata\n",
    "            clean_data = {k: v for k, v in data.items() if not k.startswith('__')}\n",
    "            annotations[mat_file.name] = clean_data\n",
    "            print(f\"Loaded {mat_file.name} with keys: {list(clean_data.keys())}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {mat_file.name}: {e}\")\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "if evaluation_dir.exists() and subjects:\n",
    "    sample_annotations = load_annotations(subjects[0])\n",
    "    \n",
    "    # Examine the structure of annotations\n",
    "    for filename, data in sample_annotations.items():\n",
    "        print(f\"\\n{filename}:\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                print(f\"  {key}: shape {value.shape}, dtype {value.dtype}\")\n",
    "                if value.size < 20:  # Print small arrays\n",
    "                    print(f\"    {value}\")\n",
    "                else:\n",
    "                    print(f\"    Sample values: {value.flat[:5]}...\")\n",
    "            else:\n",
    "                print(f\"  {key}: {type(value)} - {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what we need from the original analysis notebook\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "try:\n",
    "    # Try to import the et_util modules\n",
    "    import et_util.dataset_utils as dataset_utils\n",
    "    import et_util.embedding_preprocessing as embed_pre\n",
    "    import et_util.model_layers as model_layers\n",
    "    from et_util import experiment_utils\n",
    "    from et_util.custom_loss import normalized_weighted_euc_dist\n",
    "    print(\"Successfully imported et_util modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import et_util modules: {e}\")\n",
    "    print(\"Will define required components locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture components locally if needed\n",
    "import keras\n",
    "from keras import ops\n",
    "import keras_hub\n",
    "\n",
    "# Configuration from the original model\n",
    "MAX_TARGETS = 288\n",
    "EMBEDDING_DIM = 200  # This should match the original model\n",
    "RIDGE_REGULARIZATION = 0.001  # This should match the original model\n",
    "BACKBONE = \"densenet\"  # This should match the original model\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Embedding dim: {EMBEDDING_DIM}\")\n",
    "print(f\"  Ridge regularization: {RIDGE_REGULARIZATION}\")\n",
    "print(f\"  Backbone: {BACKBONE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model weights file exists\n",
    "model_weights_path = \"./full_model.weights.h5\"\n",
    "if os.path.exists(model_weights_path):\n",
    "    print(f\"Found model weights at: {model_weights_path}\")\n",
    "else:\n",
    "    print(f\"Model weights not found at: {model_weights_path}\")\n",
    "    print(\"Looking for weights file in current directory...\")\n",
    "    for file in os.listdir(\".\"):\n",
    "        if \"weight\" in file.lower() or file.endswith(\".h5\"):\n",
    "            print(f\"  Found: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status Update\n",
    "\n",
    "At this point, we have:\n",
    "1. ✅ Set up the notebook structure\n",
    "2. ✅ Created code to download the MPIIGaze dataset\n",
    "3. ✅ Started exploring the data format\n",
    "\n",
    "Next steps will depend on the actual dataset structure once downloaded. The notebook is ready to:\n",
    "- Complete the dataset exploration\n",
    "- Adapt the existing model architecture\n",
    "- Implement the evaluation pipeline\n",
    "- Calculate visual angle errors\n",
    "\n",
    "The user can now run this notebook to begin the evaluation process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
