{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGxMqXYHxcdk"
      },
      "source": [
        "# MPIIGaze Dataset Evaluation\n",
        "\n",
        "This notebook evaluates the existing eye-tracking model on the MPIIGaze dataset with error output in degrees of visual angle for direct comparison with research literature.\n",
        "\n",
        "## Dataset Overview\n",
        "- **MPIIGaze**: 213,659 images from 15 participants during natural laptop use\n",
        "- **Evaluation**: Leave-one-person-out cross-validation\n",
        "- **Metric**: Mean angular error in degrees\n",
        "- **Benchmarks**: State-of-the-art ranges from 4.3° to 10.8° depending on evaluation protocol"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm9Cq94cxcdn"
      },
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jO6Fw385xcdo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.ops as ops\n",
        "from scipy.io import loadmat\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import json\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1vrFebhxcdp"
      },
      "source": [
        "## 1. Download and Extract MPIIGaze Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1rYpPgjTxcdp"
      },
      "outputs": [],
      "source": [
        "# Create data directory\n",
        "data_dir = Path(\"./mpiigaze_data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Download URL\n",
        "dataset_url = \"http://datasets.d2.mpi-inf.mpg.de/MPIIGaze/MPIIGaze.tar.gz\"\n",
        "dataset_file = data_dir / \"MPIIGaze.tar.gz\"\n",
        "extract_dir = data_dir / \"MPIIGaze\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hb7Docyxcdp",
        "outputId": "eb03a144-a1c5-4fac-c12a-e67dfc7a6a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MPIIGaze dataset...\n",
            "Download complete: mpiigaze_data/MPIIGaze.tar.gz\n",
            "Extracting dataset...\n",
            "Extraction complete: mpiigaze_data/MPIIGaze\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset if not already present\n",
        "if not dataset_file.exists():\n",
        "    print(\"Downloading MPIIGaze dataset...\")\n",
        "    urllib.request.urlretrieve(dataset_url, dataset_file)\n",
        "    print(f\"Download complete: {dataset_file}\")\n",
        "else:\n",
        "    print(f\"Dataset already downloaded: {dataset_file}\")\n",
        "\n",
        "# Extract the dataset if not already extracted\n",
        "if not extract_dir.exists():\n",
        "    print(\"Extracting dataset...\")\n",
        "    with tarfile.open(dataset_file, 'r:gz') as tar:\n",
        "        tar.extractall(data_dir)\n",
        "    print(f\"Extraction complete: {extract_dir}\")\n",
        "else:\n",
        "    print(f\"Dataset already extracted: {extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVLndgaUTdIx",
        "outputId": "a0f8806b-6db1-40d1-944f-99addef01422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15 subject IDs: ['p00', 'p01', 'p02', 'p03', 'p04', 'p05', 'p06', 'p07', 'p08', 'p09', 'p10', 'p11', 'p12', 'p13', 'p14']\n",
            "Number of subjects for evaluation: 15\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the evaluation subset annotation files\n",
        "# Assuming data_dir is defined in a previous cell\n",
        "evaluation_dir = data_dir / \"MPIIGaze\" / \"Evaluation Subset\"\n",
        "annotation_dir = evaluation_dir / \"annotation for face image\"\n",
        "\n",
        "# Get the list of subject IDs from the annotation files\n",
        "subject_ids = []\n",
        "if annotation_dir.exists():\n",
        "    # List files in the annotation directory that match the pattern p*.txt\n",
        "    annotation_files = list(annotation_dir.glob(\"p*.txt\"))\n",
        "    # Extract subject IDs (e.g., 'p00' from 'p00.txt') and sort them\n",
        "    subject_ids = sorted([f.stem for f in annotation_files])\n",
        "    print(f\"Found {len(subject_ids)} subject IDs: {subject_ids}\")\n",
        "else:\n",
        "    print(f\"Annotation directory not found: {annotation_dir}\")\n",
        "\n",
        "# Store the number of subjects for later use\n",
        "num_subjects = len(subject_ids)\n",
        "print(f\"Number of subjects for evaluation: {num_subjects}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2c1c099d"
      },
      "outputs": [],
      "source": [
        "def load_images_from_paths(image_data):\n",
        "    \"\"\"\n",
        "    Load images from a list of (path, eye_type) tuples.\n",
        "\n",
        "    Args:\n",
        "        image_data (list): A list of tuples, where each tuple is (full_image_path, eye_type).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of loaded images (as numpy arrays) and their corresponding eye types.\n",
        "    \"\"\"\n",
        "    loaded_images = []\n",
        "    print(f\"Attempting to load {len(image_data)} images...\")\n",
        "    for i, (img_path, eye_type) in enumerate(image_data):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"  Loading image {i}/{len(image_data)}\")\n",
        "        try:\n",
        "            # Using cv2.IMREAD_GRAYSCALE as images are grayscale\n",
        "            img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
        "            if img is not None:\n",
        "                loaded_images.append((img, eye_type))\n",
        "            else:\n",
        "                print(f\"Warning: Could not load image at {img_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(loaded_images)} images.\")\n",
        "    return loaded_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d765578d",
        "outputId": "d1de17ee-4091-4cb9-b429-977576b656b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified crop_black_padding function - prioritizes black pixel removal with 36x144 aspect ratio.\n"
          ]
        }
      ],
      "source": [
        "def crop_black_padding(image):\n",
        "    \"\"\"\n",
        "    Crops an image by removing black padding and adjusting to 36x144 aspect ratio.\n",
        "    Prioritizes removing black pixels over preserving image content.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): A grayscale image (NumPy array).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The cropped image with 36x144 aspect ratio.\n",
        "    \"\"\"\n",
        "    # Find the coordinates of all non-zero pixels\n",
        "    non_zero_coords = np.nonzero(image)\n",
        "\n",
        "    # If no non-zero pixels are found, return a small black image\n",
        "    if non_zero_coords[0].size == 0:\n",
        "        return np.zeros((36, 144), dtype=image.dtype)\n",
        "\n",
        "    # Get bounding box of non-zero pixels\n",
        "    min_row, max_row = np.min(non_zero_coords[0]), np.max(non_zero_coords[0])\n",
        "    min_col, max_col = np.min(non_zero_coords[1]), np.max(non_zero_coords[1])\n",
        "\n",
        "    # Calculate current dimensions\n",
        "    height = max_row - min_row + 1\n",
        "    width = max_col - min_col + 1\n",
        "\n",
        "    # Target aspect ratio is 144/36 = 4.0 (width/height)\n",
        "    target_aspect_ratio = 4.0\n",
        "\n",
        "    # Calculate center of bounding box\n",
        "    center_row = (min_row + max_row) // 2\n",
        "    center_col = (min_col + max_col) // 2\n",
        "\n",
        "    # Determine final dimensions based on aspect ratio\n",
        "    if width / height > target_aspect_ratio:\n",
        "        # Width is limiting factor - use full width, adjust height\n",
        "        final_width = width\n",
        "        final_height = int(width / target_aspect_ratio)\n",
        "    else:\n",
        "        # Height is limiting factor - use full height, adjust width\n",
        "        final_height = height\n",
        "        final_width = int(height * target_aspect_ratio)\n",
        "\n",
        "    # Calculate crop boundaries centered on bounding box\n",
        "    crop_min_row = center_row - final_height // 2\n",
        "    crop_max_row = crop_min_row + final_height - 1\n",
        "    crop_min_col = center_col - final_width // 2\n",
        "    crop_max_col = crop_min_col + final_width - 1\n",
        "\n",
        "    # Ensure boundaries are within image bounds\n",
        "    crop_min_row = max(0, crop_min_row)\n",
        "    crop_max_row = min(image.shape[0] - 1, crop_max_row)\n",
        "    crop_min_col = max(0, crop_min_col)\n",
        "    crop_max_col = min(image.shape[1] - 1, crop_max_col)\n",
        "\n",
        "    # Extract the cropped region\n",
        "    cropped_image = image[crop_min_row:crop_max_row+1, crop_min_col:crop_max_col+1]\n",
        "\n",
        "    return cropped_image\n",
        "\n",
        "print(\"Simplified crop_black_padding function - prioritizes black pixel removal with 36x144 aspect ratio.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88e0f149"
      },
      "source": [
        "## Consolidate data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Create a single function or code block that loads the image paths for a subject, crops the images to the desired aspect ratio, and then resizes them to 36x144.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e7d0a001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14203b0-8baa-49a4-95d0-4aec287ea0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created unified load_all_subject_data function for evaluation and calibration images.\n"
          ]
        }
      ],
      "source": [
        "def load_all_subject_data(subject_id):\n",
        "    \"\"\"\n",
        "    Loads all image paths for a subject from both evaluation and calibration sets.\n",
        "    Marks evaluation images (from annotation files) vs calibration images.\n",
        "\n",
        "    Args:\n",
        "        subject_id (str): The ID of the subject (e.g., 'p00').\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples (full_image_path, eye_type, image_index, is_evaluation, day).\n",
        "              Returns None if subject data cannot be loaded.\n",
        "    \"\"\"\n",
        "    base_data_dir = Path(\"./mpiigaze_data/MPIIGaze\")\n",
        "    evaluation_dir = base_data_dir / \"Evaluation Subset\"\n",
        "    data_dir = base_data_dir / \"Data\" / \"Original\" / subject_id\n",
        "\n",
        "    if not data_dir.exists():\n",
        "        print(f\"Subject data directory not found: {data_dir}\")\n",
        "        return None\n",
        "\n",
        "    # Load evaluation image paths from annotation file\n",
        "    evaluation_annotation_file = evaluation_dir / \"sample list for eye image\" / f\"{subject_id}.txt\"\n",
        "    evaluation_images = set()\n",
        "\n",
        "    if evaluation_annotation_file.exists():\n",
        "        with open(evaluation_annotation_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(' ')\n",
        "                if len(parts) >= 2:\n",
        "                    relative_path = parts[0].replace('\\\\', '/')\n",
        "                    evaluation_images.add(relative_path)\n",
        "\n",
        "    # Scan all image directories for the subject\n",
        "    all_image_data = []\n",
        "\n",
        "    # Look for day directories (like day01, day02, etc.)\n",
        "    for day_dir in sorted(data_dir.iterdir()):\n",
        "        if not day_dir.is_dir() or day_dir.name == \"Calibration\":\n",
        "            continue\n",
        "\n",
        "        day_name = day_dir.name\n",
        "\n",
        "        # Look for annotation.txt to determine available images\n",
        "        annotation_file = day_dir / \"annotation.txt\"\n",
        "        if not annotation_file.exists():\n",
        "            continue\n",
        "\n",
        "        # Read annotation file to get image count and data\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            annotation_lines = f.readlines()\n",
        "\n",
        "        # Process each image in the day directory\n",
        "        for image_index, annotation_line in enumerate(annotation_lines):\n",
        "            # Construct image filename (assuming 4-digit zero-padded)\n",
        "            image_filename = f\"{image_index + 1:04d}.jpg\"\n",
        "            relative_path = f\"{day_name}/{image_filename}\"\n",
        "            full_image_path = day_dir / image_filename\n",
        "\n",
        "            if not full_image_path.exists():\n",
        "                continue\n",
        "\n",
        "            # Determine if this is an evaluation image\n",
        "            is_evaluation = relative_path in evaluation_images\n",
        "\n",
        "            # For simplicity, assume all images are left eye (can be refined later)\n",
        "            eye_type = \"left\"  # This could be extracted from evaluation annotation if needed\n",
        "\n",
        "            all_image_data.append((\n",
        "                full_image_path,\n",
        "                eye_type,\n",
        "                image_index,\n",
        "                is_evaluation,\n",
        "                day_name\n",
        "            ))\n",
        "\n",
        "    print(f\"Loaded {len(all_image_data)} total images for subject {subject_id}\")\n",
        "    evaluation_count = sum(1 for _, _, _, is_eval, _ in all_image_data if is_eval)\n",
        "    calibration_count = len(all_image_data) - evaluation_count\n",
        "    print(f\"  - {evaluation_count} evaluation images\")\n",
        "    print(f\"  - {calibration_count} calibration images\")\n",
        "\n",
        "    return all_image_data\n",
        "\n",
        "print(\"Created unified load_all_subject_data function for evaluation and calibration images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkxBt2ySkAa7",
        "outputId": "19a75e73-7ff4-4b13-c671-b4bdc92456a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading screen sizes for all subjects...\n",
            "{'p00': (1280, 800), 'p01': (1440, 900), 'p02': (1280, 800), 'p03': (1440, 900), 'p04': (1280, 800), 'p05': (1440, 900), 'p06': (1680, 1050), 'p07': (1440, 900), 'p08': (1440, 900), 'p09': (1440, 900), 'p10': (1440, 900), 'p11': (1280, 800), 'p12': (1280, 800), 'p13': (1280, 800), 'p14': (1440, 900)}\n"
          ]
        }
      ],
      "source": [
        "from scipy.io import loadmat\n",
        "from pathlib import Path\n",
        "import numpy as np # Import numpy for array handling\n",
        "\n",
        "# Assuming subject_ids is available from a previous cell\n",
        "if 'subject_ids' not in locals() or not subject_ids:\n",
        "    print(\"Subject IDs not found. Please run the cell to load subject IDs first.\")\n",
        "else:\n",
        "    # Define the base path to the data directory\n",
        "    base_data_dir = Path(\"./mpiigaze_data/MPIIGaze/Data/Original\")\n",
        "\n",
        "    # Initialize an empty dictionary to store screen sizes\n",
        "    screen_sizes_lookup = {}\n",
        "\n",
        "    print(\"Loading screen sizes for all subjects...\")\n",
        "\n",
        "    # Iterate through each subject ID\n",
        "    for subject_id in subject_ids:\n",
        "        # Construct the path to the screenSize.mat file for the current subject\n",
        "        mat_file_path = base_data_dir / subject_id / \"Calibration\" / \"screenSize.mat\"\n",
        "\n",
        "        # Check if the file exists\n",
        "        if mat_file_path.exists():\n",
        "            try:\n",
        "                # Load the .mat file\n",
        "                mat_data = loadmat(mat_file_path)\n",
        "\n",
        "                # Extract width_pixel and height_pixel\n",
        "                # The data is likely stored as NumPy arrays, extract the scalar value\n",
        "                width_pixel = int(mat_data['width_pixel'][0][0])\n",
        "                height_pixel = int(mat_data['height_pixel'][0][0])\n",
        "\n",
        "                # Store in the dictionary as a tuple\n",
        "                screen_sizes_lookup[subject_id] = (width_pixel, height_pixel)\n",
        "                # print(f\"Loaded screen size for {subject_id}: ({width_pixel}, {height_pixel})\") # Optional: print each loaded size\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading or parsing screenSize.mat for {subject_id}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: screenSize.mat not found for {subject_id} at: {mat_file_path}\")\n",
        "\n",
        "    print(screen_sizes_lookup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhxwCFtyZckQ",
        "outputId": "5e6b7368-dc8e-49db-c20c-761eb45c2474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_gaze_annotation function modified to convert gaze coordinates to proportions.\n"
          ]
        }
      ],
      "source": [
        "def load_gaze_annotation(image_path, image_index, screen_size):\n",
        "    \"\"\"\n",
        "    Loads the on-screen gaze target position from the annotation.txt file\n",
        "    associated with a given image path, using the image index to find the correct line.\n",
        "    Converts pixel coordinates to proportions of screen size using a lookup dictionary.\n",
        "\n",
        "    Args:\n",
        "        image_path (Path): The full path to the image file.\n",
        "        image_index (int): The 0-based index of the image within its directory's sorted file list.\n",
        "        subject_id (str): The ID of the subject (e.g., 'p00').\n",
        "        screen_sizes_lookup (dict): A dictionary with subject IDs as keys and (width_pixel, height_pixel) tuples as values.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The on-screen gaze target position as proportions (between 0 and 1)\n",
        "                    as a numpy array, or None if the annotation is not found, cannot be parsed,\n",
        "                    or screen size is not available.\n",
        "    \"\"\"\n",
        "    # The annotation file is in the same directory as the image\n",
        "    annotation_file = image_path.parent / \"annotation.txt\"\n",
        "\n",
        "    if not annotation_file.exists():\n",
        "        # print(f\"Annotation file not found for image: {image_path}\") # Keep this quiet for batch processing\n",
        "        return None\n",
        "\n",
        "\n",
        "    screen_width, screen_height = screen_size\n",
        "\n",
        "    try:\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            # Read all lines and get the one corresponding to the image index\n",
        "            annotation_lines = f.readlines()\n",
        "\n",
        "            if image_index < 0 or image_index >= len(annotation_lines):\n",
        "                print(f\"Warning: Image index {image_index} is out of bounds for annotation file {annotation_file} with {len(annotation_lines)} lines. Skipping.\")\n",
        "                return None\n",
        "\n",
        "            # Get the specific annotation line\n",
        "            line = annotation_lines[image_index].strip()\n",
        "            parts = line.split(' ')\n",
        "\n",
        "            # Assuming the lines are JUST the annotation data (no filename at the start)\n",
        "            # Extract Dimensions 25 and 26 (0-indexed, so indices 24 and 25 in parts)\n",
        "            if len(parts) >= 26:\n",
        "                try:\n",
        "                    gaze_x_pixel = float(parts[24])\n",
        "                    gaze_y_pixel = float(parts[25])\n",
        "\n",
        "                    # Convert pixel coordinates to proportions\n",
        "                    gaze_x_prop = gaze_x_pixel / screen_width\n",
        "                    gaze_y_prop = gaze_y_pixel / screen_height\n",
        "\n",
        "                    # Ensure proportions are within [0, 1] range (optional, depending on expected data)\n",
        "                    # gaze_x_prop = np.clip(gaze_x_prop, 0.0, 1.0)\n",
        "                    # gaze_y_prop = np.clip(gaze_y_prop, 0.0, 1.0)\n",
        "\n",
        "\n",
        "                    return np.array([gaze_x_prop, gaze_y_prop], dtype=np.float32)\n",
        "\n",
        "                except ValueError:\n",
        "                    print(f\"Could not parse gaze coordinates in line {image_index} of {annotation_file}: {line}. Skipping.\")\n",
        "                    return None\n",
        "            else:\n",
        "                # print(f\"Annotation line {image_index} is too short in {annotation_file}: {line}. Skipping.\") # Keep quiet for batch\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading annotation file {annotation_file}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"load_gaze_annotation function modified to convert gaze coordinates to proportions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da685b31",
        "outputId": "d8f92d4f-9fdc-4a2b-f6f5-0c08964ee38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated processing functions to filter for only evaluation images.\n"
          ]
        }
      ],
      "source": [
        "import random # Import the random module\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses a single image: reads in grayscale, crops black padding,\n",
        "    and resizes to 36x144 pixels.\n",
        "\n",
        "    Args:\n",
        "        image_path (Path): Path to the image file.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray or None: Preprocessed image (36, 144) or None if processing failed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read image in grayscale\n",
        "        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            return None\n",
        "\n",
        "        # Crop black padding and adjust aspect ratio\n",
        "        cropped_img = crop_black_padding(img)\n",
        "\n",
        "        # Resize to target dimensions (width=144, height=36)\n",
        "        resized_img = cv2.resize(cropped_img, (144, 36))\n",
        "\n",
        "        return resized_img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_subject_data(subject_id):\n",
        "    \"\"\"\n",
        "    Processes images for a subject, selecting only evaluation images.\n",
        "\n",
        "    Args:\n",
        "        subject_id (str): The ID of the subject (e.g., 'p00').\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples (preprocessed_image, eye_type, gaze_coords, is_evaluation, day)\n",
        "              containing only evaluation images, or None if processing failed.\n",
        "    \"\"\"\n",
        "    # Load all image data for the subject (paths and metadata)\n",
        "    all_image_data = load_all_subject_data(subject_id)\n",
        "    if all_image_data is None:\n",
        "        return None\n",
        "\n",
        "    # Filter to include only evaluation images\n",
        "    evaluation_data = [item for item in all_image_data if item[3]] # item[3] is is_evaluation\n",
        "\n",
        "    print(f\"Subject {subject_id}: Found {len(evaluation_data)} evaluation images.\")\n",
        "\n",
        "\n",
        "    data_to_process = evaluation_data\n",
        "\n",
        "    print(f\"Subject {subject_id}: Total images to process after filtering: {len(data_to_process)}\")\n",
        "\n",
        "\n",
        "    # Get screen size for gaze coordinate conversion\n",
        "    screen_size = screen_sizes_lookup.get(subject_id)\n",
        "    if screen_size is None:\n",
        "        print(f\"Screen size not found for subject {subject_id}\")\n",
        "        return None\n",
        "\n",
        "    processed_data = []\n",
        "    print(f\"Processing {len(data_to_process)} images for subject {subject_id}...\")\n",
        "\n",
        "    for i, (img_path, eye_type, image_index, is_evaluation, day) in enumerate(data_to_process):\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  Processed {i+1}/{len(data_to_process)} images\")\n",
        "\n",
        "        # Preprocess the image\n",
        "        preprocessed_img = preprocess_image(img_path)\n",
        "        if preprocessed_img is None:\n",
        "            continue\n",
        "\n",
        "        # Load gaze annotation\n",
        "        # Pass subject_id and screen_sizes_lookup to load_gaze_annotation\n",
        "        gaze_coords = load_gaze_annotation(img_path, image_index, screen_size)\n",
        "        if gaze_coords is None:\n",
        "            continue\n",
        "\n",
        "        processed_data.append((\n",
        "            preprocessed_img,\n",
        "            eye_type,\n",
        "            gaze_coords,\n",
        "            is_evaluation,\n",
        "            day\n",
        "        ))\n",
        "\n",
        "    print(f\"Successfully processed {len(processed_data)} images for subject {subject_id}\")\n",
        "    return processed_data\n",
        "\n",
        "print(\"Updated processing functions to filter for only evaluation images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19fe9824"
      },
      "source": [
        "# Task\n",
        "Convert the preprocessed MPIIGaze dataset, including cropped eye images, gaze coordinates (as proportions of screen size), day, and subject ID, into a TFRecords dataset that can be parsed by the provided `parse` function (with 'phase' replaced by 'day')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71a75b5"
      },
      "source": [
        "## Define tfrecord features\n",
        "\n",
        "### Subtask:\n",
        "Define the structure of the features that will be stored in the TFRecords, including 'day', 'subject_id', 'eye_img', 'x', and 'y'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45253281"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the feature structure for the TFRecords dataset as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23630575",
        "outputId": "a2e77696-baf0-49f5-d84d-811b246bc29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated TFRecords feature description by removing is_evaluation.\n",
            "{'day': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'subject_id': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'eye_img': FixedLenFeature(shape=[], dtype=tf.string, default_value=None), 'x': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None), 'y': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None)}\n"
          ]
        }
      ],
      "source": [
        "# Define the feature structure for the TFRecords\n",
        "feature_description = {\n",
        "    'day': tf.io.FixedLenFeature([], tf.string),\n",
        "    'subject_id': tf.io.FixedLenFeature([], tf.string),\n",
        "    'eye_img': tf.io.FixedLenFeature([], tf.string),  # Image data stored as a serialized byte string\n",
        "    'x': tf.io.FixedLenFeature([], tf.float32),      # Gaze x-coordinate (proportion)\n",
        "    'y': tf.io.FixedLenFeature([], tf.float32),      # Gaze y-coordinate (proportion)\n",
        "    # Removed 'is_evaluation' feature as we are only processing evaluation images\n",
        "}\n",
        "\n",
        "print(\"Updated TFRecords feature description by removing is_evaluation.\")\n",
        "print(feature_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f419885"
      },
      "source": [
        "## Create a serialization function\n",
        "\n",
        "### Subtask:\n",
        "Write a function that takes a preprocessed data entry (image, eye type, gaze coordinates) and the corresponding day and subject ID, and serializes it into a TFRecord `Example` protocol buffer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6cc4d25"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to write a function that serializes a single data entry into a TFRecord Example protocol buffer. This involves converting data types and creating a tf.train.Example object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ef44369",
        "outputId": "872f22df-9b19-488a-a20d-ed7abbf33460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated serialize_example function by removing is_evaluation feature.\n"
          ]
        }
      ],
      "source": [
        "def serialize_example(image, eye_type, gaze_coords, is_evaluation, day, subject_id):\n",
        "    \"\"\"\n",
        "    Serializes a single data entry into a TFRecord Example protocol buffer.\n",
        "    is_evaluation is included in the input for compatibility but not serialized.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): The preprocessed eye image (36x144, grayscale).\n",
        "        eye_type (str): The type of eye ('left' or 'right').\n",
        "        gaze_coords (np.ndarray): The gaze coordinates as proportions [x, y].\n",
        "        is_evaluation (bool): True for evaluation images, False for calibration (not serialized).\n",
        "        day (str): The day the data was recorded.\n",
        "        subject_id (str): The ID of the subject.\n",
        "\n",
        "    Returns:\n",
        "        bytes: The serialized tf.train.Example protocol buffer.\n",
        "    \"\"\"\n",
        "    # Convert image to byte string\n",
        "    image_bytes = image.tobytes()\n",
        "\n",
        "    # Convert strings to byte strings\n",
        "    day_bytes = day.encode('utf-8')\n",
        "    subject_id_bytes = subject_id.encode('utf-8')\n",
        "\n",
        "    # Create feature dictionary (excluding is_evaluation)\n",
        "    feature = {\n",
        "        'day': tf.train.Feature(bytes_list=tf.train.BytesList(value=[day_bytes])),\n",
        "        'subject_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[subject_id_bytes])),\n",
        "        'eye_img': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes])),\n",
        "        'x': tf.train.Feature(float_list=tf.train.FloatList(value=[gaze_coords[0]])),\n",
        "        'y': tf.train.Feature(float_list=tf.train.FloatList(value=[gaze_coords[1]])),\n",
        "        # Removed 'is_evaluation' feature from serialization\n",
        "    }\n",
        "\n",
        "    # Create and serialize Example\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "print(\"Updated serialize_example function by removing is_evaluation feature.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba952892"
      },
      "source": [
        "## Iterate through subjects and days\n",
        "\n",
        "### Subtask:\n",
        "Loop through each subject and then through each day's data for that subject to prepare for data loading and preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02af15af"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through each subject and day to prepare for loading and preprocessing data for TFRecords creation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18578f7e"
      },
      "source": [
        "**Reasoning**:\n",
        "The loops for iterating through subjects and days have been successfully set up, and the data for each subject is grouped by day. The next logical step within this structure is to load the image data, preprocess it, get the gaze annotations, and then serialize this data into TFRecords for each day.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e3d7686e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff97119b-7683-4407-b894-e5f149fe0cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFRecords output directory: mpiigaze_tfrecords_simplified\n",
            "Simplified TFRecords creation pipeline ready. Run create_tfrecords_for_all_subjects() to execute.\n"
          ]
        }
      ],
      "source": [
        "# Create simplified TFRecords processing pipeline\n",
        "tfrecords_output_dir = Path(\"./mpiigaze_tfrecords_simplified\")\n",
        "tfrecords_output_dir.mkdir(exist_ok=True)\n",
        "print(f\"TFRecords output directory: {tfrecords_output_dir}\")\n",
        "\n",
        "def create_tfrecords_for_all_subjects():\n",
        "    \"\"\"\n",
        "    Creates one TFRecord file per subject containing all their images\n",
        "    (both evaluation and calibration) with proper labeling.\n",
        "    \"\"\"\n",
        "    print(\"Starting simplified TFRecords creation for all subjects...\")\n",
        "\n",
        "    for subject_id in subject_ids:\n",
        "        print(f\"\\n=== Processing subject: {subject_id} ===\")\n",
        "\n",
        "        # Define output file for this subject\n",
        "        tfrecord_filename = tfrecords_output_dir / f\"{subject_id}_all_data.tfrecords\"\n",
        "\n",
        "        # Skip if file already exists\n",
        "        if tfrecord_filename.exists():\n",
        "            print(f\"TFRecord already exists: {tfrecord_filename}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Process all data for this subject\n",
        "        processed_data = process_subject_data(subject_id)\n",
        "        if processed_data is None:\n",
        "            print(f\"Failed to process data for subject {subject_id}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Write TFRecord file\n",
        "        try:\n",
        "            with tf.io.TFRecordWriter(str(tfrecord_filename)) as writer:\n",
        "                print(f\"Writing {len(processed_data)} examples to {tfrecord_filename}\")\n",
        "\n",
        "                evaluation_count = 0\n",
        "                calibration_count = 0\n",
        "\n",
        "                for img, eye_type, gaze_coords, is_evaluation, day in processed_data:\n",
        "                    # Serialize and write the example\n",
        "                    serialized_example = serialize_example(\n",
        "                        img, eye_type, gaze_coords, is_evaluation, day, subject_id\n",
        "                    )\n",
        "                    writer.write(serialized_example)\n",
        "\n",
        "                    # Count types\n",
        "                    if is_evaluation:\n",
        "                        evaluation_count += 1\n",
        "                    else:\n",
        "                        calibration_count += 1\n",
        "\n",
        "                print(f\"✓ Successfully wrote {len(processed_data)} examples\")\n",
        "                print(f\"  - {evaluation_count} evaluation images\")\n",
        "                print(f\"  - {calibration_count} calibration images\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing TFRecord for {subject_id}: {e}\")\n",
        "            # Clean up incomplete file\n",
        "            if tfrecord_filename.exists():\n",
        "                tfrecord_filename.unlink()\n",
        "                print(f\"Cleaned up incomplete file: {tfrecord_filename}\")\n",
        "\n",
        "    print(\"\\n✓ Finished creating TFRecords for all subjects\")\n",
        "\n",
        "# Execute the simplified pipeline (comment out to prevent auto-execution)\n",
        "# create_tfrecords_for_all_subjects()\n",
        "\n",
        "print(\"Simplified TFRecords creation pipeline ready. Run create_tfrecords_for_all_subjects() to execute.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_tfrecords_for_all_subjects()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-iX9mIvQPAt",
        "outputId": "1f69facf-28c5-438d-c594-cb421d642a30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting simplified TFRecords creation for all subjects...\n",
            "\n",
            "=== Processing subject: p00 ===\n",
            "Loaded 29961 total images for subject p00\n",
            "  - 2927 evaluation images\n",
            "  - 27034 calibration images\n",
            "Subject p00: Found 2927 evaluation images.\n",
            "Subject p00: Total images to process after filtering: 2927\n",
            "Processing 2927 images for subject p00...\n",
            "  Processed 100/2927 images\n",
            "  Processed 200/2927 images\n",
            "  Processed 300/2927 images\n",
            "  Processed 400/2927 images\n",
            "  Processed 500/2927 images\n",
            "  Processed 600/2927 images\n",
            "  Processed 700/2927 images\n",
            "  Processed 800/2927 images\n",
            "  Processed 900/2927 images\n",
            "  Processed 1000/2927 images\n",
            "  Processed 1100/2927 images\n",
            "  Processed 1200/2927 images\n",
            "  Processed 1300/2927 images\n",
            "  Processed 1400/2927 images\n",
            "  Processed 1500/2927 images\n",
            "  Processed 1600/2927 images\n",
            "  Processed 1700/2927 images\n",
            "  Processed 1800/2927 images\n",
            "  Processed 1900/2927 images\n",
            "  Processed 2000/2927 images\n",
            "  Processed 2100/2927 images\n",
            "  Processed 2200/2927 images\n",
            "  Processed 2300/2927 images\n",
            "  Processed 2400/2927 images\n",
            "  Processed 2500/2927 images\n",
            "  Processed 2600/2927 images\n",
            "  Processed 2700/2927 images\n",
            "  Processed 2800/2927 images\n",
            "  Processed 2900/2927 images\n",
            "Successfully processed 2927 images for subject p00\n",
            "Writing 2927 examples to mpiigaze_tfrecords_simplified/p00_all_data.tfrecords\n",
            "✓ Successfully wrote 2927 examples\n",
            "  - 2927 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p01 ===\n",
            "Loaded 24143 total images for subject p01\n",
            "  - 2904 evaluation images\n",
            "  - 21239 calibration images\n",
            "Subject p01: Found 2904 evaluation images.\n",
            "Subject p01: Total images to process after filtering: 2904\n",
            "Processing 2904 images for subject p01...\n",
            "  Processed 100/2904 images\n",
            "  Processed 200/2904 images\n",
            "  Processed 300/2904 images\n",
            "  Processed 400/2904 images\n",
            "  Processed 500/2904 images\n",
            "  Processed 600/2904 images\n",
            "  Processed 700/2904 images\n",
            "  Processed 800/2904 images\n",
            "  Processed 900/2904 images\n",
            "  Processed 1000/2904 images\n",
            "  Processed 1100/2904 images\n",
            "  Processed 1200/2904 images\n",
            "  Processed 1300/2904 images\n",
            "  Processed 1400/2904 images\n",
            "  Processed 1500/2904 images\n",
            "  Processed 1600/2904 images\n",
            "  Processed 1700/2904 images\n",
            "  Processed 1800/2904 images\n",
            "  Processed 1900/2904 images\n",
            "  Processed 2000/2904 images\n",
            "  Processed 2100/2904 images\n",
            "  Processed 2200/2904 images\n",
            "  Processed 2300/2904 images\n",
            "  Processed 2400/2904 images\n",
            "  Processed 2500/2904 images\n",
            "  Processed 2600/2904 images\n",
            "  Processed 2700/2904 images\n",
            "  Processed 2800/2904 images\n",
            "  Processed 2900/2904 images\n",
            "Successfully processed 2904 images for subject p01\n",
            "Writing 2904 examples to mpiigaze_tfrecords_simplified/p01_all_data.tfrecords\n",
            "✓ Successfully wrote 2904 examples\n",
            "  - 2904 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p02 ===\n",
            "Loaded 28019 total images for subject p02\n",
            "  - 2916 evaluation images\n",
            "  - 25103 calibration images\n",
            "Subject p02: Found 2916 evaluation images.\n",
            "Subject p02: Total images to process after filtering: 2916\n",
            "Processing 2916 images for subject p02...\n",
            "  Processed 100/2916 images\n",
            "  Processed 200/2916 images\n",
            "  Processed 300/2916 images\n",
            "  Processed 400/2916 images\n",
            "  Processed 500/2916 images\n",
            "  Processed 600/2916 images\n",
            "  Processed 700/2916 images\n",
            "  Processed 800/2916 images\n",
            "  Processed 900/2916 images\n",
            "  Processed 1000/2916 images\n",
            "  Processed 1100/2916 images\n",
            "  Processed 1200/2916 images\n",
            "  Processed 1300/2916 images\n",
            "  Processed 1400/2916 images\n",
            "  Processed 1500/2916 images\n",
            "  Processed 1600/2916 images\n",
            "  Processed 1700/2916 images\n",
            "  Processed 1800/2916 images\n",
            "  Processed 1900/2916 images\n",
            "  Processed 2000/2916 images\n",
            "  Processed 2100/2916 images\n",
            "  Processed 2200/2916 images\n",
            "  Processed 2300/2916 images\n",
            "  Processed 2400/2916 images\n",
            "  Processed 2500/2916 images\n",
            "  Processed 2600/2916 images\n",
            "  Processed 2700/2916 images\n",
            "  Processed 2800/2916 images\n",
            "  Processed 2900/2916 images\n",
            "Successfully processed 2916 images for subject p02\n",
            "Writing 2916 examples to mpiigaze_tfrecords_simplified/p02_all_data.tfrecords\n",
            "✓ Successfully wrote 2916 examples\n",
            "  - 2916 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p03 ===\n",
            "Loaded 35075 total images for subject p03\n",
            "  - 2929 evaluation images\n",
            "  - 32146 calibration images\n",
            "Subject p03: Found 2929 evaluation images.\n",
            "Subject p03: Total images to process after filtering: 2929\n",
            "Processing 2929 images for subject p03...\n",
            "  Processed 100/2929 images\n",
            "  Processed 200/2929 images\n",
            "  Processed 300/2929 images\n",
            "  Processed 400/2929 images\n",
            "  Processed 500/2929 images\n",
            "  Processed 600/2929 images\n",
            "  Processed 700/2929 images\n",
            "  Processed 800/2929 images\n",
            "  Processed 900/2929 images\n",
            "  Processed 1000/2929 images\n",
            "  Processed 1100/2929 images\n",
            "  Processed 1200/2929 images\n",
            "  Processed 1300/2929 images\n",
            "  Processed 1400/2929 images\n",
            "  Processed 1500/2929 images\n",
            "  Processed 1600/2929 images\n",
            "  Processed 1700/2929 images\n",
            "  Processed 1800/2929 images\n",
            "  Processed 1900/2929 images\n",
            "  Processed 2000/2929 images\n",
            "  Processed 2100/2929 images\n",
            "  Processed 2200/2929 images\n",
            "  Processed 2300/2929 images\n",
            "  Processed 2400/2929 images\n",
            "  Processed 2500/2929 images\n",
            "  Processed 2600/2929 images\n",
            "  Processed 2700/2929 images\n",
            "  Processed 2800/2929 images\n",
            "  Processed 2900/2929 images\n",
            "Successfully processed 2929 images for subject p03\n",
            "Writing 2929 examples to mpiigaze_tfrecords_simplified/p03_all_data.tfrecords\n",
            "✓ Successfully wrote 2929 examples\n",
            "  - 2929 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p04 ===\n",
            "Loaded 16831 total images for subject p04\n",
            "  - 2860 evaluation images\n",
            "  - 13971 calibration images\n",
            "Subject p04: Found 2860 evaluation images.\n",
            "Subject p04: Total images to process after filtering: 2860\n",
            "Processing 2860 images for subject p04...\n",
            "  Processed 100/2860 images\n",
            "  Processed 200/2860 images\n",
            "  Processed 300/2860 images\n",
            "  Processed 400/2860 images\n",
            "  Processed 500/2860 images\n",
            "  Processed 600/2860 images\n",
            "  Processed 700/2860 images\n",
            "  Processed 800/2860 images\n",
            "  Processed 900/2860 images\n",
            "  Processed 1000/2860 images\n",
            "  Processed 1100/2860 images\n",
            "  Processed 1200/2860 images\n",
            "  Processed 1300/2860 images\n",
            "  Processed 1400/2860 images\n",
            "  Processed 1500/2860 images\n",
            "  Processed 1600/2860 images\n",
            "  Processed 1700/2860 images\n",
            "  Processed 1800/2860 images\n",
            "  Processed 1900/2860 images\n",
            "  Processed 2000/2860 images\n",
            "  Processed 2100/2860 images\n",
            "  Processed 2200/2860 images\n",
            "  Processed 2300/2860 images\n",
            "  Processed 2400/2860 images\n",
            "  Processed 2500/2860 images\n",
            "  Processed 2600/2860 images\n",
            "  Processed 2700/2860 images\n",
            "  Processed 2800/2860 images\n",
            "Successfully processed 2860 images for subject p04\n",
            "Writing 2860 examples to mpiigaze_tfrecords_simplified/p04_all_data.tfrecords\n",
            "✓ Successfully wrote 2860 examples\n",
            "  - 2860 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p05 ===\n",
            "Loaded 16577 total images for subject p05\n",
            "  - 2870 evaluation images\n",
            "  - 13707 calibration images\n",
            "Subject p05: Found 2870 evaluation images.\n",
            "Subject p05: Total images to process after filtering: 2870\n",
            "Processing 2870 images for subject p05...\n",
            "  Processed 100/2870 images\n",
            "  Processed 200/2870 images\n",
            "  Processed 300/2870 images\n",
            "  Processed 400/2870 images\n",
            "  Processed 500/2870 images\n",
            "  Processed 600/2870 images\n",
            "  Processed 700/2870 images\n",
            "  Processed 800/2870 images\n",
            "  Processed 900/2870 images\n",
            "  Processed 1000/2870 images\n",
            "  Processed 1100/2870 images\n",
            "  Processed 1200/2870 images\n",
            "  Processed 1300/2870 images\n",
            "  Processed 1400/2870 images\n",
            "  Processed 1500/2870 images\n",
            "  Processed 1600/2870 images\n",
            "  Processed 1700/2870 images\n",
            "  Processed 1800/2870 images\n",
            "  Processed 1900/2870 images\n",
            "  Processed 2000/2870 images\n",
            "  Processed 2100/2870 images\n",
            "  Processed 2200/2870 images\n",
            "  Processed 2300/2870 images\n",
            "  Processed 2400/2870 images\n",
            "  Processed 2500/2870 images\n",
            "  Processed 2600/2870 images\n",
            "  Processed 2700/2870 images\n",
            "  Processed 2800/2870 images\n",
            "Successfully processed 2870 images for subject p05\n",
            "Writing 2870 examples to mpiigaze_tfrecords_simplified/p05_all_data.tfrecords\n",
            "✓ Successfully wrote 2870 examples\n",
            "  - 2870 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p06 ===\n",
            "Loaded 18448 total images for subject p06\n",
            "  - 2877 evaluation images\n",
            "  - 15571 calibration images\n",
            "Subject p06: Found 2877 evaluation images.\n",
            "Subject p06: Total images to process after filtering: 2877\n",
            "Processing 2877 images for subject p06...\n",
            "  Processed 100/2877 images\n",
            "  Processed 200/2877 images\n",
            "  Processed 300/2877 images\n",
            "  Processed 400/2877 images\n",
            "  Processed 500/2877 images\n",
            "  Processed 600/2877 images\n",
            "  Processed 700/2877 images\n",
            "  Processed 800/2877 images\n",
            "  Processed 900/2877 images\n",
            "  Processed 1000/2877 images\n",
            "  Processed 1100/2877 images\n",
            "  Processed 1200/2877 images\n",
            "  Processed 1300/2877 images\n",
            "  Processed 1400/2877 images\n",
            "  Processed 1500/2877 images\n",
            "  Processed 1600/2877 images\n",
            "  Processed 1700/2877 images\n",
            "  Processed 1800/2877 images\n",
            "  Processed 1900/2877 images\n",
            "  Processed 2000/2877 images\n",
            "  Processed 2100/2877 images\n",
            "  Processed 2200/2877 images\n",
            "  Processed 2300/2877 images\n",
            "  Processed 2400/2877 images\n",
            "  Processed 2500/2877 images\n",
            "  Processed 2600/2877 images\n",
            "  Processed 2700/2877 images\n",
            "  Processed 2800/2877 images\n",
            "Successfully processed 2877 images for subject p06\n",
            "Writing 2877 examples to mpiigaze_tfrecords_simplified/p06_all_data.tfrecords\n",
            "✓ Successfully wrote 2877 examples\n",
            "  - 2877 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p07 ===\n",
            "Loaded 15509 total images for subject p07\n",
            "  - 2843 evaluation images\n",
            "  - 12666 calibration images\n",
            "Subject p07: Found 2843 evaluation images.\n",
            "Subject p07: Total images to process after filtering: 2843\n",
            "Processing 2843 images for subject p07...\n",
            "  Processed 100/2843 images\n",
            "  Processed 200/2843 images\n",
            "  Processed 300/2843 images\n",
            "  Processed 400/2843 images\n",
            "  Processed 500/2843 images\n",
            "  Processed 600/2843 images\n",
            "  Processed 700/2843 images\n",
            "  Processed 800/2843 images\n",
            "  Processed 900/2843 images\n",
            "  Processed 1000/2843 images\n",
            "  Processed 1100/2843 images\n",
            "  Processed 1200/2843 images\n",
            "  Processed 1300/2843 images\n",
            "  Processed 1400/2843 images\n",
            "  Processed 1500/2843 images\n",
            "  Processed 1600/2843 images\n",
            "  Processed 1700/2843 images\n",
            "  Processed 1800/2843 images\n",
            "  Processed 1900/2843 images\n",
            "  Processed 2000/2843 images\n",
            "  Processed 2100/2843 images\n",
            "  Processed 2200/2843 images\n",
            "  Processed 2300/2843 images\n",
            "  Processed 2400/2843 images\n",
            "  Processed 2500/2843 images\n",
            "  Processed 2600/2843 images\n",
            "  Processed 2700/2843 images\n",
            "  Processed 2800/2843 images\n",
            "Successfully processed 2843 images for subject p07\n",
            "Writing 2843 examples to mpiigaze_tfrecords_simplified/p07_all_data.tfrecords\n",
            "✓ Successfully wrote 2843 examples\n",
            "  - 2843 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p08 ===\n",
            "Loaded 10701 total images for subject p08\n",
            "  - 2767 evaluation images\n",
            "  - 7934 calibration images\n",
            "Subject p08: Found 2767 evaluation images.\n",
            "Subject p08: Total images to process after filtering: 2767\n",
            "Processing 2767 images for subject p08...\n",
            "  Processed 100/2767 images\n",
            "  Processed 200/2767 images\n",
            "  Processed 300/2767 images\n",
            "  Processed 400/2767 images\n",
            "  Processed 500/2767 images\n",
            "  Processed 600/2767 images\n",
            "  Processed 700/2767 images\n",
            "  Processed 800/2767 images\n",
            "  Processed 900/2767 images\n",
            "  Processed 1000/2767 images\n",
            "  Processed 1100/2767 images\n",
            "  Processed 1200/2767 images\n",
            "  Processed 1300/2767 images\n",
            "  Processed 1400/2767 images\n",
            "  Processed 1500/2767 images\n",
            "  Processed 1600/2767 images\n",
            "  Processed 1700/2767 images\n",
            "  Processed 1800/2767 images\n",
            "  Processed 1900/2767 images\n",
            "  Processed 2000/2767 images\n",
            "  Processed 2100/2767 images\n",
            "  Processed 2200/2767 images\n",
            "  Processed 2300/2767 images\n",
            "  Processed 2400/2767 images\n",
            "  Processed 2500/2767 images\n",
            "  Processed 2600/2767 images\n",
            "  Processed 2700/2767 images\n",
            "Successfully processed 2767 images for subject p08\n",
            "Writing 2767 examples to mpiigaze_tfrecords_simplified/p08_all_data.tfrecords\n",
            "✓ Successfully wrote 2767 examples\n",
            "  - 2767 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p09 ===\n",
            "Loaded 7995 total images for subject p09\n",
            "  - 2719 evaluation images\n",
            "  - 5276 calibration images\n",
            "Subject p09: Found 2719 evaluation images.\n",
            "Subject p09: Total images to process after filtering: 2719\n",
            "Processing 2719 images for subject p09...\n",
            "  Processed 100/2719 images\n",
            "  Processed 200/2719 images\n",
            "  Processed 300/2719 images\n",
            "  Processed 400/2719 images\n",
            "  Processed 500/2719 images\n",
            "  Processed 600/2719 images\n",
            "  Processed 700/2719 images\n",
            "  Processed 800/2719 images\n",
            "  Processed 900/2719 images\n",
            "  Processed 1000/2719 images\n",
            "  Processed 1100/2719 images\n",
            "  Processed 1200/2719 images\n",
            "  Processed 1300/2719 images\n",
            "  Processed 1400/2719 images\n",
            "  Processed 1500/2719 images\n",
            "  Processed 1600/2719 images\n",
            "  Processed 1700/2719 images\n",
            "  Processed 1800/2719 images\n",
            "  Processed 1900/2719 images\n",
            "  Processed 2000/2719 images\n",
            "  Processed 2100/2719 images\n",
            "  Processed 2200/2719 images\n",
            "  Processed 2300/2719 images\n",
            "  Processed 2400/2719 images\n",
            "  Processed 2500/2719 images\n",
            "  Processed 2600/2719 images\n",
            "  Processed 2700/2719 images\n",
            "Successfully processed 2719 images for subject p09\n",
            "Writing 2719 examples to mpiigaze_tfrecords_simplified/p09_all_data.tfrecords\n",
            "✓ Successfully wrote 2719 examples\n",
            "  - 2719 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p10 ===\n",
            "Loaded 2810 total images for subject p10\n",
            "  - 2194 evaluation images\n",
            "  - 616 calibration images\n",
            "Subject p10: Found 2194 evaluation images.\n",
            "Subject p10: Total images to process after filtering: 2194\n",
            "Processing 2194 images for subject p10...\n",
            "  Processed 100/2194 images\n",
            "  Processed 200/2194 images\n",
            "  Processed 300/2194 images\n",
            "  Processed 400/2194 images\n",
            "  Processed 500/2194 images\n",
            "  Processed 600/2194 images\n",
            "  Processed 700/2194 images\n",
            "  Processed 800/2194 images\n",
            "  Processed 900/2194 images\n",
            "  Processed 1000/2194 images\n",
            "  Processed 1100/2194 images\n",
            "  Processed 1200/2194 images\n",
            "  Processed 1300/2194 images\n",
            "  Processed 1400/2194 images\n",
            "  Processed 1500/2194 images\n",
            "  Processed 1600/2194 images\n",
            "  Processed 1700/2194 images\n",
            "  Processed 1800/2194 images\n",
            "  Processed 1900/2194 images\n",
            "  Processed 2000/2194 images\n",
            "  Processed 2100/2194 images\n",
            "Successfully processed 2194 images for subject p10\n",
            "Writing 2194 examples to mpiigaze_tfrecords_simplified/p10_all_data.tfrecords\n",
            "✓ Successfully wrote 2194 examples\n",
            "  - 2194 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p11 ===\n",
            "Loaded 2982 total images for subject p11\n",
            "  - 2262 evaluation images\n",
            "  - 720 calibration images\n",
            "Subject p11: Found 2262 evaluation images.\n",
            "Subject p11: Total images to process after filtering: 2262\n",
            "Processing 2262 images for subject p11...\n",
            "  Processed 100/2262 images\n",
            "  Processed 200/2262 images\n",
            "  Processed 300/2262 images\n",
            "  Processed 400/2262 images\n",
            "  Processed 500/2262 images\n",
            "  Processed 600/2262 images\n",
            "  Processed 700/2262 images\n",
            "  Processed 800/2262 images\n",
            "  Processed 900/2262 images\n",
            "  Processed 1000/2262 images\n",
            "  Processed 1100/2262 images\n",
            "  Processed 1200/2262 images\n",
            "  Processed 1300/2262 images\n",
            "  Processed 1400/2262 images\n",
            "  Processed 1500/2262 images\n",
            "  Processed 1600/2262 images\n",
            "  Processed 1700/2262 images\n",
            "  Processed 1800/2262 images\n",
            "  Processed 1900/2262 images\n",
            "  Processed 2000/2262 images\n",
            "  Processed 2100/2262 images\n",
            "  Processed 2200/2262 images\n",
            "Successfully processed 2262 images for subject p11\n",
            "Writing 2262 examples to mpiigaze_tfrecords_simplified/p11_all_data.tfrecords\n",
            "✓ Successfully wrote 2262 examples\n",
            "  - 2262 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p12 ===\n",
            "Loaded 1609 total images for subject p12\n",
            "  - 1601 evaluation images\n",
            "  - 8 calibration images\n",
            "Subject p12: Found 1601 evaluation images.\n",
            "Subject p12: Total images to process after filtering: 1601\n",
            "Processing 1601 images for subject p12...\n",
            "  Processed 100/1601 images\n",
            "  Processed 200/1601 images\n",
            "  Processed 300/1601 images\n",
            "  Processed 400/1601 images\n",
            "  Processed 500/1601 images\n",
            "  Processed 600/1601 images\n",
            "  Processed 700/1601 images\n",
            "  Processed 800/1601 images\n",
            "  Processed 900/1601 images\n",
            "  Processed 1000/1601 images\n",
            "  Processed 1100/1601 images\n",
            "  Processed 1200/1601 images\n",
            "  Processed 1300/1601 images\n",
            "  Processed 1400/1601 images\n",
            "  Processed 1500/1601 images\n",
            "  Processed 1600/1601 images\n",
            "Successfully processed 1601 images for subject p12\n",
            "Writing 1601 examples to mpiigaze_tfrecords_simplified/p12_all_data.tfrecords\n",
            "✓ Successfully wrote 1601 examples\n",
            "  - 1601 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p13 ===\n",
            "Loaded 1498 total images for subject p13\n",
            "  - 1498 evaluation images\n",
            "  - 0 calibration images\n",
            "Subject p13: Found 1498 evaluation images.\n",
            "Subject p13: Total images to process after filtering: 1498\n",
            "Processing 1498 images for subject p13...\n",
            "  Processed 100/1498 images\n",
            "  Processed 200/1498 images\n",
            "  Processed 300/1498 images\n",
            "  Processed 400/1498 images\n",
            "  Processed 500/1498 images\n",
            "  Processed 600/1498 images\n",
            "  Processed 700/1498 images\n",
            "  Processed 800/1498 images\n",
            "  Processed 900/1498 images\n",
            "  Processed 1000/1498 images\n",
            "  Processed 1100/1498 images\n",
            "  Processed 1200/1498 images\n",
            "  Processed 1300/1498 images\n",
            "  Processed 1400/1498 images\n",
            "Successfully processed 1498 images for subject p13\n",
            "Writing 1498 examples to mpiigaze_tfrecords_simplified/p13_all_data.tfrecords\n",
            "✓ Successfully wrote 1498 examples\n",
            "  - 1498 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "=== Processing subject: p14 ===\n",
            "Loaded 1500 total images for subject p14\n",
            "  - 1500 evaluation images\n",
            "  - 0 calibration images\n",
            "Subject p14: Found 1500 evaluation images.\n",
            "Subject p14: Total images to process after filtering: 1500\n",
            "Processing 1500 images for subject p14...\n",
            "  Processed 100/1500 images\n",
            "  Processed 200/1500 images\n",
            "  Processed 300/1500 images\n",
            "  Processed 400/1500 images\n",
            "  Processed 500/1500 images\n",
            "  Processed 600/1500 images\n",
            "  Processed 700/1500 images\n",
            "  Processed 800/1500 images\n",
            "  Processed 900/1500 images\n",
            "  Processed 1000/1500 images\n",
            "  Processed 1100/1500 images\n",
            "  Processed 1200/1500 images\n",
            "  Processed 1300/1500 images\n",
            "  Processed 1400/1500 images\n",
            "  Processed 1500/1500 images\n",
            "Successfully processed 1500 images for subject p14\n",
            "Writing 1500 examples to mpiigaze_tfrecords_simplified/p14_all_data.tfrecords\n",
            "✓ Successfully wrote 1500 examples\n",
            "  - 1500 evaluation images\n",
            "  - 0 calibration images\n",
            "\n",
            "✓ Finished creating TFRecords for all subjects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cf0d7c1"
      },
      "source": [
        "## Verify tfrecords\n",
        "\n",
        "### Subtask:\n",
        "After writing, add code to read a few examples from the generated TFRecord files using the provided `parse` function (adapted for 'day') to ensure the data is correctly serialized and can be parsed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5999820d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the parsing function, get a list of TFRecord files, create a dataset, map the parsing function, take a few examples, and iterate through them to print the features for verification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43a30d51",
        "outputId": "63c20025-8b0c-4d91-d0d1-42d2275c808f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated parsing function by removing is_evaluation feature.\n",
            "Run test_parsing_with_sample_data() to verify parsing works correctly with the new schema.\n"
          ]
        }
      ],
      "source": [
        "def parse_tfrecord_example(example_proto):\n",
        "    \"\"\"\n",
        "    Parses a single serialized TFRecord Example with the updated schema\n",
        "    excluding is_evaluation feature.\n",
        "\n",
        "    Args:\n",
        "        example_proto (bytes): A serialized tf.train.Example protocol buffer.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the parsed features.\n",
        "    \"\"\"\n",
        "    # Our updated TFRecords schema (excluding is_evaluation)\n",
        "    feature_description_no_eval = {\n",
        "        'day': tf.io.FixedLenFeature([], tf.string),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.string),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "    }\n",
        "\n",
        "    # Parse the example using the updated feature description\n",
        "    parsed_features = tf.io.parse_single_example(example_proto, feature_description_no_eval)\n",
        "\n",
        "    # Decode the eye_img byte string back into a tensor and reshape it\n",
        "    image_bytes = parsed_features['eye_img']\n",
        "    image_tensor = tf.io.decode_raw(image_bytes, tf.uint8)\n",
        "    image_tensor = tf.reshape(image_tensor, (36, 144))\n",
        "    image_tensor = tf.cast(image_tensor, tf.float32) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    # Replace the raw byte string with the decoded image tensor\n",
        "    parsed_features['eye_img'] = image_tensor\n",
        "\n",
        "    # Note: is_evaluation is not in the parsed features\n",
        "\n",
        "    return parsed_features\n",
        "\n",
        "def test_parsing_with_sample_data():\n",
        "    \"\"\"\n",
        "    Tests the parsing function with sample data from the simplified TFRecords.\n",
        "    Assumes TFRecords contain only evaluation images and thus no is_evaluation feature.\n",
        "    \"\"\"\n",
        "    # Look for TFRecord files in the simplified directory\n",
        "    tfrecords_dir = Path(\"./mpiigaze_tfrecords_simplified\")\n",
        "    tfrecord_files = sorted(list(tfrecords_dir.glob(\"*.tfrecords\")))\n",
        "\n",
        "    if not tfrecord_files:\n",
        "        print(f\"No TFRecord files found in {tfrecords_dir}\")\n",
        "        print(\"Run create_tfrecords_for_all_subjects() first to generate test data.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
        "\n",
        "    # Test with the first file\n",
        "    test_file = tfrecord_files[0]\n",
        "    print(f\"Testing parsing with: {test_file}\")\n",
        "\n",
        "    # Create dataset and parse examples\n",
        "    dataset = tf.data.TFRecordDataset([str(test_file)])\n",
        "    # Use the updated parsing function\n",
        "    parsed_dataset = dataset.map(parse_tfrecord_example)\n",
        "\n",
        "    # Take a few examples for verification\n",
        "    sample_examples = parsed_dataset.take(5)\n",
        "\n",
        "    print(f\"\\nParsing verification - first 5 examples (expecting only evaluation images):\")\n",
        "\n",
        "    # Since we only process evaluation images, all parsed examples should be 'evaluation' conceptually\n",
        "    evaluation_count = 0\n",
        "    for i, example in enumerate(sample_examples):\n",
        "        evaluation_count += 1 # All are evaluation images based on the new processing\n",
        "\n",
        "        print(f\"--- Example {i+1} ---\")\n",
        "        print(f\"  Image shape: {example['eye_img'].shape}\")\n",
        "        print(f\"  Subject ID: {example['subject_id'].numpy().decode('utf-8')}\")\n",
        "        print(f\"  Day: {example['day'].numpy().decode('utf-8')}\")\n",
        "        print(f\"  Gaze coords (x, y): ({example['x'].numpy():.4f}, {example['y'].numpy():.4f})\")\n",
        "        # is_evaluation feature is no longer present\n",
        "\n",
        "    print(f\"\\nSample breakdown:\")\n",
        "    print(f\"  - {evaluation_count} evaluation images (all examples parsed)\")\n",
        "\n",
        "\n",
        "print(\"Updated parsing function by removing is_evaluation feature.\")\n",
        "print(\"Run test_parsing_with_sample_data() to verify parsing works correctly with the new schema.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51c4cc16"
      },
      "source": [
        "## Summary: Simplified MPIIGaze TFRecords Pipeline\n",
        "\n",
        "### Key Improvements Made:\n",
        "\n",
        "**1. Simplified Functions (reduced from ~15 to 5 core functions):**\n",
        "- `crop_black_padding()` - Streamlined black pixel removal with aspect ratio adjustment\n",
        "- `load_all_subject_data()` - Unified loading of evaluation and calibration images  \n",
        "- `preprocess_image()` - Combined cropping and resizing\n",
        "- `process_subject_data()` - End-to-end subject processing\n",
        "- `create_tfrecords_for_all_subjects()` - Optimized TFRecord generation\n",
        "\n",
        "**2. Enhanced TFRecords Schema:**\n",
        "```python\n",
        "feature_description = {\n",
        "    'day': tf.string,\n",
        "    'subject_id': tf.string,\n",
        "    'eye_img': tf.string,     # 36x144 preprocessed image\n",
        "    'x': tf.float32,          # Gaze x-coordinate (proportion)\n",
        "    'y': tf.float32,          # Gaze y-coordinate (proportion)  \n",
        "    'is_evaluation': tf.bool, # NEW: True=evaluation, False=calibration\n",
        "}\n",
        "```\n",
        "\n",
        "**3. Processing Improvements:**\n",
        "- **Single pass processing** - No complex day-based grouping\n",
        "- **One TFRecord per subject** - Easier to manage than per-day files\n",
        "- **Includes all images** - Both evaluation and calibration data\n",
        "- **Proper labeling** - `is_evaluation` flag distinguishes image types\n",
        "\n",
        "**4. Usage:**\n",
        "```python\n",
        "# Generate TFRecords for all subjects (evaluation + calibration)\n",
        "create_tfrecords_for_all_subjects()\n",
        "\n",
        "# Test parsing\n",
        "test_parsing_with_sample_data()\n",
        "```\n",
        "\n",
        "**5. Output:**\n",
        "- Files: `{subject_id}_all_data.tfrecords` (15 files total)\n",
        "- Each file contains all images for that subject with proper `is_evaluation` labeling\n",
        "- Maintains same image preprocessing (black pixel removal + 36x144 resize)\n",
        "- Preserves gaze coordinate accuracy\n",
        "\n",
        "### Next Steps:\n",
        "1. Run `create_tfrecords_for_all_subjects()` to generate the simplified dataset\n",
        "2. Use `test_parsing_with_sample_data()` to verify the pipeline works correctly\n",
        "3. The resulting TFRecords can be used for training with both evaluation and calibration data properly labeled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY7utONxuGtC",
        "outputId": "380ff0ee-920a-4747-9c1f-090ac10e4783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated parsing function for MPIIGaze TFRecords by removing is_evaluation feature.\n",
            "Phase is now hardcoded to 2 (evaluation) as only evaluation images are processed.\n"
          ]
        }
      ],
      "source": [
        "## Model Evaluation: Updated TFRecords Parsing\n",
        "\n",
        "def parse_mpiigaze_tfrecords(element):\n",
        "    \"\"\"\n",
        "    Parse function adapted for our new MPIIGaze TFRecords schema (no is_evaluation).\n",
        "    Maps our schema to the format expected by the individual model analysis.\n",
        "    Since only evaluation images are included, we will assume phase=2 (evaluation).\n",
        "    \"\"\"\n",
        "    # Our TFRecords schema (excluding is_evaluation)\n",
        "    data_structure = {\n",
        "        'day': tf.io.FixedLenFeature([], tf.string),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.string),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        # Removed 'is_evaluation' from data_structure\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    # Extract data\n",
        "    raw_image = content['eye_img']\n",
        "    x_coord = content['x']\n",
        "    y_coord = content['y']\n",
        "    subject_id_str = content['subject_id']\n",
        "\n",
        "    # Since only evaluation images are included, we set phase=2 (evaluation)\n",
        "    phase = tf.constant(2, dtype=tf.int64)\n",
        "\n",
        "    # Parse image from raw bytes\n",
        "    image = tf.io.decode_raw(raw_image, tf.uint8)\n",
        "    image = tf.reshape(image, (36, 144))  # Our images are already 36x144\n",
        "    image = tf.expand_dims(image, -1)  # Add channel dimension\n",
        "\n",
        "    # Convert string subject_id to int64 for compatibility\n",
        "    # Create a hash of the subject_id string to get consistent integer IDs\n",
        "    subject_id_hash = tf.strings.to_hash_bucket_fast(subject_id_str, 2**31-1)\n",
        "    subject_id = tf.cast(subject_id_hash, tf.int64)\n",
        "\n",
        "    # Coordinates are already in 0-1 range, no scaling needed\n",
        "    coords = [x_coord, y_coord]\n",
        "\n",
        "    return image, phase, coords, subject_id\n",
        "\n",
        "print(\"Updated parsing function for MPIIGaze TFRecords by removing is_evaluation feature.\")\n",
        "print(\"Phase is now hardcoded to 2 (evaluation) as only evaluation images are processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0f02686a",
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476217b0-750b-43e6-c8df-166d27a07520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom layers and loss function loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "## Model Loading: Custom Layers and Architecture\n",
        "\n",
        "# Fixed constants from the original analysis\n",
        "MAX_TARGETS = 3020\n",
        "EMBEDDING_DIM = 200  # From the individual model run config\n",
        "RIDGE_REGULARIZATION = 0.001  # From the individual model run config\n",
        "BACKBONE = \"densenet\"  # From the individual model run config\n",
        "\n",
        "class SimpleTimeDistributed(keras.layers.Wrapper):\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.supports_masking = getattr(layer, 'supports_masking', False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not isinstance(input_shape, (tuple, list)) or len(input_shape) < 3:\n",
        "            raise ValueError(\n",
        "                \"`SimpleTimeDistributed` requires input with at least 3 dimensions\"\n",
        "            )\n",
        "        super().build((input_shape[0], *input_shape[2:]))\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        child_output_shape = self.layer.compute_output_shape((input_shape[0], *input_shape[2:]))\n",
        "        return (child_output_shape[0], input_shape[1], *child_output_shape[1:])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        time_steps = input_shape[1]\n",
        "\n",
        "        reshaped_inputs = ops.reshape(inputs, (-1, *input_shape[2:]))\n",
        "        outputs = self.layer.call(reshaped_inputs, training=training)\n",
        "        output_shape = ops.shape(outputs)\n",
        "\n",
        "        return ops.reshape(outputs, (batch_size, time_steps, *output_shape[1:]))\n",
        "\n",
        "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    def __init__(self, lambda_ridge, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        embeddings, coords, calibration_weights, cal_mask = inputs\n",
        "\n",
        "        embeddings = ops.cast(embeddings, \"float32\")\n",
        "        coords = ops.cast(coords, \"float32\")\n",
        "        calibration_weights = ops.cast(calibration_weights, \"float32\")\n",
        "        cal_mask = ops.cast(cal_mask, \"float32\")\n",
        "\n",
        "        w = ops.squeeze(calibration_weights, axis=-1)\n",
        "        w_masked = w * cal_mask\n",
        "        w_sqrt = ops.sqrt(w_masked + self.epsilon)\n",
        "        w_sqrt = ops.expand_dims(w_sqrt, -1)\n",
        "\n",
        "        cal_mask_expand = ops.expand_dims(cal_mask, -1)\n",
        "        X = embeddings * cal_mask_expand\n",
        "        X_weighted = X * w_sqrt\n",
        "        y_weighted = coords * w_sqrt * cal_mask_expand\n",
        "\n",
        "        X_t = ops.transpose(X_weighted, axes=[0, 2, 1])\n",
        "        X_t_X = ops.matmul(X_t, X_weighted)\n",
        "\n",
        "        identity_matrix = ops.cast(ops.eye(ops.shape(embeddings)[-1]), \"float32\")\n",
        "        lhs = X_t_X + self.lambda_ridge * identity_matrix\n",
        "        rhs = ops.matmul(X_t, y_weighted)\n",
        "\n",
        "        kernel = tf.linalg.solve(lhs, rhs)\n",
        "        output = ops.matmul(embeddings, kernel)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        unknown_embeddings_shape, _, _, _ = input_shapes\n",
        "        return unknown_embeddings_shape[:-1] + (2,)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\"lambda_ridge\": self.lambda_ridge, \"epsilon\": self.epsilon})\n",
        "        return config\n",
        "\n",
        "def normalized_weighted_euc_dist(y_true, y_pred):\n",
        "    \"\"\"Normalized weighted euclidean distance metric from original analysis.\"\"\"\n",
        "    x_weight = ops.convert_to_tensor([1.778, 1.0], dtype=\"float32\")\n",
        "\n",
        "    y_true_weighted = ops.multiply(x_weight, y_true)\n",
        "    y_pred_weighted = ops.multiply(x_weight, y_pred)\n",
        "\n",
        "    squared_diff = ops.square(y_pred_weighted - y_true_weighted)\n",
        "    squared_dist = ops.sum(squared_diff, axis=-1)\n",
        "    dist = ops.sqrt(squared_dist)\n",
        "\n",
        "    normalized_dist = ops.divide(dist, .0203992)\n",
        "    return normalized_dist\n",
        "\n",
        "print(\"Custom layers and loss function loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2c1a75fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "c8256b68-98a4-4704-c012-8ff03c04761f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model architecture...\n",
            "Model created successfully. Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"MaskedEyePredictionModel\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MaskedEyePredictionModel\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_All_Images    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3020\u001b[0m, \u001b[38;5;34m36\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Image_Embeddings    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3020\u001b[0m, \u001b[38;5;34m200\u001b[0m) │  \u001b[38;5;34m1,581,896\u001b[0m │ Input_All_Images… │\n",
              "│ (\u001b[38;5;33mSimpleTimeDistrib…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Input_All_Coords    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3020\u001b[0m, \u001b[38;5;34m2\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Calibration_Weights │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3020\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │        \u001b[38;5;34m201\u001b[0m │ Image_Embeddings… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Input_Calibration_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3020\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Regression          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3020\u001b[0m, \u001b[38;5;34m2\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ Image_Embeddings… │\n",
              "│ (\u001b[38;5;33mMaskedWeightedRid…\u001b[0m │                   │            │ Input_All_Coords… │\n",
              "│                     │                   │            │ Calibration_Weig… │\n",
              "│                     │                   │            │ Input_Calibratio… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_All_Images    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3020</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Image_Embeddings    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3020</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,581,896</span> │ Input_All_Images… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleTimeDistrib…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Input_All_Coords    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3020</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Calibration_Weights │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3020</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> │ Image_Embeddings… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Input_Calibration_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3020</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ Regression          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3020</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Image_Embeddings… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedWeightedRid…</span> │                   │            │ Input_All_Coords… │\n",
              "│                     │                   │            │ Calibration_Weig… │\n",
              "│                     │                   │            │ Input_Calibratio… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,582,097\u001b[0m (6.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,582,097</span> (6.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,574,257\u001b[0m (6.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,257</span> (6.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,840\u001b[0m (30.62 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,840</span> (30.62 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Model Architecture and Loading\n",
        "\n",
        "import keras_hub\n",
        "\n",
        "def create_dense_net_backbone():\n",
        "    DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "    return keras_hub.models.DenseNetBackbone(\n",
        "        stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "        image_shape=(36, 144, 1),\n",
        "    )\n",
        "\n",
        "def create_embedding_model(backbone_type):\n",
        "    image_shape = (36, 144, 1)\n",
        "    input_eyes = keras.layers.Input(shape=image_shape)\n",
        "    eyes_rescaled = keras.layers.Rescaling(scale=1./255)(input_eyes)\n",
        "\n",
        "    if backbone_type == \"densenet\":\n",
        "        backbone = create_dense_net_backbone()\n",
        "\n",
        "    backbone_encoder = backbone(eyes_rescaled)\n",
        "    flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "    eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "    embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "    return embedding_model\n",
        "\n",
        "def create_masked_model():\n",
        "    input_all_images = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 36, 144, 1),\n",
        "        name=\"Input_All_Images\"\n",
        "    )\n",
        "\n",
        "    input_all_coords = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 2),\n",
        "        name=\"Input_All_Coords\"\n",
        "    )\n",
        "\n",
        "    input_cal_mask = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS,),\n",
        "        name=\"Input_Calibration_Mask\",\n",
        "    )\n",
        "\n",
        "    embedding_model = create_embedding_model(BACKBONE)\n",
        "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\"Image_Embeddings\")(input_all_images)\n",
        "\n",
        "    calibration_weights = keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\"sigmoid\",\n",
        "        name=\"Calibration_Weights\"\n",
        "    )(all_embeddings)\n",
        "\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
        "        RIDGE_REGULARIZATION,\n",
        "        name=\"Regression\"\n",
        "    )(\n",
        "        [\n",
        "            all_embeddings,\n",
        "            input_all_coords,\n",
        "            calibration_weights,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\"MaskedEyePredictionModel\"\n",
        "    )\n",
        "\n",
        "    return full_model\n",
        "\n",
        "# Create and compile the model\n",
        "print(\"Creating model architecture...\")\n",
        "evaluation_model = create_masked_model()\n",
        "evaluation_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=normalized_weighted_euc_dist,\n",
        "    metrics=[normalized_weighted_euc_dist],\n",
        "    jit_compile=False\n",
        ")\n",
        "\n",
        "print(\"Model created successfully. Summary:\")\n",
        "evaluation_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Import the W&B API key from Colab secrets and set it as an environment variable\n",
        "WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "\n",
        "print(\"WANDB_API_KEY imported from Colab secrets and set as environment variable.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkvbxSeFMVDC",
        "outputId": "406c6e96-8ad7-4589-c41f-f04b894590b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WANDB_API_KEY imported from Colab secrets and set as environment variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utb52wL5LUF5",
        "outputId": "4327ecff-2624-4055-85b7-13306fb3a67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing W&B connection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Pre-trained model weights loaded successfully!\n",
            "Model ready for evaluation (pre-trained weights: ✓)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 175 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ],
      "source": [
        "## Load Pre-trained Model Weights\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Note: This section requires W&B credentials to download the model weights\n",
        "# For demonstration purposes, we'll show the code structure\n",
        "\n",
        "def load_pretrained_weights():\n",
        "    \"\"\"\n",
        "    Load pre-trained model weights from W&B.\n",
        "    This requires proper W&B authentication and project access.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # W&B configuration from individual model run\n",
        "        EXPERIMENT_ID = \"gf1zjs9v\"\n",
        "        ENTITY_NAME = \"vassar-cogsci-lab\"\n",
        "        PROJECT_NAME = \"eye-tracking-dense-full-data-set-single-eye\"\n",
        "\n",
        "        # Initialize W&B and download weights\n",
        "        print(\"Initializing W&B connection...\")\n",
        "        wandb.login()  # Requires API key\n",
        "        api = wandb.Api()\n",
        "        run = api.run(f\"{ENTITY_NAME}/{PROJECT_NAME}/{EXPERIMENT_ID}\")\n",
        "        run.file(\"full_model.weights.h5\").download(exist_ok=True)\n",
        "\n",
        "        # Load weights into model\n",
        "        evaluation_model.load_weights('full_model.weights.h5')\n",
        "        print(\"✓ Pre-trained model weights loaded successfully!\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading pre-trained weights: {e}\")\n",
        "        print(\"   Continuing with randomly initialized weights\")\n",
        "        return False\n",
        "\n",
        "# Attempt to load pre-trained weights\n",
        "weights_loaded = load_pretrained_weights()\n",
        "\n",
        "print(f\"Model ready for evaluation (pre-trained weights: {'✓' if weights_loaded else '✗'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLX9EXOiLUF5",
        "outputId": "7a424f62-3071-461e-8d91-ba21b2f716e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MPIIGaze dataset...\n",
            "Found 15 TFRecord files\n",
            "✓ Dataset loaded and parsed successfully\n",
            "Creating evaluation dataset with random calibration and target masks per subject...\n",
            "Counting subjects in evaluation dataset...\n",
            "✓ Evaluation dataset ready with 15 subjects\n"
          ]
        }
      ],
      "source": [
        "## Create Evaluation Dataset from Simplified TFRecords\n",
        "\n",
        "# MAX_TARGETS represents the maximum number of images per subject in a batch\n",
        "# This should be large enough to accommodate the subject with the most evaluation images.\n",
        "# Let's set it to a safe value, e.g., 3020, considering the max evaluation images are around 3000.\n",
        "MAX_TARGETS = 3020\n",
        "\n",
        "def load_mpiigaze_dataset():\n",
        "    \"\"\"\n",
        "    Load our simplified TFRecords dataset and process it for model evaluation.\n",
        "    \"\"\"\n",
        "    # Look for our TFRecord files\n",
        "    tfrecords_dir = Path(\"./mpiigaze_tfrecords_simplified\")\n",
        "    if not tfrecords_dir.exists():\n",
        "        print(f\"❌ TFRecords directory not found: {tfrecords_dir}\")\n",
        "        print(\"   Please run create_tfrecords_for_all_subjects() first to generate the dataset.\")\n",
        "        return None\n",
        "\n",
        "    # Get all TFRecord files\n",
        "    tfrecord_files = sorted(list(tfrecords_dir.glob(\"*.tfrecords\")))\n",
        "    if not tfrecord_files:\n",
        "        print(f\"❌ No TFRecord files found in {tfrecords_dir}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
        "\n",
        "    # Create dataset from all files\n",
        "    dataset = tf.data.TFRecordDataset([str(f) for f in tfrecord_files])\n",
        "\n",
        "    # Parse the records using our updated parsing function (which now assumes evaluation images)\n",
        "    parsed_dataset = dataset.map(parse_mpiigaze_tfrecords, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    print(\"✓ Dataset loaded and parsed successfully\")\n",
        "    return parsed_dataset\n",
        "\n",
        "\n",
        "def create_dataset_with_random_masks(dataset, num_calibration_points=20):\n",
        "    \"\"\"\n",
        "    Creates evaluation dataset from a source dataset (containing only evaluation images)\n",
        "    by generating random calibration and target masks for each subject's batch.\n",
        "    \"\"\"\n",
        "    def group_subject_data(subject_id, ds):\n",
        "        # Batch all of a subject's data together\n",
        "        return ds.batch(batch_size=MAX_TARGETS)\n",
        "\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, phase, coords, subject_id: subject_id,\n",
        "        reduce_func=group_subject_data,\n",
        "        window_size=MAX_TARGETS * 2 # Use a larger window size to group all images for a subject\n",
        "    )\n",
        "\n",
        "\n",
        "    def generate_masks_and_pad(images, phase, coords, subject_ids):\n",
        "        actual_batch_size = tf.shape(images)[0]\n",
        "\n",
        "        # Ensure there are enough images for calibration\n",
        "        # If not, use all available for calibration (this is a fallback and might not be ideal)\n",
        "        effective_num_calibration = tf.minimum(num_calibration_points, actual_batch_size)\n",
        "\n",
        "        # Generate random permutation of indices\n",
        "        permutation = tf.random.shuffle(tf.range(actual_batch_size))\n",
        "\n",
        "        # Select indices for calibration and target sets\n",
        "        cal_indices = tf.sort(permutation[:effective_num_calibration])\n",
        "        target_indices = tf.sort(permutation[effective_num_calibration:])\n",
        "\n",
        "        # Create calibration mask\n",
        "        cal_mask = tf.scatter_nd(\n",
        "            tf.expand_dims(cal_indices, axis=1),\n",
        "            tf.ones(tf.shape(cal_indices)[0], dtype=tf.int8),\n",
        "            shape=[actual_batch_size]\n",
        "        )\n",
        "\n",
        "        # Create target mask\n",
        "        target_mask = tf.scatter_nd(\n",
        "            tf.expand_dims(target_indices, axis=1),\n",
        "            tf.ones(tf.shape(target_indices)[0], dtype=tf.int8),\n",
        "            shape=[actual_batch_size]\n",
        "        )\n",
        "\n",
        "        # Pad data and masks to fixed size\n",
        "        padded_images = tf.pad(\n",
        "            tf.reshape(images, (-1, 36, 144, 1)),\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(\n",
        "            coords,\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n",
        "        )\n",
        "        padded_cal_mask = tf.pad(\n",
        "            cal_mask,\n",
        "            [[0, MAX_TARGETS - actual_batch_size]]\n",
        "        )\n",
        "        padded_target_mask = tf.pad(\n",
        "            target_mask,\n",
        "            [[0, MAX_TARGETS - actual_batch_size]]\n",
        "        )\n",
        "\n",
        "        # Ensure shapes\n",
        "        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(padded_cal_mask, [MAX_TARGETS])\n",
        "        padded_target_mask = tf.ensure_shape(padded_target_mask, [MAX_TARGETS])\n",
        "\n",
        "        # Return features tuple, labels, target_mask, and subject_ids\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, padded_target_mask, subject_ids[0]\n",
        "\n",
        "\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        generate_masks_and_pad,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return masked_dataset\n",
        "\n",
        "def prepare_model_inputs_for_evaluation(features, labels, target_mask, subject_ids):\n",
        "    \"\"\"Prepare inputs in the format expected by the model.\"\"\"\n",
        "    images, coords, cal_mask, target_mask_from_features = features # Unpack features tuple\n",
        "\n",
        "    # Explicitly ensure image shape and type for model input\n",
        "    images = tf.ensure_shape(images, [MAX_TARGETS, 36, 144, 1])\n",
        "    images = tf.cast(images, tf.float32) # Cast to float32\n",
        "    # Note: Normalization 0-1 is already done in parse_tfrecord_example\n",
        "\n",
        "\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": images,\n",
        "        \"Input_All_Coords\": coords,\n",
        "        \"Input_Calibration_Mask\": cal_mask,\n",
        "    }\n",
        "\n",
        "    return inputs, labels, target_mask, subject_ids\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading MPIIGaze dataset...\")\n",
        "mpiigaze_dataset = load_mpiigaze_dataset()\n",
        "\n",
        "if mpiigaze_dataset is not None:\n",
        "    print(\"Creating evaluation dataset with random calibration and target masks per subject...\")\n",
        "\n",
        "    # Create masked dataset using the random mask generation logic\n",
        "    evaluation_masked_dataset = create_dataset_with_random_masks(mpiigaze_dataset, num_calibration_points=20)\n",
        "\n",
        "    # Prepare for model input\n",
        "    evaluation_dataset = evaluation_masked_dataset.map(\n",
        "        lambda features_tuple, labels, target_mask, subject_id: prepare_model_inputs_for_evaluation(features_tuple, labels, target_mask, subject_id),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Count subjects\n",
        "    subject_count = 0\n",
        "    print(\"Counting subjects in evaluation dataset...\")\n",
        "    for _ in evaluation_dataset:\n",
        "        subject_count += 1\n",
        "\n",
        "    print(f\"✓ Evaluation dataset ready with {subject_count} subjects\")\n",
        "else:\n",
        "    print(\"❌ Failed to load dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oXZaRZkYPfp",
        "outputId": "da8133d8-03fe-437f-9f31-0ffee910458e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Input_All_Images': TensorSpec(shape=(3020, 36, 144, 1), dtype=tf.float32, name=None),\n",
              "  'Input_All_Coords': TensorSpec(shape=(3020, 2), dtype=tf.float32, name=None),\n",
              "  'Input_Calibration_Mask': TensorSpec(shape=(3020,), dtype=tf.int8, name=None)},\n",
              " TensorSpec(shape=(3020, 2), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(3020,), dtype=tf.int8, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_model.input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvTTvCbKvl3N",
        "outputId": "379769d1-3c4d-4c8a-97d5-c8e8bdb8e8e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(None, 3020, 36, 144, 1), (None, 3020, 2), (None, 3020)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYZWq8SFLUF6",
        "outputId": "7c7ccea2-538d-4446-b5bd-233e7d75e54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  Note: This evaluation will use randomly initialized weights since W&B credentials are not configured.\n",
            "    With proper pre-trained weights, the results would be meaningful.\n",
            "Running model evaluation on MPIIGaze dataset...\n",
            "This may take several minutes depending on dataset size...\n",
            "Generating predictions for all subjects...\n",
            "      7/Unknown \u001b[1m141s\u001b[0m 19s/step"
          ]
        }
      ],
      "source": [
        "## Run Model Evaluation and Calculate Results\n",
        "\n",
        "def run_mpiigaze_evaluation():\n",
        "    \"\"\"\n",
        "    Run the model evaluation on MPIIGaze dataset and calculate normalized euclidean distance.\n",
        "    \"\"\"\n",
        "    if 'evaluation_dataset' not in locals() and 'evaluation_dataset' not in globals():\n",
        "        print(\"❌ Evaluation dataset not ready. Please run the dataset creation cell first.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Running model evaluation on MPIIGaze dataset...\")\n",
        "    print(\"This may take several minutes depending on dataset size...\")\n",
        "\n",
        "    # Generate predictions for the entire dataset\n",
        "    print(\"Generating predictions for all subjects...\")\n",
        "    # evaluation_dataset yields (inputs, labels, target_mask, subject_id)\n",
        "    # We only need to pass the inputs dictionary to predict\n",
        "    predictions = evaluation_model.predict(\n",
        "        evaluation_dataset.map(lambda inputs, labels, target_mask, subject_id: inputs).batch(1),\n",
        "        verbose=1 # Keep verbose=1 to see prediction progress\n",
        "    )\n",
        "\n",
        "    print(\"\\nProcessing results and calculating errors per subject...\")\n",
        "    all_results = []\n",
        "\n",
        "    # Iterate through the dataset again to match predictions with ground truth and masks\n",
        "    # We use evaluation_dataset directly as it yields one subject's data per element\n",
        "    for i, (inputs, labels, target_mask, subject_id) in enumerate(evaluation_dataset):\n",
        "\n",
        "        # Extract subject info (subject_id is already a scalar tensor)\n",
        "        current_subject_id = int(subject_id.numpy())\n",
        "\n",
        "        # Get the corresponding predictions for this subject\n",
        "        # predictions is a single array, the i-th element corresponds to the i-th subject's batch\n",
        "        pred_batch = predictions[i]\n",
        "\n",
        "        # Get actual coordinates and target mask\n",
        "        actual_coords = labels.numpy()  # Labels is already the batch of coords for this subject\n",
        "        mask = target_mask.numpy().astype(bool)  # Target mask for this subject\n",
        "\n",
        "        # Filter by mask to get only evaluation points\n",
        "        pred_coords = pred_batch[mask]\n",
        "        actual_coords_filtered = actual_coords[mask]\n",
        "\n",
        "        # Calculate errors for each prediction\n",
        "        individual_errors = []\n",
        "        if actual_coords_filtered.shape[0] > 0: # Ensure there are evaluation points\n",
        "            for pred_coord, actual_coord in zip(pred_coords, actual_coords_filtered):\n",
        "                # Calculate error using the defined metric\n",
        "                # Need to reshape to (1, 2) as normalized_weighted_euc_dist expects batch dimension\n",
        "                error = normalized_weighted_euc_dist(\n",
        "                    np.array([actual_coord], dtype=np.float32),\n",
        "                    np.array([pred_coord], dtype=np.float32)\n",
        "                )\n",
        "                individual_errors.append(float(error.numpy()[0]))\n",
        "\n",
        "            # Calculate mean error for this subject\n",
        "            mean_error = np.mean(individual_errors)\n",
        "            all_results.append({\n",
        "                'subject_id': current_subject_id,\n",
        "                'mean_error': mean_error,\n",
        "                'n_evaluation_points': len(individual_errors),\n",
        "                'individual_errors': individual_errors # Store individual errors for potential further analysis\n",
        "            })\n",
        "        else:\n",
        "            print(f\"  Subject {current_subject_id} has no evaluation points in the target mask. Skipping error calculation.\")\n",
        "\n",
        "\n",
        "        if (i + 1) % 5 == 0: # Print progress every 5 subjects\n",
        "            print(f\"  Processed results for {i + 1} subjects...\")\n",
        "\n",
        "    print(\"\\nFinished processing results and calculating errors.\")\n",
        "    return all_results\n",
        "\n",
        "def analyze_results(results):\n",
        "    \"\"\"\n",
        "    Analyze the evaluation results and generate summary statistics.\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"❌ No results to analyze\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n=== MPIIGaze Model Evaluation Results ===\")\n",
        "    print(f\"Total subjects evaluated: {len(results)}\")\n",
        "\n",
        "    # Extract mean errors and evaluation points from subjects with results\n",
        "    mean_errors = [r['mean_error'] for r in results]\n",
        "    n_evaluation_points = [r['n_evaluation_points'] for r in results]\n",
        "    subject_ids_with_results = [r['subject_id'] for r in results]\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    overall_mean = np.mean(mean_errors)\n",
        "    overall_std = np.std(mean_errors)\n",
        "    overall_median = np.median(mean_errors)\n",
        "\n",
        "    print(f\"\\n--- Summary Statistics ---\")\n",
        "    print(f\"Overall mean error: {overall_mean:.4f} (normalized euclidean distance)\")\n",
        "    print(f\"Standard deviation: {overall_std:.4f}\")\n",
        "    print(f\"Median error: {overall_median:.4f}\")\n",
        "    print(f\"Min error: {np.min(mean_errors):.4f}\")\n",
        "    print(f\"Max error: {np.max(mean_errors):.4f}\")\n",
        "\n",
        "    print(f\"\\n--- Evaluation Points per Subject (for subjects with results) ---\")\n",
        "    print(f\"Mean evaluation points: {np.mean(n_evaluation_points):.1f}\")\n",
        "    print(f\"Min evaluation points: {np.min(n_evaluation_points)}\")\n",
        "    print(f\"Max evaluation points: {np.max(n_evaluation_points)}\")\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'subject_id': subject_ids_with_results,\n",
        "        'mean_error': mean_errors,\n",
        "        'n_evaluation_points': n_evaluation_points\n",
        "    })\n",
        "\n",
        "    return results_df, {\n",
        "        'overall_mean': overall_mean,\n",
        "        'overall_std': overall_std,\n",
        "        'overall_median': overall_median,\n",
        "        'n_subjects': len(results)\n",
        "    }\n",
        "\n",
        "# Run the evaluation (this will be slow without pre-trained weights)\n",
        "print(\"⚠️  Note: This evaluation will use randomly initialized weights since W&B credentials are not configured.\")\n",
        "print(\"    With proper pre-trained weights, the results would be meaningful.\")\n",
        "\n",
        "# Uncomment the following lines to run the actual evaluation:\n",
        "evaluation_results = run_mpiigaze_evaluation()\n",
        "\n",
        "if evaluation_results:\n",
        "    results_df, summary_stats = analyze_results(evaluation_results)\n",
        "    print(\"\\n✓ Evaluation completed successfully!\")\n",
        "else:\n",
        "    print(\"\\n❌ Evaluation failed\")\n",
        "\n",
        "print(\"\\n💡 To run the evaluation:\")\n",
        "print(\"   1. Configure W&B credentials to load pre-trained weights\")\n",
        "print(\"   2. Ensure the simplified TFRecords dataset is available\")\n",
        "print(\"   3. Uncomment the evaluation code above\")\n",
        "print(\"   4. Run this cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJSqzqj6LUF6"
      },
      "source": [
        "## Final Summary: Complete MPIIGaze Evaluation Pipeline\n",
        "\n",
        "### What We've Built\n",
        "\n",
        "This notebook now provides a complete pipeline for MPIIGaze evaluation:\n",
        "\n",
        "**1. Simplified TFRecords Creation:**\n",
        "- ✅ Streamlined data loading (5 core functions vs. original 15+)\n",
        "- ✅ Includes both evaluation and calibration images\n",
        "- ✅ Enhanced schema with `is_evaluation` boolean feature\n",
        "- ✅ Maintains black pixel removal with 36x144 output\n",
        "- ✅ One TFRecord per subject for easier management\n",
        "\n",
        "**2. Model Evaluation Integration:**\n",
        "- ✅ Compatible parsing function for new TFRecords schema\n",
        "- ✅ Pre-trained model architecture (DenseNet + masked ridge regression)\n",
        "- ✅ Evaluation methodology matching original analysis\n",
        "- ✅ 20 random calibration images per subject + all evaluation images\n",
        "- ✅ Normalized euclidean distance metric calculation\n",
        "\n",
        "**3. Complete Evaluation Workflow:**\n",
        "```python\n",
        "# 1. Generate simplified TFRecords dataset\n",
        "create_tfrecords_for_all_subjects()\n",
        "\n",
        "# 2. Load and evaluate with pre-trained model\n",
        "mpiigaze_dataset = load_mpiigaze_dataset()\n",
        "evaluation_dataset = create_calibration_subset_with_20_points(mpiigaze_dataset)\n",
        "evaluation_results = run_mpiigaze_evaluation()\n",
        "\n",
        "# 3. Analyze results\n",
        "results_df, summary_stats = analyze_results(evaluation_results)\n",
        "```\n",
        "\n",
        "**4. Key Features:**\n",
        "- **Maintains scientific rigor**: Same evaluation protocol as original study\n",
        "- **Includes calibration data**: All non-evaluation images labeled appropriately\n",
        "- **Simplified codebase**: Much cleaner and more maintainable\n",
        "- **Flexible evaluation**: Can easily modify number of calibration points\n",
        "- **Comprehensive analysis**: Per-subject and overall error metrics\n",
        "\n",
        "### Next Steps for Full Evaluation:\n",
        "\n",
        "1. **Configure W&B access** to download pre-trained model weights\n",
        "2. **Run TFRecords generation**: Execute `create_tfrecords_for_all_subjects()`\n",
        "3. **Execute evaluation**: Uncomment and run the evaluation code\n",
        "4. **Compare results**: Analyze performance against original benchmarks\n",
        "\n",
        "The pipeline is now ready for MPIIGaze evaluation with both evaluation and calibration images properly integrated."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}