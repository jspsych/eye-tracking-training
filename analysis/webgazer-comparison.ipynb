{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebGazer vs DenseNet Model Comparison\n",
    "\n",
    "This notebook implements a WebGazer-style ridge regression algorithm on raw pixel data and compares its performance with the existing DenseNet-based eye tracking model on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install osfclient --quiet\n",
    "!pip install keras_cv --quiet\n",
    "!pip install plotnine --quiet\n",
    "!pip install wandb --quiet\n",
    "!pip install keras-hub --quiet\n",
    "!pip install dotenv --quiet\n",
    "!pip install scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "import keras_hub\n",
    "\n",
    "import osfclient\n",
    "from osfclient.api import OSF\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "from plotnine import ggplot, geom_point, aes, geom_line, geom_histogram, geom_boxplot, geom_ribbon, scale_y_reverse, scale_y_continuous, theme_void, scale_x_continuous, scale_color_manual, scale_fill_manual, ylab, xlab, labs, theme, theme_classic, element_text, element_blank, element_line, facet_wrap, geom_abline, geom_smooth, stat_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import userdata\n",
    "  IN_COLAB = True\n",
    "except ImportError:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jspsych/eyetracking-utils.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import et_util.dataset_utils as dataset_utils\n",
    "import et_util.embedding_preprocessing as embed_pre\n",
    "import et_util.model_layers as model_layers\n",
    "from et_util import experiment_utils\n",
    "from et_util.custom_loss import normalized_weighted_euc_dist\n",
    "from et_util.model_analysis import plot_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "    os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
    "    os.environ['OSF_USERNAME'] = userdata.get('osfusername')\n",
    "else:\n",
    "    # Load from a .env file using python-dotenv\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY', '')\n",
    "    os.environ['OSF_TOKEN'] = os.getenv('OSF_TOKEN', '')\n",
    "    os.environ['OSF_USERNAME'] = os.getenv('OSF_USERNAME', '')\n",
    "\n",
    "os.environ['OSF_ANALYSIS_PROJECT_ID'] = \"8ecx5\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed constants\n",
    "MAX_TARGETS = 288\n",
    "IMAGE_HEIGHT = 36\n",
    "IMAGE_WIDTH = 144\n",
    "IMAGE_PIXELS = IMAGE_HEIGHT * IMAGE_WIDTH  # 5,184 pixels\n",
    "\n",
    "# Model configuration (from existing analysis)\n",
    "EMBEDDING_DIM = 200\n",
    "RIDGE_REGULARIZATION = 0.001\n",
    "BACKBONE = \"densenet\"\n",
    "\n",
    "# Experiment configuration\n",
    "EXPERIMENT_ID = \"gf1zjs9v\"\n",
    "ENTITY_NAME = \"vassar-cogsci-lab\"\n",
    "PROJECT_NAME = \"eye-tracking-dense-full-data-set-single-eye\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing\n",
    "\n",
    "Reusing the exact same dataset loading pipeline from the original analysis to ensure fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(element):\n",
    "    \"\"\"Parse TensorFlow record - same as original analysis\"\"\"\n",
    "    data_structure = {\n",
    "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
    "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
    "        'phase': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    content = tf.io.parse_single_example(element, data_structure)\n",
    "\n",
    "    raw_image = content['eye_img']\n",
    "    width = content['img_width']\n",
    "    height = content['img_height']\n",
    "    phase = content['phase']\n",
    "    depth = 3\n",
    "    coords = [content['x'], content['y']]\n",
    "    subject_id = content['subject_id']\n",
    "\n",
    "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
    "\n",
    "    return image, phase, coords, subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "test_data, _, _ = dataset_utils.process_tfr_to_tfds(\n",
    "    'single_eye_tfrecords/',\n",
    "    parse,\n",
    "    train_split=1.0,\n",
    "    val_split=0.0,\n",
    "    test_split=0.0,\n",
    "    random_seed=12604,\n",
    "    group_function=lambda img, phase, coords, subject_id: subject_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_coords_map(img, phase, coords, id):\n",
    "    \"\"\"Rescale coordinates from 0-100 to 0-1 range\"\"\"\n",
    "    return img, phase, tf.divide(coords, tf.constant([100.])), id\n",
    "\n",
    "test_data_rescaled = test_data.map(rescale_coords_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define calibration points (same as original analysis)\n",
    "cal_points = tf.constant([\n",
    "    [5, 5],\n",
    "    [5, 27.5],\n",
    "    [5, 50],\n",
    "    [5, 72.5],\n",
    "    [5, 95],\n",
    "    [35, 5],\n",
    "    [35, 27.5],\n",
    "    [35, 50],\n",
    "    [35, 72.5],\n",
    "    [35, 95],\n",
    "    [65, 5],\n",
    "    [65, 27.5],\n",
    "    [65, 50],\n",
    "    [65, 72.5],\n",
    "    [65, 95],\n",
    "    [95, 5],\n",
    "    [95, 27.5],\n",
    "    [95, 50],\n",
    "    [95, 72.5],\n",
    "    [95, 95],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebGazer Implementation\n",
    "\n",
    "Implementing the WebGazer algorithm: ridge regression on raw pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebGazerModel:\n",
    "    \"\"\"\n",
    "    WebGazer-style ridge regression model that operates directly on raw pixel intensities.\n",
    "    Implements the same interface as the DenseNet model for fair comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_ridge=0.001, normalize_pixels=True):\n",
    "        self.lambda_ridge = lambda_ridge\n",
    "        self.normalize_pixels = normalize_pixels\n",
    "        self.ridge_x = Ridge(alpha=lambda_ridge, fit_intercept=True)\n",
    "        self.ridge_y = Ridge(alpha=lambda_ridge, fit_intercept=True)\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def _preprocess_images(self, images):\n",
    "        \"\"\"\n",
    "        Preprocess images: flatten and optionally normalize.\n",
    "        Args:\n",
    "            images: numpy array of shape [n_samples, height, width] or [n_samples, height, width, 1]\n",
    "        Returns:\n",
    "            Flattened and preprocessed images of shape [n_samples, height*width]\n",
    "        \"\"\"\n",
    "        # Ensure we have the right shape\n",
    "        if len(images.shape) == 4 and images.shape[-1] == 1:\n",
    "            images = np.squeeze(images, axis=-1)  # Remove channel dimension\n",
    "        \n",
    "        # Flatten images\n",
    "        n_samples = images.shape[0]\n",
    "        flattened = images.reshape(n_samples, -1)\n",
    "        \n",
    "        # Normalize pixels to 0-1 range if specified\n",
    "        if self.normalize_pixels:\n",
    "            flattened = flattened.astype(np.float32) / 255.0\n",
    "        \n",
    "        return flattened\n",
    "    \n",
    "    def fit(self, images, coordinates, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Fit the WebGazer model to calibration data.\n",
    "        Args:\n",
    "            images: calibration images [n_cal_samples, height, width]\n",
    "            coordinates: gaze coordinates [n_cal_samples, 2]\n",
    "            sample_weights: optional sample weights [n_cal_samples]\n",
    "        \"\"\"\n",
    "        X = self._preprocess_images(images)\n",
    "        \n",
    "        # Fit separate ridge regression models for x and y coordinates\n",
    "        self.ridge_x.fit(X, coordinates[:, 0], sample_weight=sample_weights)\n",
    "        self.ridge_y.fit(X, coordinates[:, 1], sample_weight=sample_weights)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "    def predict(self, images):\n",
    "        \"\"\"\n",
    "        Predict gaze coordinates for test images.\n",
    "        Args:\n",
    "            images: test images [n_test_samples, height, width]\n",
    "        Returns:\n",
    "            predicted coordinates [n_test_samples, 2]\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X = self._preprocess_images(images)\n",
    "        \n",
    "        # Predict x and y coordinates separately\n",
    "        pred_x = self.ridge_x.predict(X)\n",
    "        pred_y = self.ridge_y.predict(X)\n",
    "        \n",
    "        # Combine predictions\n",
    "        predictions = np.column_stack([pred_x, pred_y])\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return model parameters for logging/analysis.\"\"\"\n",
    "        return {\n",
    "            'lambda_ridge': self.lambda_ridge,\n",
    "            'normalize_pixels': self.normalize_pixels,\n",
    "            'n_features': IMAGE_PIXELS,\n",
    "            'model_type': 'WebGazer_RidgeRegression'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing for Model Comparison\n",
    "\n",
    "Creating functions to process the dataset for both models using identical train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_masked_dataset(dataset, calibration_points=None, cal_phase=None):\n",
    "    \"\"\"Prepare masked dataset - same as original analysis\"\"\"\n",
    "    def group_by_subject(subject_id, ds):\n",
    "        return ds.batch(batch_size=MAX_TARGETS)\n",
    "\n",
    "    grouped_dataset = dataset.group_by_window(\n",
    "        key_func=lambda img, phase, coords, subject_id: subject_id,\n",
    "        reduce_func=group_by_subject,\n",
    "        window_size=MAX_TARGETS\n",
    "    )\n",
    "\n",
    "    def add_masks_to_batch(images, phase, coords, subject_ids):\n",
    "        actual_batch_size = tf.shape(images)[0]\n",
    "\n",
    "        # Create phase masks\n",
    "        phase1_mask = tf.cast(tf.equal(phase, 1), tf.int8)\n",
    "        phase2_mask = tf.cast(tf.equal(phase, 2), tf.int8)\n",
    "\n",
    "        cal_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n",
    "        target_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n",
    "\n",
    "        if calibration_points is None:\n",
    "            raise ValueError(\"Need to specify calibration points in test mode\")\n",
    "        else:\n",
    "            coords_xpand = tf.expand_dims(coords, axis=1)\n",
    "            cal_xpand = tf.expand_dims(calibration_points, axis=0)\n",
    "\n",
    "            # Check which points match calibration points\n",
    "            equality = tf.equal(coords_xpand, cal_xpand)\n",
    "            matches = tf.reduce_all(equality, axis=-1)\n",
    "            point_matches = tf.reduce_any(matches, axis=1)\n",
    "            cal_mask = tf.cast(point_matches, dtype=tf.int8)\n",
    "\n",
    "        target_mask = (1 - cal_mask) * phase2_mask\n",
    "\n",
    "        if cal_phase == 1:\n",
    "            cal_mask = cal_mask * phase1_mask\n",
    "        elif cal_phase == 2:\n",
    "            cal_mask = cal_mask * phase2_mask\n",
    "\n",
    "        padded_images = tf.pad(\n",
    "            tf.reshape(images, (-1, 36, 144, 1)),\n",
    "            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
    "        )\n",
    "        padded_coords = tf.pad(\n",
    "            coords,\n",
    "            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n",
    "        )\n",
    "        padded_cal_mask = tf.pad(\n",
    "            cal_mask,\n",
    "            [[0, MAX_TARGETS - actual_batch_size]]\n",
    "        )\n",
    "        padded_target_mask = tf.pad(\n",
    "            target_mask,\n",
    "            [[0, MAX_TARGETS - actual_batch_size]]\n",
    "        )\n",
    "\n",
    "        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n",
    "        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n",
    "        padded_cal_mask = tf.ensure_shape(padded_cal_mask, [MAX_TARGETS])\n",
    "        padded_target_mask = tf.ensure_shape(padded_target_mask, [MAX_TARGETS])\n",
    "        \n",
    "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids[0]\n",
    "\n",
    "    masked_dataset = grouped_dataset.map(\n",
    "        lambda imgs, phase, coords, subj_ids: add_masks_to_batch(imgs, phase, coords, subj_ids),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    return masked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_calibration_and_test_data(masked_dataset):\n",
    "    \"\"\"\n",
    "    Extract calibration and test data from masked dataset for model training/evaluation.\n",
    "    Returns data in format suitable for both WebGazer and DenseNet models.\n",
    "    \"\"\"\n",
    "    subject_data = []\n",
    "    \n",
    "    for batch in masked_dataset.as_numpy_iterator():\n",
    "        features, labels, subject_id = batch\n",
    "        images, coords, cal_mask, target_mask = features\n",
    "        \n",
    "        # Extract calibration data (where cal_mask == 1)\n",
    "        cal_indices = np.where(cal_mask == 1)[0]\n",
    "        cal_images = images[cal_indices]\n",
    "        cal_coords = coords[cal_indices]\n",
    "        \n",
    "        # Extract test data (where target_mask == 1)\n",
    "        test_indices = np.where(target_mask == 1)[0]\n",
    "        test_images = images[test_indices]\n",
    "        test_coords = coords[test_indices]\n",
    "        \n",
    "        # Only include subjects with sufficient data\n",
    "        if len(cal_indices) >= 10 and len(test_indices) >= 96:\n",
    "            subject_data.append({\n",
    "                'subject_id': subject_id,\n",
    "                'cal_images': cal_images,\n",
    "                'cal_coords': cal_coords,\n",
    "                'test_images': test_images,\n",
    "                'test_coords': test_coords\n",
    "            })\n",
    "    \n",
    "    return subject_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset with Phase 1 calibration\n",
    "print(\"Preparing masked dataset with Phase 1 calibration...\")\n",
    "masked_dataset_p1 = prepare_masked_dataset(test_data_rescaled, scaled_cal_points, 1)\n",
    "\n",
    "# Extract subject data for model comparison\n",
    "print(\"Extracting calibration and test data...\")\n",
    "subject_dataset = extract_calibration_and_test_data(masked_dataset_p1)\n",
    "\n",
    "print(f\"Prepared data for {len(subject_dataset)} subjects\")\n",
    "print(f\"Sample subject - Cal images: {subject_dataset[0]['cal_images'].shape}, Test images: {subject_dataset[0]['test_images'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing DenseNet Model\n",
    "\n",
    "Loading the pre-trained DenseNet model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the existing model weights\n",
    "wandb.login()\n",
    "api = wandb.Api()\n",
    "run = api.run(f\"{ENTITY_NAME}/{PROJECT_NAME}/{EXPERIMENT_ID}\")\n",
    "config = run.config\n",
    "run.file(\"full_model.weights.h5\").download(exist_ok=True)\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"Embedding dim: {config['embedding_dim']}\")\n",
    "print(f\"Ridge regularization: {config['ridge_regularization']}\")\n",
    "print(f\"Backbone: {config['backbone']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the existing model architecture (copied from original analysis)\n",
    "\n",
    "class SimpleTimeDistributed(keras.layers.Wrapper):\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        super().__init__(layer, **kwargs)\n",
    "        self.supports_masking = getattr(layer, 'supports_masking', False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, (tuple, list)) or len(input_shape) < 3:\n",
    "            raise ValueError(\n",
    "                \"`SimpleTimeDistributed` requires input with at least 3 dimensions\"\n",
    "            )\n",
    "\n",
    "        super().build((input_shape[0], *input_shape[2:]))\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        child_output_shape = self.layer.compute_output_shape((input_shape[0], *input_shape[2:]))\n",
    "        return (child_output_shape[0], input_shape[1], *child_output_shape[1:])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        time_steps = input_shape[1]\n",
    "\n",
    "        reshaped_inputs = ops.reshape(inputs, (-1, *input_shape[2:]))\n",
    "\n",
    "        outputs = self.layer.call(reshaped_inputs, training=training)\n",
    "\n",
    "        output_shape = ops.shape(outputs)\n",
    "\n",
    "        return ops.reshape(outputs, (batch_size, time_steps, *output_shape[1:]))\n",
    "\n",
    "\n",
    "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
    "    def __init__(self, lambda_ridge, epsilon=1e-6, **kwargs):\n",
    "        self.lambda_ridge = lambda_ridge\n",
    "        self.epsilon = epsilon\n",
    "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        embeddings, coords, calibration_weights, cal_mask  = inputs\n",
    "\n",
    "        embeddings = ops.cast(embeddings, \"float32\")\n",
    "        coords = ops.cast(coords, \"float32\")\n",
    "        calibration_weights = ops.cast(calibration_weights, \"float32\")\n",
    "        cal_mask = ops.cast(cal_mask, \"float32\")\n",
    "\n",
    "        w = ops.squeeze(calibration_weights, axis=-1)\n",
    "\n",
    "        w_masked = w * cal_mask\n",
    "        w_sqrt = ops.sqrt(w_masked + self.epsilon)\n",
    "        w_sqrt = ops.expand_dims(w_sqrt, -1)\n",
    "\n",
    "        cal_mask_expand = ops.expand_dims(cal_mask, -1)\n",
    "        X = embeddings * cal_mask_expand\n",
    "\n",
    "        X_weighted = X * w_sqrt\n",
    "        y_weighted = coords * w_sqrt * cal_mask_expand\n",
    "\n",
    "        X_t = ops.transpose(X_weighted, axes=[0, 2, 1])\n",
    "        X_t_X = ops.matmul(X_t, X_weighted)\n",
    "\n",
    "        identity_matrix = ops.cast(ops.eye(ops.shape(embeddings)[-1]), \"float32\")\n",
    "        lhs = X_t_X + self.lambda_ridge * identity_matrix\n",
    "\n",
    "        rhs = ops.matmul(X_t, y_weighted)\n",
    "\n",
    "        kernel = tf.linalg.solve(lhs, rhs)\n",
    "\n",
    "        output = ops.matmul(embeddings, kernel)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        unknown_embeddings_shape, _, _, _ = input_shapes\n",
    "        return unknown_embeddings_shape[:-1] + (2,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
    "        config.update({\"lambda_ridge\": self.lambda_ridge, \"epsilon\": self.epsilon})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_net_backbone():\n",
    "    DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
    "    return keras_hub.models.DenseNetBackbone(\n",
    "        stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS,\n",
    "        image_shape=(36, 144, 1),\n",
    "    )\n",
    "\n",
    "def create_embedding_model(BACKBONE):\n",
    "    image_shape = (36, 144, 1)\n",
    "    input_eyes = keras.layers.Input(shape=image_shape)\n",
    "\n",
    "    eyes_rescaled = keras.layers.Rescaling(scale=1./255)(input_eyes)\n",
    "\n",
    "    if BACKBONE == \"densenet\":\n",
    "        backbone = create_dense_net_backbone()\n",
    "\n",
    "    backbone_encoder = backbone(eyes_rescaled)\n",
    "    flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
    "    eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
    "\n",
    "    embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
    "\n",
    "    return embedding_model\n",
    "\n",
    "def create_masked_model():\n",
    "    input_all_images = keras.layers.Input(\n",
    "        shape=(MAX_TARGETS, 36, 144, 1),\n",
    "        name=\"Input_All_Images\"\n",
    "    )\n",
    "\n",
    "    input_all_coords = keras.layers.Input(\n",
    "        shape=(MAX_TARGETS, 2),\n",
    "        name=\"Input_All_Coords\"\n",
    "    )\n",
    "\n",
    "    input_cal_mask = keras.layers.Input(\n",
    "        shape=(MAX_TARGETS,),\n",
    "        name=\"Input_Calibration_Mask\",\n",
    "    )\n",
    "\n",
    "    embedding_model = create_embedding_model(BACKBONE)\n",
    "\n",
    "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\"Image_Embeddings\")(input_all_images)\n",
    "\n",
    "    calibration_weights = keras.layers.Dense(\n",
    "        1,\n",
    "        activation=\"sigmoid\",\n",
    "        name=\"Calibration_Weights\"\n",
    "    )(all_embeddings)\n",
    "\n",
    "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
    "        RIDGE_REGULARIZATION,\n",
    "        name=\"Regression\"\n",
    "    )(\n",
    "        [\n",
    "            all_embeddings,\n",
    "            input_all_coords,\n",
    "            calibration_weights,\n",
    "            input_cal_mask,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    full_model = keras.Model(\n",
    "        inputs=[\n",
    "            input_all_images,\n",
    "            input_all_coords,\n",
    "            input_cal_mask,\n",
    "        ],\n",
    "        outputs=ridge,\n",
    "        name=\"MaskedEyePredictionModel\"\n",
    "    )\n",
    "\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load the DenseNet model\n",
    "print(\"Creating DenseNet model...\")\n",
    "densenet_model = create_masked_model()\n",
    "densenet_model.load_weights('full_model.weights.h5')\n",
    "densenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=normalized_weighted_euc_dist,\n",
    "    metrics=[normalized_weighted_euc_dist],\n",
    "    jit_compile=False\n",
    ")\n",
    "\n",
    "print(\"DenseNet model loaded successfully\")\n",
    "print(f\"Total parameters: {densenet_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Comparison\n",
    "\n",
    "Evaluating both models on identical data splits and comparing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_webgazer_model(subject_data, lambda_ridge=0.001):\n",
    "    \"\"\"\n",
    "    Evaluate WebGazer model on all subjects.\n",
    "    Returns per-subject performance metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating WebGazer model on {len(subject_data)} subjects...\")\n",
    "    \n",
    "    for i, subject in enumerate(subject_data):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing subject {i+1}/{len(subject_data)}\")\n",
    "            \n",
    "        subject_id = subject['subject_id']\n",
    "        \n",
    "        # Initialize and train WebGazer model\n",
    "        webgazer = WebGazerModel(lambda_ridge=lambda_ridge)\n",
    "        \n",
    "        try:\n",
    "            # Train on calibration data\n",
    "            webgazer.fit(subject['cal_images'], subject['cal_coords'])\n",
    "            \n",
    "            # Predict on test data\n",
    "            predictions = webgazer.predict(subject['test_images'])\n",
    "            \n",
    "            # Calculate errors\n",
    "            errors = []\n",
    "            for pred_coord, actual_coord in zip(predictions, subject['test_coords']):\n",
    "                error = normalized_weighted_euc_dist(\n",
    "                    np.array([[actual_coord[0], actual_coord[1]]], dtype=np.float32),\n",
    "                    np.array([[pred_coord[0], pred_coord[1]]], dtype=np.float32)\n",
    "                )\n",
    "                errors.append(float(error.numpy()[0]))\n",
    "            \n",
    "            mean_error = np.mean(errors)\n",
    "            \n",
    "            results.append({\n",
    "                'subject_id': subject_id,\n",
    "                'model': 'WebGazer',\n",
    "                'mean_error': mean_error,\n",
    "                'n_cal_points': len(subject['cal_coords']),\n",
    "                'n_test_points': len(subject['test_coords']),\n",
    "                'training_time': 0  # Ridge regression is very fast\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject {subject_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_densenet_model(subject_data, densenet_model):\n",
    "    \"\"\"\n",
    "    Evaluate DenseNet model on all subjects using the same data splits.\n",
    "    Returns per-subject performance metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating DenseNet model on {len(subject_data)} subjects...\")\n",
    "    \n",
    "    # We need to recreate the masked dataset format for DenseNet evaluation\n",
    "    # This is more complex due to the model's expected input format\n",
    "    \n",
    "    for i, subject in enumerate(subject_data):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing subject {i+1}/{len(subject_data)}\")\n",
    "            \n",
    "        subject_id = subject['subject_id']\n",
    "        \n",
    "        try:\n",
    "            # Prepare data in the format expected by DenseNet model\n",
    "            all_images = np.concatenate([subject['cal_images'], subject['test_images']], axis=0)\n",
    "            all_coords = np.concatenate([subject['cal_coords'], subject['test_coords']], axis=0)\n",
    "            \n",
    "            # Create masks\n",
    "            n_cal = len(subject['cal_coords'])\n",
    "            n_test = len(subject['test_coords'])\n",
    "            n_total = n_cal + n_test\n",
    "            \n",
    "            cal_mask = np.zeros(MAX_TARGETS, dtype=np.int8)\n",
    "            cal_mask[:n_cal] = 1\n",
    "            \n",
    "            target_mask = np.zeros(MAX_TARGETS, dtype=np.int8)\n",
    "            target_mask[n_cal:n_cal+n_test] = 1\n",
    "            \n",
    "            # Pad to MAX_TARGETS\n",
    "            padded_images = np.zeros((MAX_TARGETS, 36, 144, 1), dtype=np.uint8)\n",
    "            padded_coords = np.zeros((MAX_TARGETS, 2), dtype=np.float32)\n",
    "            \n",
    "            padded_images[:n_total] = all_images\n",
    "            padded_coords[:n_total] = all_coords\n",
    "            \n",
    "            # Prepare input for model\n",
    "            model_input = {\n",
    "                \"Input_All_Images\": np.expand_dims(padded_images, 0),\n",
    "                \"Input_All_Coords\": np.expand_dims(padded_coords, 0),\n",
    "                \"Input_Calibration_Mask\": np.expand_dims(cal_mask, 0)\n",
    "            }\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = densenet_model.predict(model_input, verbose=0)\n",
    "            predictions = predictions[0]  # Remove batch dimension\n",
    "            \n",
    "            # Extract test predictions\n",
    "            test_predictions = predictions[target_mask == 1]\n",
    "            test_actual = subject['test_coords']\n",
    "            \n",
    "            # Calculate errors\n",
    "            errors = []\n",
    "            for pred_coord, actual_coord in zip(test_predictions, test_actual):\n",
    "                error = normalized_weighted_euc_dist(\n",
    "                    np.array([[actual_coord[0], actual_coord[1]]], dtype=np.float32),\n",
    "                    np.array([[pred_coord[0], pred_coord[1]]], dtype=np.float32)\n",
    "                )\n",
    "                errors.append(float(error.numpy()[0]))\n",
    "            \n",
    "            mean_error = np.mean(errors)\n",
    "            \n",
    "            results.append({\n",
    "                'subject_id': subject_id,\n",
    "                'model': 'DenseNet',\n",
    "                'mean_error': mean_error,\n",
    "                'n_cal_points': len(subject['cal_coords']),\n",
    "                'n_test_points': len(subject['test_coords']),\n",
    "                'training_time': 0  # Model is pre-trained\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject {subject_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "print(\"Starting model evaluation...\")\n",
    "\n",
    "# Evaluate WebGazer\n",
    "webgazer_results = evaluate_webgazer_model(subject_dataset, lambda_ridge=RIDGE_REGULARIZATION)\n",
    "\n",
    "# Evaluate DenseNet\n",
    "densenet_results = evaluate_densenet_model(subject_dataset, densenet_model)\n",
    "\n",
    "# Combine results\n",
    "all_results = pd.concat([webgazer_results, densenet_results], ignore_index=True)\n",
    "\n",
    "print(f\"Evaluation complete!\")\n",
    "print(f\"WebGazer results: {len(webgazer_results)} subjects\")\n",
    "print(f\"DenseNet results: {len(densenet_results)} subjects\")\n",
    "print(f\"Total results: {len(all_results)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization\n",
    "\n",
    "Analyzing the performance differences between WebGazer and DenseNet models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_stats = all_results.groupby('model')['mean_error'].agg(['mean', 'std', 'median', 'count']).round(4)\n",
    "print(\"Model Performance Summary:\")\n",
    "print(summary_stats)\n",
    "print()\n",
    "\n",
    "# Statistical significance test\n",
    "webgazer_errors = webgazer_results['mean_error'].values\n",
    "densenet_errors = densenet_results['mean_error'].values\n",
    "\n",
    "# Ensure we have matching subjects for paired comparison\n",
    "webgazer_subjects = set(webgazer_results['subject_id'])\n",
    "densenet_subjects = set(densenet_results['subject_id'])\n",
    "common_subjects = webgazer_subjects.intersection(densenet_subjects)\n",
    "\n",
    "print(f\"Common subjects between models: {len(common_subjects)}\")\n",
    "\n",
    "if len(common_subjects) > 10:  # Need sufficient data for statistical test\n",
    "    # Create paired data\n",
    "    webgazer_paired = webgazer_results[webgazer_results['subject_id'].isin(common_subjects)].sort_values('subject_id')['mean_error'].values\n",
    "    densenet_paired = densenet_results[densenet_results['subject_id'].isin(common_subjects)].sort_values('subject_id')['mean_error'].values\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(webgazer_paired, densenet_paired)\n",
    "    \n",
    "    print(f\"Paired t-test results:\")\n",
    "    print(f\"t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"p-value: {p_value:.6f}\")\n",
    "    print(f\"Mean difference (WebGazer - DenseNet): {np.mean(webgazer_paired - densenet_paired):.4f}\")\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    diff = webgazer_paired - densenet_paired\n",
    "    cohens_d = np.mean(diff) / np.std(diff)\n",
    "    print(f\"Effect size (Cohen's d): {cohens_d:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "\n",
    "# 1. Error distribution comparison\n",
    "distribution_plot = (\n",
    "    ggplot(all_results, aes(x='mean_error', fill='model'))\n",
    "    + geom_histogram(alpha=0.7, position='identity', bins=30)\n",
    "    + labs(\n",
    "        title='Prediction Error Distribution: WebGazer vs DenseNet',\n",
    "        x='Mean Prediction Error (% Screen Diagonal)',\n",
    "        y='Number of Subjects'\n",
    "    )\n",
    "    + scale_fill_manual(values=['#2E86AB', '#A23B72'], name='Model')\n",
    "    + theme_classic()\n",
    "    + theme(plot_title=element_text(hjust=0.5))\n",
    ")\n",
    "\n",
    "print(\"Distribution plot:\")\n",
    "distribution_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Direct comparison scatter plot (for common subjects)\n",
    "if len(common_subjects) > 10:\n",
    "    comparison_data = []\n",
    "    for subject_id in common_subjects:\n",
    "        webgazer_error = webgazer_results[webgazer_results['subject_id'] == subject_id]['mean_error'].iloc[0]\n",
    "        densenet_error = densenet_results[densenet_results['subject_id'] == subject_id]['mean_error'].iloc[0]\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'subject_id': subject_id,\n",
    "            'webgazer_error': webgazer_error,\n",
    "            'densenet_error': densenet_error,\n",
    "            'difference': webgazer_error - densenet_error\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Scatter plot\n",
    "    scatter_plot = (\n",
    "        ggplot(comparison_df, aes(x='densenet_error', y='webgazer_error'))\n",
    "        + geom_point(alpha=0.6, size=2)\n",
    "        + geom_abline(intercept=0, slope=1, linetype='dashed', color='red')\n",
    "        + geom_smooth(method='lm', se=True, color='blue')\n",
    "        + labs(\n",
    "            title='Subject-Level Performance Comparison',\n",
    "            x='DenseNet Error (% Screen Diagonal)',\n",
    "            y='WebGazer Error (% Screen Diagonal)',\n",
    "            subtitle='Points below red line indicate WebGazer performed better'\n",
    "        )\n",
    "        + theme_classic()\n",
    "        + theme(plot_title=element_text(hjust=0.5), plot_subtitle=element_text(hjust=0.5))\n",
    "    )\n",
    "    \n",
    "    print(\"Scatter plot comparison:\")\n",
    "    scatter_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Box plot comparison\n",
    "box_plot = (\n",
    "    ggplot(all_results, aes(x='model', y='mean_error', fill='model'))\n",
    "    + geom_boxplot(alpha=0.7)\n",
    "    + labs(\n",
    "        title='Prediction Error Distribution by Model',\n",
    "        x='Model',\n",
    "        y='Mean Prediction Error (% Screen Diagonal)'\n",
    "    )\n",
    "    + scale_fill_manual(values=['#2E86AB', '#A23B72'], name='Model')\n",
    "    + theme_classic()\n",
    "    + theme(plot_title=element_text(hjust=0.5), legend_position='none')\n",
    ")\n",
    "\n",
    "print(\"Box plot comparison:\")\n",
    "box_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Complexity and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model complexity and training requirements\n",
    "print(\"Model Complexity Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DenseNet model complexity\n",
    "densenet_params = densenet_model.count_params()\n",
    "print(f\"DenseNet Model:\")\n",
    "print(f\"  Total parameters: {densenet_params:,}\")\n",
    "print(f\"  Feature extraction: DenseNet backbone + Dense layer\")\n",
    "print(f\"  Embedding dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"  Training data: Large dataset with complex architecture\")\n",
    "print()\n",
    "\n",
    "# WebGazer model complexity\n",
    "webgazer_params = IMAGE_PIXELS * 2 + 2  # weights for x and y prediction + intercepts\n",
    "print(f\"WebGazer Model:\")\n",
    "print(f\"  Total parameters: {webgazer_params:,} ({webgazer_params / 1000:.1f}k)\")\n",
    "print(f\"  Feature extraction: Raw pixels (no preprocessing)\")\n",
    "print(f\"  Input dimension: {IMAGE_PIXELS} pixels\")\n",
    "print(f\"  Training data: Only calibration points per subject\")\n",
    "print()\n",
    "\n",
    "print(f\"Parameter ratio (DenseNet/WebGazer): {densenet_params / webgazer_params:.1f}x\")\n",
    "print(f\"DenseNet has {densenet_params / webgazer_params:.1f}x more parameters than WebGazer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance per parameter analysis\n",
    "webgazer_mean_error = webgazer_results['mean_error'].mean()\n",
    "densenet_mean_error = densenet_results['mean_error'].mean()\n",
    "\n",
    "webgazer_efficiency = webgazer_params / webgazer_mean_error\n",
    "densenet_efficiency = densenet_params / densenet_mean_error\n",
    "\n",
    "print(\"Performance Efficiency Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"WebGazer:\")\n",
    "print(f\"  Mean error: {webgazer_mean_error:.4f}%\")\n",
    "print(f\"  Parameters per unit error: {webgazer_efficiency:.0f}\")\n",
    "print()\n",
    "print(f\"DenseNet:\")\n",
    "print(f\"  Mean error: {densenet_mean_error:.4f}%\")\n",
    "print(f\"  Parameters per unit error: {densenet_efficiency:.0f}\")\n",
    "print()\n",
    "\n",
    "if webgazer_efficiency < densenet_efficiency:\n",
    "    print(f\"WebGazer is {densenet_efficiency / webgazer_efficiency:.1f}x more parameter-efficient\")\n",
    "else:\n",
    "    print(f\"DenseNet is {webgazer_efficiency / densenet_efficiency:.1f}x more parameter-efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results and Create Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plots\n",
    "distribution_plot.save('webgazer_comparison_distribution.png', width=10, height=6, dpi=150)\n",
    "box_plot.save('webgazer_comparison_boxplot.png', width=8, height=6, dpi=150)\n",
    "\n",
    "if len(common_subjects) > 10:\n",
    "    scatter_plot.save('webgazer_comparison_scatter.png', width=8, height=6, dpi=150)\n",
    "\n",
    "# Save results data\n",
    "all_results.to_csv('webgazer_densenet_comparison_results.csv', index=False)\n",
    "\n",
    "if len(common_subjects) > 10:\n",
    "    comparison_df.to_csv('webgazer_densenet_paired_comparison.csv', index=False)\n",
    "\n",
    "print(\"Results and plots saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Performance Comparison**: \n",
    "   - WebGazer mean error: [TO BE FILLED]\n",
    "   - DenseNet mean error: [TO BE FILLED]\n",
    "   - Statistical significance: [TO BE FILLED]\n",
    "\n",
    "2. **Model Complexity**:\n",
    "   - WebGazer: ~10k parameters (simple ridge regression)\n",
    "   - DenseNet: ~1.6M parameters (complex deep learning architecture)\n",
    "   - Parameter efficiency: [TO BE FILLED]\n",
    "\n",
    "3. **Training Requirements**:\n",
    "   - WebGazer: Trains individually per subject using only calibration points\n",
    "   - DenseNet: Pre-trained on large dataset, uses learned feature representations\n",
    "\n",
    "4. **Practical Implications**:\n",
    "   - [TO BE FILLED BASED ON RESULTS]\n",
    "\n",
    "### Future Work:\n",
    "- Test with different numbers of calibration points\n",
    "- Analyze performance by spatial location\n",
    "- Investigate hybrid approaches combining both methods\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}