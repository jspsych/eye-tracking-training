{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Clone repository if running on Google Colab\n",
    "import os\n",
    "\n",
    "# IMPORTANT: Set Keras backend BEFORE any keras imports (including et_util)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository to get et_util package\n",
    "    if not os.path.exists('eye-tracking-training'):\n",
    "        !git clone https://github.com/jspsych/eye-tracking-training.git\n",
    "    \n",
    "    # Add the repository to Python path so we can import et_util\n",
    "    import sys\n",
    "    repo_path = '/content/eye-tracking-training'\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.insert(0, repo_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77auPa5eUbA3"
   },
   "source": "# WebGazer vs Our Model Comparison\n\nThis notebook implements a WebGazer-style ridge regression algorithm on raw pixel data and compares its performance with our deep learning-based eye tracking model on the same dataset. Our model supports multiple backbone architectures (DenseNet, ViT, or Hybrid CNN-Transformer)."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRCHGEzzUbA5"
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6KNP1DOUbA5"
   },
   "outputs": [],
   "source": "!pip install osfclient --quiet\n!pip install plotnine --quiet\n!pip install wandb --quiet\n!pip install keras-hub --quiet\n!pip install python-dotenv --quiet\n!pip install scikit-learn --quiet"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6tc7m_nUbA6"
   },
   "outputs": [],
   "source": "import os\nimport gc\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom scipy import stats\n\nimport keras\nfrom keras import ops\nimport keras_hub\n\nimport osfclient\nfrom osfclient.api import OSF\n\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger\n\nfrom plotnine import ggplot, geom_point, aes, geom_line, geom_histogram, geom_boxplot, geom_ribbon, scale_y_reverse, scale_y_continuous, theme_void, scale_x_continuous, scale_color_manual, scale_fill_manual, ylab, xlab, labs, theme, theme_classic, element_text, element_blank, element_line, facet_wrap, geom_abline, geom_smooth, stat_smooth"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwrV3f9kUbA6"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import userdata\n",
    "  IN_COLAB = True\n",
    "except ImportError:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLqGhPCxUbA7"
   },
   "outputs": [],
   "source": "import et_util.dataset_utils as dataset_utils\nfrom et_util.dataset_utils import parse_single_eye_tfrecord_with_phase as parse, rescale_coords_map_with_phase as rescale_coords_map\nfrom et_util.custom_loss import normalized_weighted_euc_dist\nfrom et_util.custom_layers import (\n    SimpleTimeDistributed,\n    MaskedWeightedRidgeRegressionLayer,\n    MaskInspectorLayer,\n    ResidualBlock,\n    AddPositionalEmbedding,\n)\nfrom et_util.model_analysis import plot_model_performance\nfrom et_util.inference_utils import create_flexible_inference_model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqLrsSCRUbA7"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "    os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
    "    os.environ['OSF_USERNAME'] = userdata.get('osfusername')\n",
    "else:\n",
    "    # Load from a .env file using python-dotenv\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY', '')\n",
    "    os.environ['OSF_TOKEN'] = os.getenv('OSF_TOKEN', '')\n",
    "    os.environ['OSF_USERNAME'] = os.getenv('OSF_USERNAME', '')\n",
    "\n",
    "os.environ['OSF_ANALYSIS_PROJECT_ID'] = \"8ecx5\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxC1xwfuUbA7"
   },
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3_sDnLhUbA7"
   },
   "outputs": [],
   "source": "# Fixed constants\nMAX_TARGETS = 288\nIMAGE_HEIGHT = 36\nIMAGE_WIDTH = 144\nIMAGE_PIXELS = IMAGE_HEIGHT * IMAGE_WIDTH  # 5,184 pixels\n\n# Model configuration defaults (will be overridden by W&B config)\nEMBEDDING_DIM = 200\nRIDGE_REGULARIZATION = 0.001\n\n# Experiment configuration - updated to match serializable model project\nEXPERIMENT_ID = \"mck7rj6y\"\nENTITY_NAME = \"vassar-cogsci-lab\"\nPROJECT_NAME = \"eye-tracking-serializable-model\"\n\n# Model display name for plots and results\nOUR_MODEL_NAME = \"Our Model\""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3BpBVFaUbA7"
   },
   "source": [
    "## Dataset Loading and Preprocessing\n",
    "\n",
    "Reusing the exact same dataset loading pipeline from the original analysis to ensure fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elvf34ouUbA8",
    "outputId": "eb6e5cf9-44e3-43da-9c7c-23613dae45ea"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100% 675M/675M [00:04<00:00, 160Mbytes/s]\n"
     ]
    }
   ],
   "source": [
    "!osf -p uf2sh fetch single_eye_tfrecords.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-jfET9WUbA8"
   },
   "outputs": [],
   "source": [
    "!mkdir single_eye_tfrecords\n",
    "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKEEFtxyUbA8"
   },
   "outputs": [],
   "source": "# parse function already imported from et_util.dataset_utils at the top"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4Qyl67oUbA8"
   },
   "outputs": [],
   "source": "# Load the dataset with phase-aware parsing\ntest_data, _, _ = dataset_utils.process_tfr_to_tfds(\n    'single_eye_tfrecords/',\n    parse,\n    train_split=1.0,\n    val_split=0.0,\n    test_split=0.0,\n    random_seed=12604,\n    group_function=lambda img, phase, coords, subject_id: subject_id\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGWuHG94UbA8"
   },
   "outputs": [],
   "source": "# Rescale coordinates from 0-100 to 0-1 range using the imported function\ntest_data_rescaled = test_data.map(rescale_coords_map)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltsIB8rNUbA8"
   },
   "outputs": [],
   "source": [
    "# Define calibration points (same as original analysis)\n",
    "cal_points = tf.constant([\n",
    "    [5, 5],\n",
    "    [5, 27.5],\n",
    "    [5, 50],\n",
    "    [5, 72.5],\n",
    "    [5, 95],\n",
    "    [35, 5],\n",
    "    [35, 27.5],\n",
    "    [35, 50],\n",
    "    [35, 72.5],\n",
    "    [35, 95],\n",
    "    [65, 5],\n",
    "    [65, 27.5],\n",
    "    [65, 50],\n",
    "    [65, 72.5],\n",
    "    [65, 95],\n",
    "    [95, 5],\n",
    "    [95, 27.5],\n",
    "    [95, 50],\n",
    "    [95, 72.5],\n",
    "    [95, 95],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BSnyRPzUbA8"
   },
   "source": [
    "## WebGazer Implementation\n",
    "\n",
    "Implementing the WebGazer algorithm: ridge regression on raw pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25Eslg-lUbA8"
   },
   "outputs": [],
   "source": [
    "class WebGazerModel:\n",
    "    \"\"\"\n",
    "    WebGazer-style ridge regression model that operates directly on raw pixel intensities.\n",
    "    Implements the same interface as the DenseNet model for fair comparison.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambda_ridge=0.001, normalize_pixels=True):\n",
    "        self.lambda_ridge = lambda_ridge\n",
    "        self.normalize_pixels = normalize_pixels\n",
    "        self.ridge_x = Ridge(alpha=lambda_ridge, fit_intercept=True)\n",
    "        self.ridge_y = Ridge(alpha=lambda_ridge, fit_intercept=True)\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _preprocess_images(self, images):\n",
    "        \"\"\"\n",
    "        Preprocess images: flatten and optionally normalize.\n",
    "        Args:\n",
    "            images: numpy array of shape [n_samples, height, width] or [n_samples, height, width, 1]\n",
    "        Returns:\n",
    "            Flattened and preprocessed images of shape [n_samples, height*width]\n",
    "        \"\"\"\n",
    "        # Ensure we have the right shape\n",
    "        if len(images.shape) == 4 and images.shape[-1] == 1:\n",
    "            images = np.squeeze(images, axis=-1)  # Remove channel dimension\n",
    "\n",
    "        # Flatten images\n",
    "        n_samples = images.shape[0]\n",
    "        flattened = images.reshape(n_samples, -1)\n",
    "\n",
    "        # Normalize pixels to 0-1 range if specified\n",
    "        if self.normalize_pixels:\n",
    "            flattened = flattened.astype(np.float32) / 255.0\n",
    "\n",
    "        return flattened\n",
    "\n",
    "    def fit(self, images, coordinates, sample_weights=None):\n",
    "        \"\"\"\n",
    "        Fit the WebGazer model to calibration data.\n",
    "        Args:\n",
    "            images: calibration images [n_cal_samples, height, width]\n",
    "            coordinates: gaze coordinates [n_cal_samples, 2]\n",
    "            sample_weights: optional sample weights [n_cal_samples]\n",
    "        \"\"\"\n",
    "        X = self._preprocess_images(images)\n",
    "\n",
    "        # Fit separate ridge regression models for x and y coordinates\n",
    "        self.ridge_x.fit(X, coordinates[:, 0], sample_weight=sample_weights)\n",
    "        self.ridge_y.fit(X, coordinates[:, 1], sample_weight=sample_weights)\n",
    "\n",
    "        self.is_fitted = True\n",
    "\n",
    "    def predict(self, images):\n",
    "        \"\"\"\n",
    "        Predict gaze coordinates for test images.\n",
    "        Args:\n",
    "            images: test images [n_test_samples, height, width]\n",
    "        Returns:\n",
    "            predicted coordinates [n_test_samples, 2]\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "\n",
    "        X = self._preprocess_images(images)\n",
    "\n",
    "        # Predict x and y coordinates separately\n",
    "        pred_x = self.ridge_x.predict(X)\n",
    "        pred_y = self.ridge_y.predict(X)\n",
    "\n",
    "        # Combine predictions\n",
    "        predictions = np.column_stack([pred_x, pred_y])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Return model parameters for logging/analysis.\"\"\"\n",
    "        return {\n",
    "            'lambda_ridge': self.lambda_ridge,\n",
    "            'normalize_pixels': self.normalize_pixels,\n",
    "            'n_features': IMAGE_PIXELS,\n",
    "            'model_type': 'WebGazer_RidgeRegression'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuNHoJ9iUbA9"
   },
   "source": [
    "## Data Processing for Model Comparison\n",
    "\n",
    "Creating functions to process the dataset for both models using identical train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaxq4eXeUbA9"
   },
   "outputs": [],
   "source": "def prepare_masked_dataset(dataset, calibration_points=None, cal_phase=None):\n    \"\"\"Prepare masked dataset with phase-aware calibration - matching analysis notebook.\n    \n    Note: With the flexible inference model, tf.ensure_shape() is no longer required\n    since the model can handle dynamic shapes. We still pad to MAX_TARGETS for \n    consistent batching, but the model will work with any number of points.\n    \"\"\"\n    def group_by_subject(subject_id, ds):\n        return ds.batch(batch_size=MAX_TARGETS)\n\n    grouped_dataset = dataset.group_by_window(\n        key_func=lambda img, phase, coords, subject_id: subject_id,\n        reduce_func=group_by_subject,\n        window_size=MAX_TARGETS\n    )\n\n    def add_masks_to_batch(images, phase, coords, subject_ids):\n        actual_batch_size = tf.shape(images)[0]\n\n        # Create phase masks\n        phase1_mask = tf.cast(tf.equal(phase, 1), tf.int8)\n        phase2_mask = tf.cast(tf.equal(phase, 2), tf.int8)\n\n        cal_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n        target_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n\n        if calibration_points is None:\n            raise ValueError(\"Need to specify calibration points in test mode\")\n        else:\n            coords_xpand = tf.expand_dims(coords, axis=1)\n            cal_xpand = tf.expand_dims(calibration_points, axis=0)\n\n            # Check which points match calibration points\n            equality = tf.equal(coords_xpand, cal_xpand)\n            matches = tf.reduce_all(equality, axis=-1)\n            point_matches = tf.reduce_any(matches, axis=1)\n            cal_mask = tf.cast(point_matches, dtype=tf.int8)\n\n        target_mask = (1 - cal_mask) * phase2_mask\n\n        if cal_phase == 1:\n            cal_mask = cal_mask * phase1_mask\n        elif cal_phase == 2:\n            cal_mask = cal_mask * phase2_mask\n\n        padded_images = tf.pad(\n            tf.reshape(images, (-1, 36, 144, 1)),\n            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n        )\n        padded_coords = tf.pad(\n            coords,\n            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n        )\n        padded_cal_mask = tf.pad(\n            cal_mask,\n            [[0, MAX_TARGETS - actual_batch_size]]\n        )\n        padded_target_mask = tf.pad(\n            target_mask,\n            [[0, MAX_TARGETS - actual_batch_size]]\n        )\n\n        # Note: tf.ensure_shape() removed - flexible model handles dynamic shapes\n        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids[0]\n\n    masked_dataset = grouped_dataset.map(\n        lambda imgs, phase, coords, subj_ids: add_masks_to_batch(imgs, phase, coords, subj_ids),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n\n    return masked_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0zwAL3uUbA9"
   },
   "outputs": [],
   "source": "def prepare_model_inputs(features, labels, subject_ids):\n    \"\"\"\n    Prepare inputs in the format expected by the gaze model.\n    Used with tf.data.Dataset.map() for efficient batched prediction.\n    \"\"\"\n    images, coords, cal_mask, target_mask = features\n\n    inputs = {\n        \"Input_All_Images\": images,\n        \"Input_All_Coords\": coords,\n        \"Input_Calibration_Mask\": cal_mask,\n        \"Input_Target_Mask\": target_mask\n    }\n\n    return inputs, labels, target_mask, subject_ids\n\n\ndef extract_calibration_and_test_data_for_webgazer(masked_dataset):\n    \"\"\"\n    Extract calibration and test data from masked dataset for WebGazer evaluation.\n    WebGazer uses sklearn which requires numpy arrays.\n    \"\"\"\n    subject_data = []\n\n    for batch in masked_dataset.as_numpy_iterator():\n        features, labels, subject_id = batch\n        images, coords, cal_mask, target_mask = features\n\n        # Extract calibration data (where cal_mask == 1)\n        cal_indices = np.where(cal_mask == 1)[0]\n        cal_images = images[cal_indices]\n        cal_coords = coords[cal_indices]\n\n        # Extract test data (where target_mask == 1)\n        test_indices = np.where(target_mask == 1)[0]\n        test_images = images[test_indices]\n        test_coords = coords[test_indices]\n\n        # Only include subjects with sufficient data\n        if len(cal_indices) >= 10 and len(test_indices) >= 96:\n            subject_data.append({\n                'subject_id': subject_id,\n                'cal_images': cal_images,\n                'cal_coords': cal_coords,\n                'test_images': test_images,\n                'test_coords': test_coords\n            })\n\n    return subject_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN9q0wV0UbA9",
    "outputId": "9c2c8160-ff42-4c40-82d4-aded9f6069ad"
   },
   "outputs": [],
   "source": "# Prepare the dataset with Phase 1 calibration\nprint(\"Preparing masked dataset with Phase 1 calibration...\")\nmasked_dataset_p1 = prepare_masked_dataset(test_data_rescaled, scaled_cal_points, 1)\n\n# Create gaze model dataset (memory efficient - no caching)\ngaze_model_dataset = masked_dataset_p1.map(\n    prepare_model_inputs,\n    num_parallel_calls=tf.data.AUTOTUNE\n)\n\nprint(\"Dataset prepared for streaming evaluation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8Sbm5LVUbA9"
   },
   "source": "## Load Our Trained Eye Tracking Model\n\nLoading the pre-trained gaze prediction model for comparison. This model uses a deep learning backbone (DenseNet, ViT, or Hybrid CNN-Transformer) with ridge regression for subject-specific calibration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvsliBgIUbA9",
    "outputId": "4d0c57ec-3a03-472b-eb5c-3689b1bf0e95"
   },
   "outputs": [],
   "source": "# Download the model from W&B\nwandb.login()\napi = wandb.Api()\nrun = api.run(f\"{ENTITY_NAME}/{PROJECT_NAME}/{EXPERIMENT_ID}\")\nconfig = run.config\n\n# Download the full serialized model (new format)\nrun.file(\"full_model.keras\").download(exist_ok=True)\n\n# Get backbone type from config for display\nbackbone_type = config.get('backbone', 'unknown')\n\nprint(\"Model configuration:\")\nprint(f\"Backbone: {backbone_type}\")\nprint(f\"Embedding dim: {config['embedding_dim']}\")\nprint(f\"Ridge regularization: {config['ridge_regularization']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFo4UzBVUbA-",
    "outputId": "581afd5d-e2f5-4d14-c2bd-90801139f059"
   },
   "outputs": [],
   "source": "# Load the model using flexible inference utilities\n# This rewires the trained model with flexible input shapes (None instead of 288)\n# All trained layers and weights are preserved\ngaze_model = create_flexible_inference_model('full_model.keras')\n\nprint(f\"{OUR_MODEL_NAME} loaded successfully with flexible inference (backbone: {backbone_type})\")\ngaze_model.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiBoexwsUbA-"
   },
   "source": "## Model Evaluation and Comparison\n\nEvaluating both models on identical data splits and comparing performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Csq8Fpu3UbA-"
   },
   "outputs": [],
   "source": "def evaluate_webgazer_model(subject_data, lambda_ridge=0.001):\n    \"\"\"\n    Evaluate WebGazer model on all subjects.\n    Returns per-subject performance metrics.\n    \"\"\"\n    results = []\n\n    print(f\"Evaluating WebGazer model on {len(subject_data)} subjects...\")\n\n    for i, subject in enumerate(subject_data):\n        if i % 50 == 0:\n            print(f\"Processing subject {i+1}/{len(subject_data)}\")\n\n        subject_id = subject['subject_id']\n\n        # Initialize and train WebGazer model\n        webgazer = WebGazerModel(lambda_ridge=lambda_ridge)\n\n        try:\n            # Train on calibration data\n            webgazer.fit(subject['cal_images'], subject['cal_coords'])\n\n            # Predict on test data\n            predictions = webgazer.predict(subject['test_images'])\n\n            # Calculate errors using backend-agnostic approach\n            errors = []\n            for pred_coord, actual_coord in zip(predictions, subject['test_coords']):\n                error = normalized_weighted_euc_dist(\n                    np.array([[actual_coord[0], actual_coord[1]]], dtype=np.float32),\n                    np.array([[pred_coord[0], pred_coord[1]]], dtype=np.float32)\n                )\n                errors.append(float(error[0]))  # Works with both JAX and TensorFlow backends\n\n            mean_error = np.mean(errors)\n\n            results.append({\n                'subject_id': subject_id,\n                'model': 'WebGazer',\n                'mean_error': mean_error,\n                'n_cal_points': len(subject['cal_coords']),\n                'n_test_points': len(subject['test_coords'])\n            })\n\n        except Exception as e:\n            print(f\"Error processing subject {subject_id}: {e}\")\n            continue\n\n    return pd.DataFrame(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9-_xbmNUbA-"
   },
   "outputs": [],
   "source": "def evaluate_gaze_model(gaze_model_dataset, gaze_model, model_name, batch_size=1):\n    \"\"\"\n    Evaluate gaze model using pure tf.data streaming - data stays on disk until needed.\n    Processes one batch at a time to minimize RAM usage.\n    \"\"\"\n    print(f\"Evaluating {model_name} with batch_size={batch_size}...\")\n    results = []\n    \n    # Stream through dataset in batches\n    batched_dataset = gaze_model_dataset.batch(batch_size)\n    \n    for batch_idx, batch_data in enumerate(batched_dataset):\n        if batch_idx % 20 == 0:\n            print(f\"Processing batch {batch_idx}...\")\n        \n        # Unpack - subject_ids now included from prepare_model_inputs\n        batch_inputs, batch_labels, batch_target_masks, batch_subject_ids = batch_data\n        \n        # Run model prediction (stays in TF/JAX until .numpy())\n        batch_predictions = gaze_model(batch_inputs, training=False)\n        \n        # Only convert what we need to numpy, one subject at a time\n        current_batch_size = batch_predictions.shape[0]\n        for i in range(current_batch_size):\n            # Extract single subject data\n            subject_id = int(batch_subject_ids[i].numpy())\n            predictions = batch_predictions[i].numpy()\n            target_mask = batch_target_masks[i].numpy()\n            actual_coords = batch_labels[i].numpy()\n            cal_mask = batch_inputs['Input_Calibration_Mask'][i].numpy()\n            \n            # Extract test predictions using target mask\n            test_predictions = predictions[target_mask == 1]\n            test_actual = actual_coords[target_mask == 1]\n            \n            # Skip subjects with insufficient test data\n            if len(test_predictions) < 96:\n                continue\n            \n            # Calculate errors using vectorized approach\n            pred_array = np.array(test_predictions, dtype=np.float32)\n            actual_array = np.array(test_actual, dtype=np.float32)\n            \n            # Vectorized error calculation\n            errors = normalized_weighted_euc_dist(actual_array, pred_array)\n            mean_error = float(np.mean(errors))\n            n_cal = int(np.sum(cal_mask))\n            \n            results.append({\n                'subject_id': subject_id,\n                'model': model_name,\n                'mean_error': mean_error,\n                'n_cal_points': n_cal,\n                'n_test_points': len(test_predictions)\n            })\n        \n        # Explicit cleanup after each batch\n        del batch_predictions, batch_inputs, batch_labels, batch_target_masks\n        if batch_idx % 10 == 0:\n            gc.collect()\n    \n    print(f\"Completed evaluation for {len(results)} subjects\")\n    return pd.DataFrame(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_psTz8vUbA-",
    "outputId": "8046ed4a-693b-4c72-bc18-67e27c30a43d"
   },
   "outputs": [],
   "source": "# Evaluate gaze model FIRST using streaming (no data loaded to RAM yet)\nprint(\"=== Evaluating Our Model (streaming from disk) ===\")\nour_model_results = evaluate_gaze_model(gaze_model_dataset, gaze_model, OUR_MODEL_NAME, batch_size=1)\n\n# Clear gaze model from memory before loading WebGazer data\nprint(\"\\nClearing model from memory...\")\ndel gaze_model\ngc.collect()\n\nprint(\"\\n=== Evaluating WebGazer ===\")\n# Now extract data for WebGazer (loads to numpy - but gaze model is cleared)\nprint(\"Extracting data for WebGazer (requires numpy arrays)...\")\nmasked_dataset_for_webgazer = prepare_masked_dataset(test_data_rescaled, scaled_cal_points, 1)\nwebgazer_subject_data = extract_calibration_and_test_data_for_webgazer(masked_dataset_for_webgazer)\nprint(f\"Prepared data for {len(webgazer_subject_data)} subjects\")\n\n# Evaluate WebGazer\nwebgazer_results = evaluate_webgazer_model(webgazer_subject_data, lambda_ridge=RIDGE_REGULARIZATION)\n\n# Clear WebGazer data\ndel webgazer_subject_data\ngc.collect()\n\n# Combine results\nall_results = pd.concat([webgazer_results, our_model_results], ignore_index=True)\n\nprint(f\"\\n=== Evaluation Complete ===\")\nprint(f\"WebGazer results: {len(webgazer_results)} subjects\")\nprint(f\"{OUR_MODEL_NAME} results: {len(our_model_results)} subjects\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huYAceVSUbA-"
   },
   "source": "## Results Analysis and Visualization\n\nAnalyzing the performance differences between WebGazer and our model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-aFhbo0UbA-",
    "outputId": "ab2845c8-841d-4fa9-d474-96ab9160cc9d"
   },
   "outputs": [],
   "source": "# Summary statistics\nsummary_stats = all_results.groupby('model')['mean_error'].agg(['mean', 'std', 'median', 'count']).round(4)\nprint(\"Model Performance Summary:\")\nprint(summary_stats)\nprint()\n\n# Statistical significance test\nwebgazer_errors = webgazer_results['mean_error'].values\nour_model_errors = our_model_results['mean_error'].values\n\n# Ensure we have matching subjects for paired comparison\nwebgazer_subjects = set(webgazer_results['subject_id'])\nour_model_subjects = set(our_model_results['subject_id'])\ncommon_subjects = webgazer_subjects.intersection(our_model_subjects)\n\nprint(f\"Common subjects between models: {len(common_subjects)}\")\n\nif len(common_subjects) > 10:  # Need sufficient data for statistical test\n    # Create paired data\n    webgazer_paired = webgazer_results[webgazer_results['subject_id'].isin(common_subjects)].sort_values('subject_id')['mean_error'].values\n    our_model_paired = our_model_results[our_model_results['subject_id'].isin(common_subjects)].sort_values('subject_id')['mean_error'].values\n\n    # Paired t-test\n    t_stat, p_value = stats.ttest_rel(webgazer_paired, our_model_paired)\n\n    print(f\"\\nPaired t-test results:\")\n    print(f\"t-statistic: {t_stat:.4f}\")\n    print(f\"p-value: {p_value:.6f}\")\n    print(f\"Mean difference (WebGazer - {OUR_MODEL_NAME}): {np.mean(webgazer_paired - our_model_paired):.4f}\")\n\n    # Effect size (Cohen's d)\n    diff = webgazer_paired - our_model_paired\n    cohens_d = np.mean(diff) / np.std(diff)\n    print(f\"Effect size (Cohen's d): {cohens_d:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "iqwgGzeAUbA-",
    "outputId": "60a0c004-657d-4431-ba04-fd3e1a531b49"
   },
   "outputs": [],
   "source": "# Create comparison visualizations\n\n# 1. Error distribution comparison\ndistribution_plot = (\n    ggplot(all_results, aes(x='mean_error', fill='model'))\n    + geom_histogram(alpha=0.7, position='identity', bins=30)\n    + labs(\n        title=f'Prediction Error Distribution: WebGazer vs {OUR_MODEL_NAME}',\n        x='Mean Prediction Error (% Screen Diagonal)',\n        y='Number of Subjects'\n    )\n    + scale_fill_manual(values=['#A23B72', '#2E86AB'], name='Model')\n    + theme_classic()\n    + theme(plot_title=element_text(hjust=0.5))\n)\n\nprint(\"Distribution plot:\")\ndistribution_plot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "jPEurXo1UbA_",
    "outputId": "6febd8a1-8177-4193-a577-facd08e58584"
   },
   "outputs": [],
   "source": "from plotnine import coord_equal\n# 2. Direct comparison scatter plot (for common subjects)\n\ncomparison_data = []\nfor subject_id in common_subjects:\n    webgazer_error = webgazer_results[webgazer_results['subject_id'] == subject_id]['mean_error'].iloc[0]\n    our_model_error = our_model_results[our_model_results['subject_id'] == subject_id]['mean_error'].iloc[0]\n\n    comparison_data.append({\n        'subject_id': subject_id,\n        'webgazer_error': webgazer_error*100,\n        'our_model_error': our_model_error*100,\n        'difference': webgazer_error - our_model_error\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\n\n# Scatter plot\nscatter_plot = (\n    ggplot(comparison_df, aes(x='our_model_error', y='webgazer_error'))\n    + geom_point(alpha=0.6, size=2)\n    + geom_abline(intercept=0, slope=1, linetype='dashed', color='red')\n    + labs(\n        title='Subject-Level Performance Comparison',\n        x=f'{OUR_MODEL_NAME} Error (% Screen Diagonal)',\n        y='WebGazer Error (% Screen Diagonal)',\n        subtitle='Points above red line indicate our model performed better'\n    )\n    + coord_equal(xlim=(3, 45), ylim=(3, 45))\n    + theme_classic()\n    + theme(plot_title=element_text(hjust=0.5), plot_subtitle=element_text(hjust=0.5))\n)\n\nprint(\"Scatter plot comparison:\")\n\nour_model_better = sum(comparison_df['difference'] > 0)\npercent_better = (our_model_better / len(comparison_df)) * 100\n\nprint(f\"{OUR_MODEL_NAME} performed better than WebGazer in {our_model_better} subjects ({percent_better:.1f}%)\")\n\naverage_improvement = comparison_df['difference'].mean() * 100\nprint(f\"Average improvement over WebGazer: {average_improvement:.2f}%\")\n\nscatter_plot"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLCEhA5nUbBE"
   },
   "source": [
    "## Save Results and Create Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y57YHLHFUbBE",
    "outputId": "5ab10cb4-5faa-474b-82ec-ebbcb42aeb7c"
   },
   "outputs": [],
   "source": "# Save plots\ndistribution_plot.save('webgazer_comparison_distribution.png', width=10, height=6, dpi=150)\n\nif len(common_subjects) > 10:\n    scatter_plot.save('webgazer_comparison_scatter.png', width=8, height=6, dpi=150)\n\n# Save results data\nall_results.to_csv('webgazer_comparison_results.csv', index=False)\n\nif len(common_subjects) > 10:\n    comparison_df.to_csv('webgazer_paired_comparison.csv', index=False)\n\nprint(\"Results and plots saved successfully\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}