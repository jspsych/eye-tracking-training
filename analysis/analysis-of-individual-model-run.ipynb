{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LAJHmSmF84"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW4fMCshWBrD"
      },
      "outputs": [],
      "source": [
        "!pip install osfclient --quiet\n",
        "!pip install keras_cv --quiet\n",
        "!pip install plotnine --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install keras-hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQRQZ3D5Vwqc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "import keras_hub\n",
        "import keras_cv\n",
        "\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "\n",
        "from plotnine import ggplot, geom_point, aes, geom_line, geom_histogram, geom_boxplot, geom_ribbon, scale_y_reverse, theme_void, scale_x_continuous, scale_color_manual, scale_fill_manual, ylab, xlab, labs, theme, theme_classic, element_text, element_blank, element_line"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://{GH_USERNAME}:{GH_TOKEN}@github.com/jspsych/eyetracking-utils.git --quiet"
      ],
      "metadata": {
        "id": "ZIGJtmq7tXd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import et_util.dataset_utils as dataset_utils\n",
        "import et_util.embedding_preprocessing as embed_pre\n",
        "import et_util.model_layers as model_layers\n",
        "from et_util import experiment_utils\n",
        "from et_util.custom_loss import normalized_weighted_euc_dist\n",
        "from et_util.model_analysis import plot_model_performance"
      ],
      "metadata": {
        "id": "--Och-5ZtjR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BUzHdPEc-ef"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
        "os.environ['OSF_USERNAME'] = userdata.get('osfusername')\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9v9bxBcHc0"
      },
      "source": [
        "# Configure W&B experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SPGBdleCfbe"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed constants\n",
        "MAX_TARGETS = 288\n",
        "\n",
        "# Config constants\n",
        "EMBEDDING_DIM = 200\n",
        "RIDGE_REGULARIZATION = 0.1\n",
        "TRAIN_EPOCHS = 30\n",
        "MIN_CAL_POINTS = 8\n",
        "MAX_CAL_POINTS = 40\n",
        "\n",
        "BACKBONE = \"densenet\"\n",
        "\n",
        "AUGMENTATION = False\n",
        "\n",
        "INITIAL_LEARNING_RATE = 0.00001\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 5\n",
        "TRAIN_EPOCHS = 40\n",
        "WARMUP_EPOCHS = 2\n",
        "DECAY_EPOCHS = TRAIN_EPOCHS - WARMUP_EPOCHS\n",
        "DECAY_ALPHA = 0.01"
      ],
      "metadata": {
        "id": "WQwNaBoP1T9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeHGiFxtvo0"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset from OSF"
      ],
      "metadata": {
        "id": "b2DUvhFF9oXr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tswV6s38WbtO"
      },
      "outputs": [],
      "source": [
        "!osf -p uf2sh fetch single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgWOhWTt4iD"
      },
      "source": [
        "# Process raw data records into TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4h8QhGvWhZf"
      },
      "outputs": [],
      "source": [
        "!mkdir single_eye_tfrecords\n",
        "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZRcQgPadqtI"
      },
      "outputs": [],
      "source": [
        "def parse(element):\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'phase': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    #landmarks = content['landmarks']\n",
        "    raw_image = content['eye_img']\n",
        "    width = content['img_width']\n",
        "    height = content['img_height']\n",
        "    phase = content['phase']\n",
        "    depth = 3\n",
        "    coords = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    # landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n",
        "    # landmarks = tf.reshape(landmarks, shape=(478, 3))\n",
        "\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, phase, coords, subject_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWv57eb5XHVP"
      },
      "outputs": [],
      "source": [
        "test_data, _, _ = dataset_utils.process_tfr_to_tfds(\n",
        "    'single_eye_tfrecords/',\n",
        "    parse,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, phase, coords, subject_id: subject_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FULm58Cxu9oa"
      },
      "source": [
        "## Rescale the `x,y` coordinates to be 0-1 instead of 0-100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apcLK9klHLQV"
      },
      "outputs": [],
      "source": [
        "def rescale_coords_map(img, phase, coords, id):\n",
        "  return img, phase, tf.divide(coords, tf.constant([100.])), id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b47CtWKqHg0B"
      },
      "outputs": [],
      "source": [
        "test_data_rescaled = test_data.map(rescale_coords_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepared masked dataset with phase calibration info"
      ],
      "metadata": {
        "id": "8KfcIpyMrybH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_masked_dataset(dataset, calibration_points=None, cal_phase=None):\n",
        "    # Step 1: Group dataset by subject_id and batch all images\n",
        "    def group_by_subject(subject_id, ds):\n",
        "        return ds.batch(batch_size=MAX_TARGETS)\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, phase, coords, subject_id: subject_id,\n",
        "        reduce_func=group_by_subject,\n",
        "        window_size=MAX_TARGETS\n",
        "    )\n",
        "\n",
        "    # Step 2: Filter out subjects with not enough data points ï¼ˆ288 total; 144 in each phase)\n",
        "    def filter_by_image_count(images, phase, coords, subject_ids):\n",
        "        total_image_count = tf.shape(images)[0] >= MAX_TARGETS\n",
        "        phase1_image_count = tf.reduce_sum(tf.cast(tf.equal(phase, 1), tf.int32)) >= 144\n",
        "        phase2_image_count = tf.reduce_sum(tf.cast(tf.equal(phase, 2), tf.int32)) >= 144\n",
        "        return tf.logical_and(total_image_count, tf.logical_and(phase1_image_count, phase2_image_count))\n",
        "\n",
        "   # grouped_dataset = grouped_dataset.filter(filter_by_image_count)\n",
        "\n",
        "    # Step 3: Transform each batch to include masks\n",
        "    def add_masks_to_batch(images, phase, coords, subject_ids):\n",
        "\n",
        "        actual_batch_size = tf.shape(images)[0] # how many total images for a given subject\n",
        "\n",
        "        # Create phase masks\n",
        "        phase1_mask = tf.cast(tf.equal(phase, 1), tf.int8)\n",
        "        phase2_mask = tf.cast(tf.equal(phase, 2), tf.int8)\n",
        "\n",
        "        cal_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n",
        "        target_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n",
        "\n",
        "        if calibration_points is None:\n",
        "          n_cal_images = tf.random.uniform(\n",
        "              shape=[],\n",
        "              minval=MIN_CAL_POINTS,\n",
        "              maxval=MAX_CAL_POINTS,\n",
        "              dtype=tf.int32\n",
        "          )\n",
        "          random_indices = tf.random.shuffle(tf.range(actual_batch_size))\n",
        "          cal_indices = random_indices[:n_cal_images]\n",
        "\n",
        "          cal_mask = tf.scatter_nd(\n",
        "              tf.expand_dims(cal_indices, 1),\n",
        "              tf.ones(n_cal_images, dtype=tf.int8),\n",
        "              [MAX_TARGETS]\n",
        "          )\n",
        "        else:\n",
        "          coords_xpand = tf.expand_dims(coords, axis=1)\n",
        "          cal_xpand = tf.expand_dims(calibration_points, axis=0)\n",
        "\n",
        "          # Check which points match calibration points\n",
        "          equality = tf.equal(coords_xpand, cal_xpand)\n",
        "          matches = tf.reduce_all(equality, axis=-1)\n",
        "          point_matches = tf.reduce_any(matches, axis=1)\n",
        "          cal_mask = tf.cast(point_matches, dtype=tf.int8)\n",
        "\n",
        "        target_mask = (1 - cal_mask) * phase2_mask\n",
        "\n",
        "        if cal_phase == 1:\n",
        "          cal_mask = cal_mask * phase1_mask\n",
        "        elif cal_phase == 2:\n",
        "          cal_mask = cal_mask * phase2_mask\n",
        "\n",
        "        padded_images = tf.pad(\n",
        "            tf.reshape(images, (-1, 36, 144, 1)),\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(\n",
        "            coords,\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n",
        "        )\n",
        "        padded_cal_mask = tf.pad(\n",
        "            cal_mask,\n",
        "            [[0, MAX_TARGETS - actual_batch_size]]\n",
        "        )\n",
        "        padded_target_mask = tf.pad(\n",
        "            target_mask,\n",
        "            [[0, MAX_TARGETS - actual_batch_size]]\n",
        "        )\n",
        "\n",
        "        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(padded_cal_mask, [MAX_TARGETS])\n",
        "        padded_target_mask = tf.ensure_shape(padded_target_mask, [MAX_TARGETS])\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids[0]\n",
        "\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        lambda imgs, phase, coords, subj_ids: add_masks_to_batch(imgs, phase, coords, subj_ids),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return masked_dataset"
      ],
      "metadata": {
        "id": "jfdmO7MKba7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subject_id_list(features, labels, subject_ids):\n",
        "  return subject_ids"
      ],
      "metadata": {
        "id": "aNC0iCcH5pLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model_inputs(features, labels, subject_ids):\n",
        "    images, coords, cal_mask, target_mask = features\n",
        "\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": images,\n",
        "        \"Input_All_Coords\": coords,\n",
        "        \"Input_Calibration_Mask\": cal_mask,\n",
        "        \"Input_Target_Mask\": target_mask\n",
        "    }\n",
        "\n",
        "    return inputs, labels, target_mask"
      ],
      "metadata": {
        "id": "WntiqEBSCeOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "fgurOg1cq4T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_dataset_p1 = prepare_masked_dataset(test_data_rescaled, scaled_cal_points, 1)\n",
        "\n",
        "masked_dataset_p2 = prepare_masked_dataset(test_data_rescaled, scaled_cal_points, 2)\n",
        "\n",
        "masked_dataset_subject_ids = masked_dataset_p1.map(get_subject_id_list)"
      ],
      "metadata": {
        "id": "0gac_vkwbfZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subject_ids = []\n",
        "for e in masked_dataset_subject_ids.as_numpy_iterator():\n",
        "  subject_ids.append(e)\n",
        "subject_ids = np.array(subject_ids).astype(int)"
      ],
      "metadata": {
        "id": "WWbgci8A55XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds_p1 = masked_dataset_p1.map(\n",
        "    prepare_model_inputs,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds_p2 = masked_dataset_p2.map(\n",
        "    prepare_model_inputs,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "GlHsEEq0D1Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Should be the same for either phase cal choices\n",
        "SUBJECTS = 0\n",
        "for e in test_ds_p1.as_numpy_iterator():\n",
        "  SUBJECTS += 1"
      ],
      "metadata": {
        "id": "mzwFyg1cD3ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUBJECTS"
      ],
      "metadata": {
        "id": "GsI3d1_Kut8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgNRPZIhncRM"
      },
      "source": [
        "# Download the Model Config and Weights File from W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p_UXl0Unfsc"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "\n",
        "# https://wandb.ai/vassar-cogsci-lab/eye-tracking-dense-full-data-set-single-eye/runs/up4mb4u3?nw=nwuserjoshdeleeuw\n",
        "run = api.run(\"vassar-cogsci-lab/eye-tracking-dense-full-data-set-single-eye/v0lc4agm\")\n",
        "\n",
        "config = run.config\n",
        "run.file(\"full_model.weights.h5\").download(exist_ok=True)\n",
        "\n",
        "EMBEDDING_DIM = config[\"embedding_dim\"]\n",
        "RIDGE_REGULARIZATION = config[\"ridge_regularization\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19yxxx_Ts8H"
      },
      "source": [
        "# Re-construct Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom layers"
      ],
      "metadata": {
        "id": "Ow5T7cNRCHb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTimeDistributed(keras.layers.Wrapper):\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.supports_masking = getattr(layer, 'supports_masking', False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not isinstance(input_shape, (tuple, list)) or len(input_shape) < 3:\n",
        "            raise ValueError(\n",
        "                \"`SimpleTimeDistributed` requires input with at least 3 dimensions\"\n",
        "            )\n",
        "\n",
        "        super().build((input_shape[0], *input_shape[2:]))\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        child_output_shape = self.layer.compute_output_shape((input_shape[0], *input_shape[2:]))\n",
        "        return (child_output_shape[0], input_shape[1], *child_output_shape[1:])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        time_steps = input_shape[1]\n",
        "\n",
        "        reshaped_inputs = ops.reshape(inputs, (-1, *input_shape[2:]))\n",
        "\n",
        "        outputs = self.layer.call(reshaped_inputs, training=training)\n",
        "\n",
        "        output_shape = ops.shape(outputs)\n",
        "\n",
        "        return ops.reshape(outputs, (batch_size, time_steps, *output_shape[1:]))"
      ],
      "metadata": {
        "id": "7eXv-aXgnmyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGk0aRz_ebqk"
      },
      "outputs": [],
      "source": [
        "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    def __init__(self, lambda_ridge, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        embeddings, coords, calibration_weights, cal_mask  = inputs\n",
        "\n",
        "        embeddings = ops.cast(embeddings, \"float32\")\n",
        "        coords = ops.cast(coords, \"float32\")\n",
        "        calibration_weights = ops.cast(calibration_weights, \"float32\")\n",
        "        cal_mask = ops.cast(cal_mask, \"float32\")\n",
        "\n",
        "        w = ops.squeeze(calibration_weights, axis=-1)\n",
        "\n",
        "        w_masked = w * cal_mask\n",
        "        w_sqrt = ops.sqrt(w_masked + self.epsilon)\n",
        "        w_sqrt = ops.expand_dims(w_sqrt, -1)\n",
        "\n",
        "        cal_mask_expand = ops.expand_dims(cal_mask, -1)\n",
        "        X = embeddings * cal_mask_expand\n",
        "\n",
        "        X_weighted = X * w_sqrt\n",
        "        y_weighted = coords * w_sqrt * cal_mask_expand\n",
        "\n",
        "        X_t = ops.transpose(X_weighted, axes=[0, 2, 1])\n",
        "        X_t_X = ops.matmul(X_t, X_weighted)\n",
        "\n",
        "        identity_matrix = ops.cast(ops.eye(ops.shape(embeddings)[-1]), \"float32\")\n",
        "        lhs = X_t_X + self.lambda_ridge * identity_matrix\n",
        "\n",
        "        rhs = ops.matmul(X_t, y_weighted)\n",
        "\n",
        "        kernel = tf.linalg.solve(lhs, rhs)\n",
        "\n",
        "        output = ops.matmul(embeddings, kernel)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        unknown_embeddings_shape, _, _, _ = input_shapes\n",
        "        return unknown_embeddings_shape[:-1] + (2,)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\"lambda_ridge\": self.lambda_ridge, \"epsilon\": self.epsilon})\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskInspectorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MaskInspectorLayer, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "    def call(self, inputs, mask=None):\n",
        "      tf.print(\"Layer mask:\", mask)\n",
        "      return inputs"
      ],
      "metadata": {
        "id": "rqTTR5I9n0de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom loss"
      ],
      "metadata": {
        "id": "qCIOW0_pEIjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_weighted_euc_dist(y_true, y_pred):\n",
        "    x_weight = ops.convert_to_tensor([1.778, 1.0], dtype=\"float32\")\n",
        "\n",
        "    y_true_weighted = ops.multiply(x_weight, y_true)\n",
        "    y_pred_weighted = ops.multiply(x_weight, y_pred)\n",
        "\n",
        "    squared_diff = ops.square(y_pred_weighted - y_true_weighted)\n",
        "    squared_dist = ops.sum(squared_diff, axis=-1)\n",
        "    dist = ops.sqrt(squared_dist)\n",
        "\n",
        "    normalized_dist = ops.divide(dist, .0203992)\n",
        "\n",
        "    return normalized_dist"
      ],
      "metadata": {
        "id": "LO6UVbVvyZLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuZobSgkgbW"
      },
      "source": [
        "## Embedding model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dense_net_backbone():\n",
        "  DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "  return keras_hub.models.DenseNetBackbone(\n",
        "      stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "      image_shape=(36, 144, 1),\n",
        "  )"
      ],
      "metadata": {
        "id": "ue5oH-yT4y20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnKs983UsyDj"
      },
      "outputs": [],
      "source": [
        "def create_embedding_model(BACKBONE):\n",
        "  image_shape = (36, 144, 1)\n",
        "  input_eyes = keras.layers.Input(shape=image_shape)\n",
        "\n",
        "  eyes_rescaled = keras.layers.Rescaling(scale=1./255)(input_eyes)\n",
        "\n",
        "  # Continue with the backbone\n",
        "  # backbone = create_rednet_backbone()\n",
        "\n",
        "\n",
        "  # backbone = keras.Sequential([\n",
        "  #     keras.layers.Flatten(),\n",
        "  #     keras.layers.Dense(10, activation=\"relu\")\n",
        "  # ])\n",
        "  if BACKBONE == \"densenet\":\n",
        "    backbone = create_dense_net_backbone()\n",
        "\n",
        "  # backbone = create_efficientnet_backbone()\n",
        "\n",
        "  # backbone = keras_hub.models.MiTBackbone(\n",
        "  #   image_shape=(36,144,1),\n",
        "  #   layerwise_depths=[2,2,2,2],\n",
        "  #   num_layers=4,\n",
        "  #   layerwise_num_heads=[1,2,5,8],\n",
        "  #   layerwise_sr_ratios=[8,4,2,1],\n",
        "  #   max_drop_path_rate=0.1,\n",
        "  #   layerwise_patch_sizes=[7,3,3,3],\n",
        "  #   layerwise_strides=[4,2,2,2],\n",
        "  #   hidden_dims=[32,64,160,256]\n",
        "  # )\n",
        "\n",
        "  backbone_encoder = backbone(eyes_rescaled)\n",
        "  flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "  eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "  embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "\n",
        "  return embedding_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtm4EUSZwXDC"
      },
      "source": [
        "## Full trainable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JOA5GNqTUW-"
      },
      "outputs": [],
      "source": [
        "def create_masked_model():\n",
        "    input_all_images = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 36, 144, 1),\n",
        "        name=\"Input_All_Images\"\n",
        "    )\n",
        "\n",
        "    input_all_coords = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 2),\n",
        "        name=\"Input_All_Coords\"\n",
        "    )\n",
        "\n",
        "    input_cal_mask = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS,),\n",
        "        name=\"Input_Calibration_Mask\",\n",
        "    )\n",
        "\n",
        "    embedding_model = create_embedding_model(BACKBONE)\n",
        "\n",
        "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\"Image_Embeddings\")(input_all_images)\n",
        "\n",
        "    calibration_weights = keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\"sigmoid\",\n",
        "        name=\"Calibration_Weights\"\n",
        "    )(all_embeddings)\n",
        "\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
        "        RIDGE_REGULARIZATION,\n",
        "        name=\"Regression\"\n",
        "    )(\n",
        "        [\n",
        "            all_embeddings,\n",
        "            input_all_coords,\n",
        "            calibration_weights,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\"MaskedEyePredictionModel\"\n",
        "    )\n",
        "\n",
        "    return full_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xAFsV9x41BZ"
      },
      "outputs": [],
      "source": [
        "full_model = create_masked_model()\n",
        "full_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_model.load_weights('full_model.weights.h5')\n",
        "full_model.summary()\n",
        "\n",
        "full_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=normalized_weighted_euc_dist,\n",
        "    metrics=[normalized_weighted_euc_dist],\n",
        "    jit_compile = False\n",
        ")"
      ],
      "metadata": {
        "id": "dRGwIhvnXBOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzxl5IX0ycoR"
      },
      "source": [
        "# Analysis \\#1: P1 vs P2 Subject Mean Loss Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1 calibration loss distribution across subjects"
      ],
      "metadata": {
        "id": "HNdqPsia_lwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_p1 = full_model.predict(test_ds_p1.batch(BATCH_SIZE))"
      ],
      "metadata": {
        "id": "GQJEayJ_yQvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_OUDhetR5LdO"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store the batch losses\n",
        "batch_losses_p1 = []\n",
        "\n",
        "for pred_batch, ds_batch in zip(pred_p1, test_ds_p1.batch(1).as_numpy_iterator()):\n",
        "    # Extract ground truth and subject ID from dataset batch\n",
        "    y_true = ds_batch[1]\n",
        "    mask = ds_batch[0]['Input_Target_Mask'].reshape(-1)\n",
        "\n",
        "    # Calculate losses for all points in this batch\n",
        "    batch_point_losses = normalized_weighted_euc_dist(y_true, pred_batch).numpy().reshape(-1)\n",
        "    batch_point_losses = batch_point_losses[mask == 1]\n",
        "\n",
        "    # Store average loss for this batch and subject ID\n",
        "    batch_losses_p1.append(batch_point_losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subj_mean_losses_p1 = [np.mean(losses) for losses in batch_losses_p1]"
      ],
      "metadata": {
        "id": "Jb8IhRMw1j9x",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blgWDH3E5Swd"
      },
      "outputs": [],
      "source": [
        "subj_mean_losses_df_p1 = pd.DataFrame({'mean_loss': subj_mean_losses_p1})\n",
        "\n",
        "subj_mean_losses_plot_p1 = (ggplot(subj_mean_losses_df_p1, aes(x='mean_loss'))\n",
        " + geom_histogram(bins=22, color='white', fill='#333')\n",
        " + labs(title='Distribution of Subject Mean Model Loss\\n(Calibration with Phase 1 Points)',\n",
        "       x='Mean Loss - Proportion of Screen Diagonal',\n",
        "       y='#Subjects')\n",
        " + theme_classic()\n",
        ")\n",
        "\n",
        "subj_mean_losses_plot_p1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2 calibration loss distribution across subjects"
      ],
      "metadata": {
        "id": "Jb1Sy-ET_pF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_p2 = full_model.predict(test_ds_p2.batch(BATCH_SIZE))"
      ],
      "metadata": {
        "id": "Mr68-eUzsSLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSfQ2msf_DrL"
      },
      "outputs": [],
      "source": [
        "batch_losses_p2 = []\n",
        "subject_ids_p2 = []\n",
        "\n",
        "for pred_batch, ds_batch in zip(pred_p2, test_ds_p2.batch(1).as_numpy_iterator()):\n",
        "    y_true = ds_batch[1]\n",
        "    subject_id = ds_batch[2][0]\n",
        "    mask = ds_batch[0]['Input_Target_Mask'].reshape(-1)\n",
        "\n",
        "    batch_point_losses = normalized_weighted_euc_dist(y_true, pred_batch).numpy().reshape(-1)\n",
        "    batch_point_losses = batch_point_losses[mask == 1]\n",
        "\n",
        "    batch_losses_p2.append(batch_point_losses)\n",
        "    subject_ids_p2.append(subject_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subj_mean_losses_p2 = [np.mean(losses) for losses in batch_losses_p2]"
      ],
      "metadata": {
        "id": "CQYwc_00W1Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subj_mean_losses_df_p2 = pd.DataFrame({'mean_loss': subj_mean_losses_p2})\n",
        "\n",
        "subj_mean_losses_plot_p2 = (ggplot(subj_mean_losses_df_p2, aes(x='mean_loss'))\n",
        " + geom_histogram(bins=22, color='white', fill='#333')\n",
        " + labs(title='Distribution of Subject Mean Model Loss\\n(Calibration with Phase 2 Points)',\n",
        "       x='Mean Loss - Proportion of Screen Diagonal',\n",
        "       y='#Subjects')\n",
        " + theme_classic()\n",
        ")\n",
        "\n",
        "subj_mean_losses_plot_p2"
      ],
      "metadata": {
        "id": "RqmBGZtcWyG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIxNESdIRyue"
      },
      "source": [
        "# Analysis \\#2: P1 vs P2 Visualization for Individual Subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjjMINThRkLa"
      },
      "outputs": [],
      "source": [
        "subj_mean_losses_df_p1['subject_id'] = subject_ids\n",
        "subj_mean_losses_df_p2['subject_id'] = subject_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subj_mean_losses_df = pd.merge(subj_mean_losses_df_p1[subj_mean_losses_df_p1['mean_loss'] <= 15], subj_mean_losses_df_p2[subj_mean_losses_df_p2['mean_loss'] <= 15], on='subject_id', suffixes=('_p1', '_p2'))\n",
        "subj_mean_losses_df['mean_loss_diff'] = abs(subj_mean_losses_df['mean_loss_p1'] - subj_mean_losses_df['mean_loss_p2'])"
      ],
      "metadata": {
        "id": "UShx-0nyJnOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw-OsZcNTMR_"
      },
      "outputs": [],
      "source": [
        "def visualize_pred_target_diff(subject_id, subject_idx, cal_coords, phase):\n",
        "  test_ds = test_ds_p1 if phase == 1 else test_ds_p2\n",
        "\n",
        "  y_true = []\n",
        "\n",
        "  for e in test_ds.skip(subject_idx).take(1).as_numpy_iterator():\n",
        "    mask = e[0]['Input_Target_Mask'].reshape(-1)\n",
        "    y_true = e[1][mask == 1]\n",
        "\n",
        "  y_pred = full_model.predict(test_ds.skip(subject_idx).take(1).batch(1))[0]\n",
        "  y_pred = y_pred[mask == 1]\n",
        "\n",
        "  losses = normalized_weighted_euc_dist(y_true, y_pred).numpy().reshape(-1)\n",
        "\n",
        "  cal_coords_df = pd.DataFrame({\"sample\": range(len(cal_coords)), \"x\": cal_coords[:,0], \"y\": cal_coords[:,1], \"point_type\": \"calibration\"})\n",
        "  y_true_df = pd.DataFrame({\"sample\": range(len(y_true)), \"x\": y_true[:,0], \"y\": y_true[:,1], \"point_type\": \"target\"})\n",
        "  y_pred_df = pd.DataFrame({\"sample\": range(len(y_true)), \"x\": y_pred[:,0], \"y\": y_pred[:,1], \"point_type\": \"prediction\"})\n",
        "\n",
        "  y_df = pd.concat([cal_coords_df, y_true_df,y_pred_df])\n",
        "\n",
        "  return (ggplot(y_df, aes(x=\"x\", y=\"y\", color=\"point_type\", group=\"sample\"))+\n",
        "          geom_point()+\n",
        "          geom_line(color=\"gray\", size=0.3)+\n",
        "          scale_y_reverse()+\n",
        "          scale_color_manual(values=[\"red\", \"black\", \"gray\"])+\n",
        "          theme_void() +\n",
        "          labs(title=f\"Subject: {max_diff_row_subject_id}\\n(Calibrated with phase {phase} points)\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Max difference subject"
      ],
      "metadata": {
        "id": "fUYCt444t267"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_diff_row_idx = subj_mean_losses_df['mean_loss_diff'].idxmax()\n",
        "max_diff_row = subj_mean_losses_df.loc[max_diff_row_idx]\n",
        "max_diff_row_subject_id = max_diff_row['subject_id'].astype(int)"
      ],
      "metadata": {
        "id": "6f8u6O-4zW75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_diff_point_loss_visualization_plot_p1 = visualize_pred_target_diff(max_diff_row_subject_id, max_diff_row_idx, scaled_cal_points, phase=1)\n",
        "\n",
        "max_diff_point_loss_visualization_plot_p1"
      ],
      "metadata": {
        "id": "_0olDhmr2yoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_diff_point_loss_visualization_plot_p2 = visualize_pred_target_diff(max_diff_row_subject_id, max_diff_row_idx, scaled_cal_points, phase=2)\n",
        "\n",
        "max_diff_point_loss_visualization_plot_p2"
      ],
      "metadata": {
        "id": "UI68Jpe26SsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Min difference subject"
      ],
      "metadata": {
        "id": "SLQsJyt-6PY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_diff_row_idx = subj_mean_losses_df['mean_loss_diff'].idxmin()\n",
        "min_diff_row = subj_mean_losses_df.loc[min_diff_row_idx]\n",
        "min_diff_row_subject_id = min_diff_row['subject_id'].astype(int)"
      ],
      "metadata": {
        "id": "V12akQfw6PY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_diff_point_loss_visualization_plot_p1 = visualize_pred_target_diff(min_diff_row_subject_id, min_diff_row_idx, scaled_cal_points, phase=1)\n",
        "\n",
        "min_diff_point_loss_visualization_plot_p1"
      ],
      "metadata": {
        "id": "A5pOINHM6PY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_diff_point_loss_visualization_plot_p2 = visualize_pred_target_diff(min_diff_row_subject_id, min_diff_row_idx, scaled_cal_points, phase=2)\n",
        "\n",
        "min_diff_point_loss_visualization_plot_p2"
      ],
      "metadata": {
        "id": "j57s3VZT6vLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best subject"
      ],
      "metadata": {
        "id": "dEgEiVvr6-g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subj_mean_losses_df['total_loss'] = subj_mean_losses_df['mean_loss_p1'] + subj_mean_losses_df['mean_loss_p2']\n",
        "\n",
        "best_subj_row_idx = subj_mean_losses_df['total_loss'].idxmin()\n",
        "best_subj_row = subj_mean_losses_df.loc[best_subj_row_idx]\n",
        "best_subj_row_subject_id = best_subj_row['subject_id'].astype(int)"
      ],
      "metadata": {
        "id": "0Br9VNzq6-g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_subj_point_loss_visualization_plot_p1 = visualize_pred_target_diff(best_subj_row_subject_id, best_subj_row_idx, scaled_cal_points, phase=1)\n",
        "\n",
        "best_subj_point_loss_visualization_plot_p1"
      ],
      "metadata": {
        "id": "FfQfl6vz6-g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_subj_point_loss_visualization_plot_p2 = visualize_pred_target_diff(best_subj_row_subject_id, best_subj_row_idx, scaled_cal_points, phase=2)\n",
        "\n",
        "best_subj_point_loss_visualization_plot_p2"
      ],
      "metadata": {
        "id": "BRyKwms16-g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Worst subject"
      ],
      "metadata": {
        "id": "gxeJzBBH7t9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "worst_subj_row_idx = subj_mean_losses_df['total_loss'].idxmax()\n",
        "worst_subj_row = subj_mean_losses_df.loc[worst_subj_row_idx]\n",
        "worst_subj_row_subject_id = worst_subj_row['subject_id'].astype(int)"
      ],
      "metadata": {
        "id": "mYGxXyZ97t9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worst_subj_point_loss_visualization_plot_p1 = visualize_pred_target_diff(worst_subj_row_subject_id, worst_subj_row_idx, scaled_cal_points, phase=1)\n",
        "\n",
        "worst_subj_point_loss_visualization_plot_p1"
      ],
      "metadata": {
        "id": "R1MPUSeM7t9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worst_subj_point_loss_visualization_plot_p2 = visualize_pred_target_diff(worst_subj_row_subject_id, worst_subj_row_idx, scaled_cal_points, phase=2)\n",
        "\n",
        "worst_subj_point_loss_visualization_plot_p2"
      ],
      "metadata": {
        "id": "Z1RTsTT87t9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis \\#3: Eye Images Animated Visualization for Different Loss Ranges"
      ],
      "metadata": {
        "id": "BrleMxzTw-E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subj_losses_df_p1 = pd.DataFrame({'subject_id': subject_ids, 'losses': batch_losses_p1})\n",
        "subj_losses_df_p1 = subj_losses_df_p1[subj_losses_df_p1['losses'].apply(len) >= 124]\n",
        "subj_losses_df_p1['mean_loss'] = subj_losses_df_p1['losses'].apply(np.mean)"
      ],
      "metadata": {
        "id": "b0SaK0mEG5T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_array = np.array(subj_losses_df_p1['losses'].to_list())"
      ],
      "metadata": {
        "id": "z0f1l4J2KUuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subj_mean_losses_df_5 = subj_losses_df_p1[(subj_losses_df_p1['mean_loss'] >= 4) & (subj_losses_df_p1['mean_loss'] <= 6)]\n",
        "\n",
        "subj_mean_losses_df_10 = subj_losses_df_p1[(subj_losses_df_p1['mean_loss'] >= 8) & (subj_losses_df_p1['mean_loss'] <= 12)]\n",
        "\n",
        "subj_mean_losses_df_15 = subj_losses_df_p1[(subj_losses_df_p1['mean_loss'] >= 14) & (subj_losses_df_p1['mean_loss'] <= 16)]\n",
        "\n",
        "subj_mean_losses_df_20 = subj_losses_df_p1[(subj_losses_df_p1['mean_loss'] >= 19) & (subj_losses_df_p1['mean_loss'] <= 21)]"
      ],
      "metadata": {
        "id": "3gWJ7BaJAURw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "import base64\n",
        "\n",
        "def visualize_eye_tracking_data(dataset, subject_id, mean_loss, interval=100, figsize=(10, 6)):\n",
        "    # Extract data from the dataset\n",
        "    for inputs, labels, _ in dataset.take(1):\n",
        "        images = inputs[\"Input_All_Images\"]\n",
        "        coords = inputs[\"Input_All_Coords\"]\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        valid_images = images.numpy()\n",
        "        valid_coords = coords.numpy()\n",
        "\n",
        "        # Sort by X coordinate, then by Y coordinate\n",
        "        sorted_indices = np.lexsort((valid_coords[:, 0], valid_coords[:, 1]))\n",
        "        valid_images = valid_images[sorted_indices]\n",
        "        valid_coords = valid_coords[sorted_indices]\n",
        "\n",
        "        num_images = valid_images.shape[0]\n",
        "\n",
        "    # Set up the figure\n",
        "    plt.ioff()  # Turn off interactive mode to avoid displaying during generation\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Create a function that draws each frame\n",
        "    def draw_frame(frame_num):\n",
        "        ax.clear()\n",
        "\n",
        "        # Set up the 16:9 coordinate space\n",
        "        ax.set_xlim(0, 100)\n",
        "        ax.set_ylim(100, 0)  # 16:9 aspect ratio\n",
        "\n",
        "        # Get the current image and coordinates\n",
        "        img = valid_images[frame_num].squeeze()\n",
        "        x, y = valid_coords[frame_num]\n",
        "\n",
        "        # Draw rectangle for the screen\n",
        "        rect = plt.Rectangle((0, 0), 100, 100, fill=False, color='black', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Add image display in the top right corner\n",
        "        img_display = ax.inset_axes([0.35, 0.35, 0.3, 0.3], transform=ax.transAxes)\n",
        "        img_display.imshow(np.fliplr(img), cmap='gray')\n",
        "        img_display.axis('off')\n",
        "\n",
        "        # Draw red dot at the coordinate - make it bigger for visibility\n",
        "        ax.scatter(x * 100, y * 100, color='red', s=150, zorder=5,\n",
        "                  edgecolor='white', linewidth=1.5)\n",
        "\n",
        "        # Draw crosshair\n",
        "        ax.axhline(y * 100, color='gray', linestyle='--', alpha=0.5, zorder=1)\n",
        "        ax.axvline(x * 100, color='gray', linestyle='--', alpha=0.5, zorder=1)\n",
        "\n",
        "        # Set title\n",
        "        ax.set_title(f'Eye Tracking Visualization\\nSubject: {subject_id}\\nMean Loss: {mean_loss}')\n",
        "\n",
        "        # Set axis labels\n",
        "        ax.set_xlabel('X Coordinate')\n",
        "        ax.set_ylabel('Y Coordinate')\n",
        "\n",
        "    # Create the animation\n",
        "    anim = animation.FuncAnimation(fig, draw_frame, frames=num_images, interval=interval)\n",
        "\n",
        "  # Use animation's to_jshtml method which directly generates HTML\n",
        "    html_animation = anim.to_jshtml()\n",
        "\n",
        "    # Clean up\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Create an HTML object that can be displayed in the notebook\n",
        "    return HTML(html_animation)"
      ],
      "metadata": {
        "id": "gd9YAdo5w9O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss ~5"
      ],
      "metadata": {
        "id": "vsdKCetqHQLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_loss_5_row = subj_mean_losses_df_5.sample(n=1)\n",
        "rand_loss_5_idx = rand_loss_5_row.index[0]\n",
        "rand_loss_5_id = rand_loss_5_row['subject_id'].iloc[0].astype(int)\n",
        "rand_loss_5_mean_loss = rand_loss_5_row['mean_loss'].iloc[0]"
      ],
      "metadata": {
        "id": "EbjLnAanD9Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eye_animation_loss_5 = visualize_eye_tracking_data(test_ds_p1.skip(rand_loss_5_idx).take(1), rand_loss_5_id, rand_loss_5_mean_loss)\n",
        "eye_animation_loss_5"
      ],
      "metadata": {
        "id": "CCP0EOus0nHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss ~10"
      ],
      "metadata": {
        "id": "-hYTsIQtJiGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_loss_10_row = subj_mean_losses_df_10.sample(n=1)\n",
        "rand_loss_10_idx = rand_loss_10_row.index[0]\n",
        "rand_loss_10_id = rand_loss_10_row['subject_id'].iloc[0].astype(int)\n",
        "rand_loss_10_mean_loss = rand_loss_10_row['mean_loss'].iloc[0]"
      ],
      "metadata": {
        "id": "XqXJvUnKJiGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eye_animation_loss_10 = visualize_eye_tracking_data(test_ds_p1.skip(rand_loss_10_idx).take(1), rand_loss_10_id, rand_loss_10_mean_loss)\n",
        "eye_animation_loss_10"
      ],
      "metadata": {
        "id": "EJ3wg4R6JiGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss ~15"
      ],
      "metadata": {
        "id": "iJFmawrnJpZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_loss_15_row = subj_mean_losses_df_15.sample(n=1)\n",
        "rand_loss_15_idx = rand_loss_15_row.index[0]\n",
        "rand_loss_15_id = rand_loss_15_row['subject_id'].iloc[0].astype(int)\n",
        "rand_loss_15_mean_loss = rand_loss_15_row['mean_loss'].iloc[0]"
      ],
      "metadata": {
        "id": "z9mIRi8oJpZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eye_animation_loss_15 = visualize_eye_tracking_data(test_ds_p1.skip(rand_loss_15_idx).take(1), rand_loss_15_id, rand_loss_15_mean_loss)\n",
        "eye_animation_loss_15"
      ],
      "metadata": {
        "id": "8-YEqa1kJpZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss ~20"
      ],
      "metadata": {
        "id": "Wj0MY6qLJtq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_loss_20_row = subj_mean_losses_df_20.sample(n=1)\n",
        "rand_loss_20_idx = rand_loss_20_row.index[0]\n",
        "rand_loss_20_id = rand_loss_20_row['subject_id'].iloc[0].astype(int)\n",
        "rand_loss_20_mean_loss = rand_loss_20_row['mean_loss'].iloc[0]"
      ],
      "metadata": {
        "id": "d3UI9Q1WJtq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eye_animation_loss_20 = visualize_eye_tracking_data(test_ds_p1.skip(rand_loss_20_idx).take(1), rand_loss_20_id, rand_loss_20_mean_loss)\n",
        "eye_animation_loss_20"
      ],
      "metadata": {
        "id": "Fp-nZnN-Jtq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Subject"
      ],
      "metadata": {
        "id": "F_2TOq4mK3UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eye_animation_loss_best_subj = visualize_eye_tracking_data(test_ds_p1.skip(best_subj_row_idx).take(1), best_subj_row_subject_id, subj_mean_losses_df.iloc[best_subj_row_idx]['mean_loss_p1'])\n",
        "eye_animation_loss_best_subj"
      ],
      "metadata": {
        "id": "3MtJPPA5K3UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Worst Subject (for losses between 0 and 15 in both phases)"
      ],
      "metadata": {
        "id": "MTAv5PzYMi6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eye_animation_loss_worst_subj = visualize_eye_tracking_data(test_ds_p1.skip(worst_subj_row_idx).take(1), worst_subj_row_subject_id, subj_mean_losses_df.iloc[worst_subj_row_idx]['mean_loss_p1'])\n",
        "eye_animation_loss_worst_subj"
      ],
      "metadata": {
        "id": "YptJ4npRMi6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLWqn8aV-JiV"
      },
      "source": [
        "# Effect of number of calibration points on model performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!osf -p th7fv fetch  model_losses_across_cal_configs_df.csv"
      ],
      "metadata": {
        "id": "ze4dA6gEWCiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibration points"
      ],
      "metadata": {
        "id": "LkiPnxE_HA2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4MgPOmF-sMe"
      },
      "outputs": [],
      "source": [
        "cols = [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95]\n",
        "rows = [5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95]\n",
        "\n",
        "cal_cols = [5, 35, 65, 95]\n",
        "cal_rows = [5, 27.5, 50, 72.5, 95]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_points = [[a, b] for a in cols for b in rows]\n",
        "cal_points = [[c, d] for c in cal_cols for d in cal_rows]"
      ],
      "metadata": {
        "id": "_5Tgm2ccx8S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-4wGeWpoTFJ"
      },
      "outputs": [],
      "source": [
        "cols_cols_indices = [\n",
        "    [0,15],\n",
        "    [0,7,15],\n",
        "    [0,5,10,15],\n",
        "    [0,4,8,12,15],\n",
        "    [0,3,6,9,12,15],\n",
        "    [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "]\n",
        "\n",
        "rows_rows_indices = [\n",
        "    [0,8],\n",
        "    [0,4,8],\n",
        "    [0,2,4,6,8],\n",
        "    [0,1,2,3,4,5,6,7,8]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFwAAmeJpBk0"
      },
      "outputs": [],
      "source": [
        "cols_rows_indices = [\n",
        "    [tf.constant([cols[x],rows[y]], dtype=tf.float32)\n",
        "    for x in cols_indices for y in rows_indices]\n",
        "    for cols_indices in cols_cols_indices for rows_indices in rows_rows_indices\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_rows_indices = sorted(cols_rows_indices, key=lambda x: len(x))\n",
        "for i, config in enumerate(cols_rows_indices):\n",
        "  if i > 0 and len(config) == len(cols_rows_indices[i-1]):\n",
        "    cols_rows_indices.pop(i)"
      ],
      "metadata": {
        "id": "vXZCnmZOSr_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaBN0liGrPvq"
      },
      "outputs": [],
      "source": [
        "cols_rows_indices_scaled = [tf.divide(config, tf.constant([100.])) for config in cols_rows_indices]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(zip(*all_points)))"
      ],
      "metadata": {
        "id": "oeJS-6yWyUPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y6AtvOFYH8W"
      },
      "outputs": [],
      "source": [
        "def visualize_points(points):\n",
        "  plt.figure(figsize=(16, 9))\n",
        "  x_coords, y_coords = list(zip(*points))\n",
        "  x_all, y_all = list(zip(*all_points))\n",
        "  plt.scatter(x_all, y_all, alpha=0.3)\n",
        "  plt.scatter(x_coords, y_coords, c='black')\n",
        "\n",
        "  # Set x and y ticks to be the rows and cols\n",
        "  plt.xticks(cols)\n",
        "  plt.yticks(rows)\n",
        "  plt.xlabel(\"% of screen width\", fontsize=16)\n",
        "  plt.ylabel(\"% of screen height\", fontsize=16)\n",
        "  plt.title(\"Calibration Points\", fontsize=20)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_points(cal_points)"
      ],
      "metadata": {
        "id": "4CIA6Xitxs8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter function for subjects missing calibration points"
      ],
      "metadata": {
        "id": "oSwu9LpkNBjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_by_n_cal_points(features, labels, subject_ids, cal_points):\n",
        "  n_cal_points = len(cal_points)\n",
        "  images, coords, cal_mask, target_mask = features\n",
        "\n",
        "  return tf.equal(tf.reduce_sum(cal_mask, axis=-1), n_cal_points)\n"
      ],
      "metadata": {
        "id": "jkfsNDKyKwJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1 boxplot across \\# cal points"
      ],
      "metadata": {
        "id": "bu0RsNH8HCn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_losses_across_cal_phase1 = []\n",
        "for i, cal_config in enumerate(cols_rows_indices_scaled):\n",
        "  # Make masked dataset with the cal_config\n",
        "  masked_dataset_p1_config = prepare_masked_dataset(test_data_rescaled, cal_config, 1)\n",
        "\n",
        "  masked_dataset_p1_config_filtered = masked_dataset_p1_config.filter(\n",
        "      lambda features, labels, subject_ids: filter_by_n_cal_points(\n",
        "          features, labels, subject_ids, cal_config)\n",
        "  )\n",
        "\n",
        "  test_ds_p1_config = masked_dataset_p1_config_filtered.map(\n",
        "      prepare_model_inputs,\n",
        "      num_parallel_calls=tf.data.AUTOTUNE\n",
        "  ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  # Get predictions\n",
        "  predictions_p1_config = full_model.predict(test_ds_p1_config.batch(BATCH_SIZE))\n",
        "\n",
        "  batch_losses_p1_config = []\n",
        "\n",
        "  for pred_batch, ds_batch in zip(predictions_p1_config, test_ds_p1_config.batch(1).as_numpy_iterator()):\n",
        "      y_true = ds_batch[1]\n",
        "      mask = ds_batch[0]['Input_Target_Mask'].reshape(-1)\n",
        "\n",
        "      batch_point_losses = normalized_weighted_euc_dist(y_true, pred_batch).numpy().reshape(-1)\n",
        "      batch_point_losses = batch_point_losses[mask == 1]\n",
        "\n",
        "      batch_losses_p1_config.append(batch_point_losses)\n",
        "  batch_losses_across_cal_phase1.append((cal_config, batch_losses_p1_config))"
      ],
      "metadata": {
        "id": "iLEWktF8B8cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cal_points_phase1 = [len(cal_config) for cal_config, _ in batch_losses_across_cal_phase1]\n",
        "# [len=19 array of\n",
        "    #[len=146 array of\n",
        "        # [len=144 array of losses\n",
        "        # for each point]\n",
        "    # for each subject]\n",
        "# for each cal_config]\n",
        "model_losses_phase1 = [batch_losses for _, batch_losses in batch_losses_across_cal_phase1]"
      ],
      "metadata": {
        "id": "uoHmXeE6Xz2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_data_phase1 = []\n",
        "for i, cal_config in enumerate(model_losses):  # Iterate through 19 cal_configs\n",
        "  for j, subject_model_losses in enumerate(cal_config): # Iterate through 146 subjects\n",
        "      img_count = np.count_nonzero(~np.isnan(subject_model_losses))\n",
        "      row = {\n",
        "          'num_cal_points': num_cal_points[i],\n",
        "          'subject_id': j,\n",
        "          'img_count': img_count,\n",
        "          'subj_mean_loss': np.mean(subject_model_losses)}\n",
        "      row.update({'point_' + str(k) + '_loss': val for k, val in enumerate(subject_model_losses)})\n",
        "      model_losses_df_data_phase1.append(row)\n",
        "\n",
        "model_losses_df_phase1 = pd.DataFrame(model_losses_df_phase1_data)\n",
        "model_losses_df_phase1 = model_losses_df_phase1.sort_values(by=['num_cal_points', 'subject_id'])"
      ],
      "metadata": {
        "id": "nnj444Zs4Qhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase1"
      ],
      "metadata": {
        "id": "5ue0zUYn93vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase1.to_csv('model_losses_across_cal_configs_phase1_df.csv')"
      ],
      "metadata": {
        "id": "UttnzKQ0xrPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_sorted_phase1 = model_losses_df_phase1.sort_values(by='num_cal_points')\n",
        "\n",
        "ordered_cal_points_phase1 = sorted(model_losses_df_phase1['num_cal_points'].unique())\n",
        "ordered_cal_points_str_phase1 = [str(x) for x in ordered_cal_points_phase1]\n",
        "\n",
        "model_losses_df_sorted_phase1['num_cal_points_cat'] = pd.Categorical(\n",
        "    model_losses_df_sorted_phase1['num_cal_points'].astype(str),\n",
        "    categories=ordered_cal_points_str_phase1,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_phase1_boxplot = (\n",
        "    ggplot(model_losses_df_sorted_phase1, aes(x='num_cal_points_cat', y='subj_mean_loss'))\n",
        "    + geom_boxplot()\n",
        "    + labs(\n",
        "        title=\"Model Error Across Number of Calibration Points\",\n",
        "        x=\"#Calibration points\",\n",
        "        y=\"Model error\"\n",
        "    )\n",
        "    + theme_classic()\n",
        "    + theme(\n",
        "        plot_title=element_text(hjust=0.5),\n",
        "        axis_text_x=element_text(angle=45, hjust=1)\n",
        "    )\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_phase1_boxplot"
      ],
      "metadata": {
        "id": "qY6SfYzT8weB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2 boxplot across \\# cal points"
      ],
      "metadata": {
        "id": "dtsSj-j9ajrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_losses_across_cal_phase2 = []\n",
        "for i, cal_config in enumerate(cols_rows_indices_scaled):\n",
        "  # Make masked dataset with the cal_config\n",
        "  masked_dataset_p2_config = prepare_masked_dataset(test_data_rescaled, cal_config, 2)\n",
        "\n",
        "  masked_dataset_p2_config_filtered = masked_dataset_p2_config.filter(\n",
        "      lambda features, labels, subject_ids: filter_by_n_cal_points(\n",
        "          features, labels, subject_ids, cal_config)\n",
        "  )\n",
        "\n",
        "  test_ds_p2_config = masked_dataset_p2_config_filtered.map(\n",
        "      prepare_model_inputs,\n",
        "      num_parallel_calls=tf.data.AUTOTUNE\n",
        "  ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  # Get predictions\n",
        "  predictions_p2_config = full_model.predict(test_ds_p2_config.batch(BATCH_SIZE))\n",
        "\n",
        "  batch_losses_p2_config = []\n",
        "\n",
        "  for pred_batch, ds_batch in zip(predictions_p2_config, test_ds_p2_config.batch(1).as_numpy_iterator()):\n",
        "      y_true = ds_batch[1]\n",
        "      mask = ds_batch[0]['Input_Target_Mask'].reshape(-1)\n",
        "\n",
        "      batch_point_losses = normalized_weighted_euc_dist(y_true, pred_batch).numpy().reshape(-1)\n",
        "      batch_point_losses = batch_point_losses[mask == 1]\n",
        "\n",
        "      batch_losses_p2_config.append(batch_point_losses)\n",
        "  batch_losses_across_cal_phase2.append((cal_config, batch_losses_p2_config))"
      ],
      "metadata": {
        "id": "Ne_oVk0MajrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cal_points_phase2 = [len(cal_config) for cal_config, _ in batch_losses_across_cal_phase2]\n",
        "model_losses_phase2 = [batch_losses for _, batch_losses in batch_losses_across_cal_phase2]"
      ],
      "metadata": {
        "id": "kjUXS91EajrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_data_phase2 = []\n",
        "for i, cal_config in enumerate(model_losses_phase2):  # Iterate through 19 cal_configs\n",
        "  for j, subject_model_losses in enumerate(cal_config): # Iterate through 146 subjects\n",
        "      img_count = np.count_nonzero(~np.isnan(subject_model_losses))\n",
        "      row = {\n",
        "          'num_cal_points': num_cal_points_phase2[i],\n",
        "          'subject_id': j,\n",
        "          'img_count': img_count,\n",
        "          'subj_mean_loss': np.mean(subject_model_losses)}\n",
        "      row.update({'point_' + str(k) + '_loss': val for k, val in enumerate(subject_model_losses)})\n",
        "      model_losses_df_data_phase2.append(row)\n",
        "\n",
        "model_losses_df_phase2 = pd.DataFrame(model_losses_df_data_phase2)\n",
        "model_losses_df_phase2 = model_losses_df_phase2.sort_values(by=['num_cal_points', 'subject_id'])"
      ],
      "metadata": {
        "id": "ZTikdCbUajrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase2"
      ],
      "metadata": {
        "id": "ultZI6HIajrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase2.to_csv('model_losses_across_cal_configs_phase2_df.csv')"
      ],
      "metadata": {
        "id": "NKBWHsa1ajrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_sorted_phase2 = model_losses_df_phase2.sort_values(by='num_cal_points')\n",
        "\n",
        "ordered_cal_points_phase2 = sorted(model_losses_df_phase2['num_cal_points'].unique())\n",
        "ordered_cal_points_str_phase2 = [str(x) for x in ordered_cal_points_phase2]\n",
        "\n",
        "model_losses_df_sorted_phase2['num_cal_points_cat'] = pd.Categorical(\n",
        "    model_losses_df_sorted_phase2['num_cal_points'].astype(str),\n",
        "    categories=ordered_cal_points_str_phase2,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_phase2_boxplot = (\n",
        "    ggplot(model_losses_df_sorted_phase2, aes(x='num_cal_points_cat', y='subj_mean_loss'))\n",
        "    + geom_boxplot()\n",
        "    + labs(\n",
        "        title=\"Model Error Across Number of Calibration Points\",\n",
        "        x=\"#Calibration points\",\n",
        "        y=\"Model error\"\n",
        "    )\n",
        "    + theme_classic()\n",
        "    + theme(\n",
        "        plot_title=element_text(hjust=0.5),\n",
        "        axis_text_x=element_text(angle=45, hjust=1)\n",
        "    )\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_phase2_boxplot"
      ],
      "metadata": {
        "id": "YdyCpN-sajrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1+2 boxplot across \\# cal points"
      ],
      "metadata": {
        "id": "QxJPp53BD0NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_losses_across_cal_phase_both = []\n",
        "for i, cal_config in enumerate(cols_rows_indices_scaled):\n",
        "  # Make masked dataset with the cal_config\n",
        "  masked_dataset_phase_both_cal_config = prepare_masked_dataset(test_data_rescaled, cal_config)\n",
        "\n",
        "  masked_dataset_phase_both_cal_config_filtered = masked_dataset_phase_both_cal_config.filter(\n",
        "      lambda features, labels, subject_ids: filter_by_n_cal_points(\n",
        "          features, labels, subject_ids, cal_config)\n",
        "  )\n",
        "\n",
        "  test_ds_phase_both_cal_config = masked_dataset_phase_both_cal_config_filtered.map(\n",
        "      prepare_model_inputs,\n",
        "      num_parallel_calls=tf.data.AUTOTUNE\n",
        "  ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  # Get predictions\n",
        "  predictions_phase_both_cal_config = full_model.predict(test_ds_phase_both_cal_config.batch(BATCH_SIZE))\n",
        "\n",
        "  batch_losses_phase_both_cal_config = []\n",
        "\n",
        "  for pred_batch, ds_batch in zip(predictions_phase_both_cal_config, test_ds_phase_both_cal_config.batch(1).as_numpy_iterator()):\n",
        "      y_true = ds_batch[1]\n",
        "      mask = ds_batch[0]['Input_Target_Mask'].reshape(-1)\n",
        "\n",
        "      batch_point_losses = normalized_weighted_euc_dist(y_true, pred_batch).numpy().reshape(-1)\n",
        "      batch_point_losses = batch_point_losses[mask == 1]\n",
        "\n",
        "      batch_losses_phase_both_cal_config.append(batch_point_losses)\n",
        "  batch_losses_across_cal_phase_both.append((cal_config, batch_losses_phase_both_cal_config))"
      ],
      "metadata": {
        "id": "ANabWGVlD0NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cal_points_phase_both = [len(cal_config) for cal_config, _ in batch_losses_across_cal_phase_both]\n",
        "model_losses_phase_both = [batch_losses for _, batch_losses in batch_losses_across_cal_phase_both]"
      ],
      "metadata": {
        "id": "G3OH9EN6D0NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_data_phase_both = []\n",
        "for i, cal_config in enumerate(model_losses_phase_both):  # Iterate through 19 cal_configs\n",
        "  for j, subject_model_losses in enumerate(cal_config): # Iterate through 146 subjects\n",
        "      img_count = np.count_nonzero(~np.isnan(subject_model_losses))\n",
        "      row = {\n",
        "          'num_cal_points': num_cal_points_phase_both[i],\n",
        "          'subject_id': j,\n",
        "          'img_count': img_count,\n",
        "          'subj_mean_loss': np.mean(subject_model_losses)}\n",
        "      row.update({'point_' + str(k) + '_loss': val for k, val in enumerate(subject_model_losses)})\n",
        "      model_losses_df_data_phase_both.append(row)\n",
        "\n",
        "model_losses_df_phase_both = pd.DataFrame(model_losses_df_data_phase_both)\n",
        "model_losses_df_phase_both = model_losses_df_phase_both.sort_values(by=['num_cal_points', 'subject_id'])"
      ],
      "metadata": {
        "id": "K6C0Ht2CD0NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase_both"
      ],
      "metadata": {
        "id": "IqTiRBY8D0NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase_both.to_csv('model_losses_across_cal_configs_phase_both_df.csv')"
      ],
      "metadata": {
        "id": "SIypK6VSD0NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_sorted_phase_both = model_losses_df_phase_both.sort_values(by='num_cal_points')\n",
        "\n",
        "ordered_cal_points_phase_both = sorted(model_losses_df_phase_both['num_cal_points'].unique())\n",
        "ordered_cal_points_str_phase_both = [str(x) for x in ordered_cal_points_phase_both]\n",
        "\n",
        "model_losses_df_sorted_phase_both['num_cal_points_cat'] = pd.Categorical(\n",
        "    model_losses_df_sorted_phase_both['num_cal_points'].astype(str),\n",
        "    categories=ordered_cal_points_str_phase_both,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_phase_both_boxplot = (\n",
        "    ggplot(model_losses_df_sorted_phase_both, aes(x='num_cal_points_cat', y='subj_mean_loss'))\n",
        "    + geom_boxplot()\n",
        "    + labs(\n",
        "        title=\"Model Error Across Number of Calibration Points\",\n",
        "        x=\"#Calibration points\",\n",
        "        y=\"Model error\"\n",
        "    )\n",
        "    + theme_classic()\n",
        "    + theme(\n",
        "        plot_title=element_text(hjust=0.5),\n",
        "        axis_text_x=element_text(angle=45, hjust=1)\n",
        "    )\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_phase_both_boxplot"
      ],
      "metadata": {
        "id": "5LrVWr_ED0NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1 ribbon plot across \\# cal points"
      ],
      "metadata": {
        "id": "lrrJTcdcFpcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase1['mean_loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform('mean')\n",
        "model_losses_df_phase1['median_loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform('median')\n",
        "model_losses_df_phase1['min_loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform('min')\n",
        "model_losses_df_phase1['max_loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform('max')"
      ],
      "metadata": {
        "id": "orSdq8FEuUFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase1['2_50 loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 2.5))\n",
        "model_losses_df_phase1['12_5 loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 12.5))\n",
        "model_losses_df_phase1['25_0 loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 25))\n",
        "model_losses_df_phase1['75_0 loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 75))\n",
        "model_losses_df_phase1['87_5 loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 87.5))\n",
        "model_losses_df_phase1['97_5 loss'] = model_losses_df_phase1.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 97.5))"
      ],
      "metadata": {
        "id": "5yNBe53Uwr9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase1"
      ],
      "metadata": {
        "id": "U2IOa72fA1PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ribbon_map = {'2.5% to 97.5%': '#b8f7ff', '12.5% to 87.5%': '#8dc1fc', '25% to 75%': '#5d68de'}\n",
        "\n",
        "model_losses_across_cal_configs_ribbon_plot_phase1 = (\n",
        "    ggplot(model_losses_df_phase1, aes(x='num_cal_points')) +\n",
        "    geom_ribbon(aes(ymin='2_50 loss', ymax='97_5 loss', fill='\"2.5% to 97.5%\"'), alpha=0.6) +\n",
        "    geom_ribbon(aes(ymin='12_5 loss', ymax='87_5 loss', fill='\"12.5% to 87.5%\"'), alpha=0.6) +\n",
        "    geom_ribbon(aes(ymin='25_0 loss', ymax='75_0 loss', fill='\"25% to 75%\"'), alpha=0.6) +\n",
        "    geom_line(aes(y='median_loss'), color='#333', size=1) +\n",
        "    geom_point(aes(y='median_loss'), size=1, alpha=0.05, color='#333') +\n",
        "    geom_point(aes(x='num_cal_points', y='min_loss'), size=0.5, alpha=0.05) +\n",
        "    geom_point(aes(x='num_cal_points', y='max_loss'), size=0.5, alpha=0.05) +\n",
        "    scale_x_continuous(breaks=(4,8,15,20,25,30,36,45,54,80,144), limits=(None, 144)) +\n",
        "    scale_fill_manual(\n",
        "        name=\"% of subjects\",\n",
        "        values=ribbon_map\n",
        "    ) +\n",
        "    labs(\n",
        "        x='#Calibration Points',\n",
        "        y='Model loss',\n",
        "        title='Loss Distribution across different #calibration points (Phase 1)'\n",
        "    ) +\n",
        "    theme(\n",
        "        panel_grid=element_blank(),\n",
        "        panel_background=element_blank(),\n",
        "        plot_background=element_blank(),\n",
        "        axis_line=element_line(color='black', size=1),\n",
        "        axis_text_x=element_text(hjust=1),\n",
        "        axis_title=element_text(),\n",
        "        plot_title=element_text(),\n",
        "        legend_title=element_text(size=10),\n",
        "        legend_text=element_text(size=8),\n",
        "        legend_position=\"bottom\"\n",
        "    )\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_ribbon_plot_phase1"
      ],
      "metadata": {
        "id": "wo67Eu3ctEsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2 ribbon plot across \\# cal points"
      ],
      "metadata": {
        "id": "tbf5NtoTb0fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase2['mean_loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform('mean')\n",
        "model_losses_df_phase2['median_loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform('median')\n",
        "model_losses_df_phase2['min_loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform('min')\n",
        "model_losses_df_phase2['max_loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform('max')"
      ],
      "metadata": {
        "id": "hsqMXy23b0fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase2['2_50 loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 2.5))\n",
        "model_losses_df_phase2['12_5 loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 12.5))\n",
        "model_losses_df_phase2['25_0 loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 25))\n",
        "model_losses_df_phase2['75_0 loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 75))\n",
        "model_losses_df_phase2['87_5 loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 87.5))\n",
        "model_losses_df_phase2['97_5 loss'] = model_losses_df_phase2.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 97.5))"
      ],
      "metadata": {
        "id": "Tk-jWOQub0fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase2"
      ],
      "metadata": {
        "id": "R5TXI0Tab0fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ribbon_map = {'2.5% to 97.5%': '#b8f7ff', '12.5% to 87.5%': '#8dc1fc', '25% to 75%': '#5d68de'}\n",
        "\n",
        "model_losses_across_cal_configs_ribbon_plot_phase2 = (\n",
        "    ggplot(model_losses_df_phase2, aes(x='num_cal_points')) +\n",
        "    geom_ribbon(aes(ymin='2_50 loss', ymax='97_5 loss', fill='\"2.5% to 97.5%\"'), alpha=0.6) +\n",
        "    geom_ribbon(aes(ymin='12_5 loss', ymax='87_5 loss', fill='\"12.5% to 87.5%\"'), alpha=0.6) +\n",
        "    geom_ribbon(aes(ymin='25_0 loss', ymax='75_0 loss', fill='\"25% to 75%\"'), alpha=0.6) +\n",
        "    geom_line(aes(y='median_loss'), color='#333', size=1) +\n",
        "    geom_point(aes(y='median_loss'), size=1, alpha=0.05, color='#333') +\n",
        "    geom_point(aes(x='num_cal_points', y='min_loss'), size=0.5, alpha=0.05) +\n",
        "    geom_point(aes(x='num_cal_points', y='max_loss'), size=0.5, alpha=0.05) +\n",
        "    scale_x_continuous(breaks=(4,8,15,20,25,30,36,45,54,80,144), limits=(None, 144)) +\n",
        "    scale_fill_manual(\n",
        "        name=\"% of subjects\",\n",
        "        values=ribbon_map\n",
        "    ) +\n",
        "    labs(\n",
        "        x='#Calibration Points',\n",
        "        y='Model loss',\n",
        "        title='Loss Distribution across different #calibration points (Phase 2)'\n",
        "    ) +\n",
        "    theme(\n",
        "        panel_grid=element_blank(),\n",
        "        panel_background=element_blank(),\n",
        "        plot_background=element_blank(),\n",
        "        axis_line=element_line(color='black', size=1),\n",
        "        axis_text_x=element_text(hjust=1),\n",
        "        axis_title=element_text(),\n",
        "        plot_title=element_text(),\n",
        "        legend_title=element_text(size=10),\n",
        "        legend_text=element_text(size=8),\n",
        "        legend_position=\"bottom\"\n",
        "    )\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_ribbon_plot_phase2"
      ],
      "metadata": {
        "id": "gyGvRxt1b0fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1+2 ribbon plot across \\# cal points"
      ],
      "metadata": {
        "id": "pPtI5f6uFuap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase_both['mean_loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform('mean')\n",
        "model_losses_df_phase_both['median_loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform('median')\n",
        "model_losses_df_phase_both['min_loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform('min')\n",
        "model_losses_df_phase_both['max_loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform('max')"
      ],
      "metadata": {
        "id": "A9uOHpZMFuaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase_both['2_50 loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 2.5))\n",
        "model_losses_df_phase_both['12_5 loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 12.5))\n",
        "model_losses_df_phase_both['25_0 loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 25))\n",
        "model_losses_df_phase_both['75_0 loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 75))\n",
        "model_losses_df_phase_both['87_5 loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 87.5))\n",
        "model_losses_df_phase_both['97_5 loss'] = model_losses_df_phase_both.groupby('num_cal_points')['subj_mean_loss'].transform(lambda x: np.percentile(x, 97.5))"
      ],
      "metadata": {
        "id": "8CaoFFpAFuar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_losses_df_phase_both"
      ],
      "metadata": {
        "id": "pS6LIXADFuar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ribbon_map = {'2.5% to 97.5%': '#b8f7ff', '12.5% to 87.5%': '#8dc1fc', '25% to 75%': '#5d68de'}\n",
        "\n",
        "model_losses_across_cal_configs_ribbon_plot_phase_both = (\n",
        "    ggplot(model_losses_df_phase_both, aes(x='num_cal_points')) +\n",
        "    geom_ribbon(aes(ymin='2_50 loss', ymax='97_5 loss', fill='\"2.5% to 97.5%\"'), alpha=0.6) +\n",
        "    geom_ribbon(aes(ymin='12_5 loss', ymax='87_5 loss', fill='\"12.5% to 87.5%\"'), alpha=0.6) +\n",
        "    geom_ribbon(aes(ymin='25_0 loss', ymax='75_0 loss', fill='\"25% to 75%\"'), alpha=0.6) +\n",
        "    geom_line(aes(y='median_loss'), color='#333', size=1) +\n",
        "    geom_point(aes(y='median_loss'), size=1, alpha=0.05, color='#333') +\n",
        "    geom_point(aes(x='num_cal_points', y='min_loss'), size=0.5, alpha=0.05) +\n",
        "    geom_point(aes(x='num_cal_points', y='max_loss'), size=0.5, alpha=0.05) +\n",
        "    scale_x_continuous(breaks=(4,8,15,20,25,30,36,45,54,80,144), limits=(None, 144)) +\n",
        "    scale_fill_manual(\n",
        "        name=\"% of subjects\",\n",
        "        values=ribbon_map\n",
        "    ) +\n",
        "    labs(\n",
        "        x='#Calibration Points',\n",
        "        y='Model loss',\n",
        "        title='Loss Distribution across different #calibration points (Phase 2)'\n",
        "    ) +\n",
        "    theme(\n",
        "        panel_grid=element_blank(),\n",
        "        panel_background=element_blank(),\n",
        "        plot_background=element_blank(),\n",
        "        axis_line=element_line(color='black', size=1),\n",
        "        axis_text_x=element_text(hjust=1),\n",
        "        axis_title=element_text(),\n",
        "        plot_title=element_text(),\n",
        "        legend_title=element_text(size=10),\n",
        "        legend_text=element_text(size=8),\n",
        "        legend_position=\"bottom\"\n",
        "    )\n",
        ")\n",
        "\n",
        "model_losses_across_cal_configs_ribbon_plot_phase_both"
      ],
      "metadata": {
        "id": "EFbcMyiIFuar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9k1pHkzWXtE"
      },
      "source": [
        "# Effect of glasses on model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lid2QPQWbk_"
      },
      "outputs": [],
      "source": [
        "!osf -p uf2sh fetch glasses_info.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTsFHAl_vHfG"
      },
      "outputs": [],
      "source": [
        "glasses_df = pd.read_csv('glasses_info.csv')\n",
        "glasses_df['subject_b36'] = glasses_df['subject_id'].apply(lambda x: int(x, 36))\n",
        "glasses_df['wore_glasses'] = glasses_df['wore_glasses'].apply(lambda x: True if x == 'yes' else False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOn6XN6T4bhH"
      },
      "outputs": [],
      "source": [
        "losses_df = pd.DataFrame(columns=[\"subject_b36\", \"batch_loss\"])\n",
        "losses_df[\"subject_b36\"] = subject_ids\n",
        "losses_df['batch_loss'] = batch_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuvvpyOK5qrV"
      },
      "outputs": [],
      "source": [
        "validation_glasses_df = pd.merge(glasses_df, losses_df, on='subject_b36', how=\"inner\")\n",
        "validation_glasses_df = validation_glasses_df.drop(['subject_id'], axis=1)\n",
        "validation_glasses_only_df = validation_glasses_df[validation_glasses_df['wore_glasses'] == True]\n",
        "validation_no_glasses_df = validation_glasses_df[validation_glasses_df['wore_glasses'] == False]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_glasses_df"
      ],
      "metadata": {
        "id": "crcd-1hpI181"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TILTEIdJLI_O"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "n, bins, patches = plt.hist(\n",
        "    [validation_glasses_only_df['batch_loss'], validation_no_glasses_df['batch_loss']],\n",
        "    bins=np.arange(0.01,0.23, 0.01),\n",
        "    edgecolor='white',\n",
        "    label=['Wore glasses', 'Did not wear glasses'],\n",
        "    color=['#333', '#AAA'])\n",
        "plt.title('Effect of Wearing Glasses on Model Error')\n",
        "plt.xlabel('Avg. Error - Proportion of Screen Diagonal')\n",
        "plt.ylabel('# subjects')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spread out vs focused calibration"
      ],
      "metadata": {
        "id": "RlSR_8-4N1WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "middle_cols = [29, 47, 53, 71]\n",
        "middle_rows = [27.5, 50, 72.5,]\n",
        "middle_cols_rows_indices = [tf.constant([x, y], dtype=tf.float32) for y in middle_rows for x in middle_cols]\n",
        "middle_cols_rows_indices_scaled = tf.divide(middle_cols_rows_indices, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "XFcoMY2yN4xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## filter before running predictions"
      ],
      "metadata": {
        "id": "G0gob8OHu6q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "middle_cols_full = [29, 35, 41, 47, 53, 59, 65, 71]\n",
        "middle_rows_full = [27.5, 38.75, 50, 61.25, 72.5,]\n",
        "middle_cols_rows_indices_full = [tf.constant([x, y], dtype=tf.float32) for y in middle_rows_full for x in middle_cols_full]\n",
        "middle_cols_rows_indices_full_scaled = tf.divide(middle_cols_rows_indices_full, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "m1GJOJk3j8X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def middle_filter_cal_points(image, mesh, coords, id):\n",
        "\n",
        "  return tf.reduce_any(tf.reduce_all(tf.equal(coords, middle_cols_rows_indices_full_scaled), axis=1))\n",
        "\n",
        "def middle_reducer_function_fixed_pts_with_id(subject_id, ds):\n",
        "\n",
        "  non_cal_points = ds.batch(144, drop_remainder=True).map(map_for_non_calibration_pts)\n",
        "\n",
        "  points = ds.filter(middle_filter_cal_points).batch(len(middle_cols_rows_indices_full)).map(map_for_calibration_pts).repeat()\n",
        "\n",
        "  merged = tf.data.Dataset.zip(points, non_cal_points)\n",
        "\n",
        "  return merged.map(lambda x, y: map_for_merged_with_id(x, y, subject_id))"
      ],
      "metadata": {
        "id": "lvhYDyUkjQZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## filter after running predictions"
      ],
      "metadata": {
        "id": "eAOupvvgvBVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = middle_cols_rows_indices\n",
        "scaled_cal_points = middle_cols_rows_indices_scaled\n",
        "v = validation_data_rescaled.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ").cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "vT5I0qBqW9lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "middle_batch_losses = []\n",
        "middle_subject_ids = []\n",
        "middle_y_true = []\n",
        "filtered_y_true = []\n",
        "\n",
        "for e in v.batch(1).as_numpy_iterator():\n",
        "    middle_y_true.append(e[1])\n",
        "    middle_subject_ids.append(e[2][0])\n",
        "\n",
        "middle_y_true = np.array(middle_y_true).reshape(-1, 144, 2)\n",
        "middle_predictions = full_model.predict(v.batch(1))\n",
        "\n",
        "for i in range(len(middle_predictions)): # 153\n",
        "  loss = normalized_weighted_euc_dist(middle_y_true[i], middle_predictions[i]).numpy()\n",
        "  new_loss = []\n",
        "  new_y_true = []\n",
        "  for j, trial_loss in enumerate(loss): # 144\n",
        "    if (middle_y_true[i][j][0] > 0.25 and\n",
        "        middle_y_true[i][j][0] < 0.75 and\n",
        "        middle_y_true[i][j][1] > 0.17 and\n",
        "        middle_y_true[i][j][1] < 0.83):\n",
        "      new_loss.append(trial_loss)\n",
        "      new_y_true.append(middle_y_true[i][j])\n",
        "  filtered_y_true.append(new_y_true)\n",
        "  middle_batch_losses.append(new_loss)\n",
        "\n",
        "middle_batch_losses = np.array(middle_batch_losses)\n",
        "\n",
        "# Get mean per subject\n",
        "middle_batch_losses = np.mean(middle_batch_losses, axis=1)"
      ],
      "metadata": {
        "id": "98MNDpwHa-q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outer_cols = [5, 17, 29, 41, 59, 71, 83, 95]\n",
        "outer_rows = [5, 27.25, 50, 72.5, 95]\n",
        "outer_cols_rows_indices = [tf.constant([x, y], dtype=tf.float32) for y in outer_rows for x in outer_cols]\n",
        "outer_cols_rows_indices_scaled = tf.divide(outer_cols_rows_indices, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "dtvXFGRUNPqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = outer_cols_rows_indices\n",
        "scaled_cal_points = outer_cols_rows_indices_scaled\n",
        "v = validation_data_rescaled.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ").cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "apmzSWmNXJ8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outer_batch_losses = []\n",
        "outer_subject_ids = []\n",
        "outer_y_true = []\n",
        "filtered_y_true = []\n",
        "\n",
        "for e in v.batch(1).as_numpy_iterator():\n",
        "    outer_y_true.append(e[1])\n",
        "    outer_subject_ids.append(e[2][0])\n",
        "\n",
        "outer_y_true = np.array(outer_y_true).reshape(-1, 144, 2)\n",
        "outer_predictions = full_model.predict(v.batch(1))\n",
        "\n",
        "for i in range(len(outer_predictions)): # 153\n",
        "  loss = normalized_weighted_euc_dist(outer_y_true[i], outer_predictions[i]).numpy()\n",
        "  new_loss = []\n",
        "  new_y_true = []\n",
        "  for j, trial_loss in enumerate(loss): # 144\n",
        "    if (outer_y_true[i][j][0] > 0.25 and\n",
        "        outer_y_true[i][j][0] < 0.75 and\n",
        "        outer_y_true[i][j][1] > 0.17 and\n",
        "        outer_y_true[i][j][1] < 0.83):\n",
        "      new_loss.append(trial_loss)\n",
        "      new_y_true.append(outer_y_true[i][j])\n",
        "  filtered_y_true.append(new_y_true)\n",
        "  outer_batch_losses.append(new_loss)\n",
        "\n",
        "outer_batch_losses = np.array(outer_batch_losses)\n",
        "\n",
        "# Get mean per subject\n",
        "outer_batch_losses = np.mean(outer_batch_losses, axis=1)"
      ],
      "metadata": {
        "id": "Z7IZ6U2pO_8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "n, bins, patches = plt.hist(\n",
        "    [middle_batch_losses, outer_batch_losses],\n",
        "    bins=np.arange(0.01,0.23, 0.01),\n",
        "    color=['#333', '#AAA'],\n",
        "    edgecolor='white',\n",
        "    label=['Focused calibration', 'Spread out calibration'])\n",
        "plt.title('Focused vs. Spread out calibration')\n",
        "plt.xlabel('Avg. Error - Proportion of Screen Diagonal')\n",
        "plt.ylabel('# subjects')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RaIqHYh-PEhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Horizontal AOIs vs Vertical AOIs"
      ],
      "metadata": {
        "id": "OY6NQx9zN5d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95]\n",
        "rows = [5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95]\n",
        "total_points = tf.constant([[col, row] for col in cols for row in rows], dtype=tf.float32)"
      ],
      "metadata": {
        "id": "AK0T-ivAe5Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "AyPwQj5gN7sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = validation_data_rescaled.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ").cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "5Av6_ZcIFXiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the batch losses\n",
        "subject_ids = []\n",
        "y_true = []\n",
        "\n",
        "for e in v.batch(1).as_numpy_iterator():\n",
        "\n",
        "    y_true.append(e[1])\n",
        "    subject_ids.append(e[2][0])\n",
        "\n",
        "y_true = np.array(y_true).reshape(-1, 144, 2)\n",
        "\n",
        "# this step is slower than expected. not sure what's going on.\n",
        "predictions = full_model.predict(v.batch(1))"
      ],
      "metadata": {
        "id": "y_K2bhsQOCuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_losses = []\n",
        "x_losses = []\n",
        "y_losses = []\n",
        "\n",
        "for i in range(len(predictions)): #153\n",
        "  loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
        "  subject_x_loss = []\n",
        "  subject_y_loss = []\n",
        "  for j in range(len(predictions[i])): #144\n",
        "    subject_trial_x_loss = abs(predictions[i][j][0] - y_true[i][j][0])\n",
        "    subject_trial_y_loss = abs(predictions[i][j][1] - y_true[i][j][1])\n",
        "    subject_x_loss.append(subject_trial_x_loss)\n",
        "    subject_y_loss.append(subject_trial_y_loss)\n",
        "    # subject_x_loss.append(subject_trial_x_loss if subject_trial_x_loss < 0.12 else float('nan'))\n",
        "    # subject_y_loss.append(subject_trial_y_loss if subject_trial_y_loss < 0.12 else float('nan'))\n",
        "  x_losses.append(subject_x_loss)\n",
        "  y_losses.append(subject_y_loss)\n",
        "  batch_losses.append(loss)\n",
        "\n",
        "batch_losses = np.array(batch_losses)\n",
        "x_losses = np.array(x_losses)\n",
        "y_losses = np.array(y_losses)"
      ],
      "metadata": {
        "id": "HGuTNVJdVa0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_y_losses = np.sqrt(np.add(np.square(y_losses.ravel()), np.square(x_losses.ravel())))"
      ],
      "metadata": {
        "id": "3xy2_Jfdi-F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_losses = x_losses.ravel()\n",
        "y_losses = y_losses.ravel()"
      ],
      "metadata": {
        "id": "fb-Nx0U2lB8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "n, bins, patches = plt.hist(y_losses, bins=np.arange(0,0.2, 0.001), edgecolor='white')\n",
        "plt.title('Histogram of Model Error')\n",
        "plt.xlabel('Avg. Error - Proportion of Screen Diagonal')\n",
        "plt.ylabel('# subjects')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vlmVH1aJjffs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_losses_sorted = []\n",
        "y_losses_sorted = []\n",
        "y_true_subject_sorted = []\n",
        "for y_true_subject, *loss_arrays in zip(y_true, *[x_losses, y_losses]):\n",
        "  sorted_indices = sorted(range(len(y_true_subject)), key=lambda i: (y_true_subject[i][0], y_true_subject[i][1]))\n",
        "  y_true_subject_sorted.append([y_true_subject[i] for i in sorted_indices])\n",
        "  x_losses_sorted.append([loss_arrays[0][i] for i in sorted_indices])\n",
        "  y_losses_sorted.append([loss_arrays[1][i] for i in sorted_indices])"
      ],
      "metadata": {
        "id": "-MGvPsm-wXYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_losses_avg = np.nanmean(x_losses_sorted, axis=0)\n",
        "y_losses_avg = np.nanmean(y_losses_sorted, axis=0)"
      ],
      "metadata": {
        "id": "9a3NZC7Bkphc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_losses_avg = x_losses_avg.reshape(9, 16)\n",
        "y_losses_avg = y_losses_avg.reshape(9, 16)"
      ],
      "metadata": {
        "id": "v62LQG8ImqfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(x_losses_avg, vmin=0, vmax=0.12, cmap=\"RdYlBu_r\")\n",
        "\n",
        "ax.set_xticks(np.arange(len(cols)), labels=cols)\n",
        "ax.set_yticks(np.arange(len(rows)), labels=rows)\n",
        "\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(rows)):\n",
        "    for j in range(len(cols)):\n",
        "        text = ax.text(j, i, np.round(x_losses_avg[i, j], decimals=2),\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"Horizontal Loss\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BFXF9S2xJL5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(y_losses_avg, vmin=0, vmax=0.12, cmap=\"RdYlBu_r\")\n",
        "\n",
        "ax.set_xticks(np.arange(len(cols)), labels=cols)\n",
        "ax.set_yticks(np.arange(len(rows)), labels=rows)\n",
        "\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(rows)):\n",
        "    for j in range(len(cols)):\n",
        "        text = ax.text(j, i, np.round(y_losses_avg[i, j], decimals=2),\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"Vertical Loss\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a6lcA34Sona7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_y_losses_avg = np.sqrt(np.add(np.square(y_losses_avg), np.square(x_losses_avg)))"
      ],
      "metadata": {
        "id": "JoukZgVtp-QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(x_y_losses_avg, vmin=0, vmax=0.12, cmap=\"RdYlBu_r\")\n",
        "\n",
        "ax.set_xticks(np.arange(len(cols)), labels=cols)\n",
        "ax.set_yticks(np.arange(len(rows)), labels=rows)\n",
        "\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(rows)):\n",
        "    for j in range(len(cols)):\n",
        "        text = ax.text(j, i, np.round(x_y_losses_avg[i, j], decimals=2),\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"Horizontal + Vertical Loss\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NUIawdEMf_k6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "G9LAJHmSmF84",
        "er9v9bxBcHc0",
        "wKeHGiFxtvo0",
        "kFgWOhWTt4iD",
        "YgNRPZIhncRM",
        "t19yxxx_Ts8H",
        "rzxl5IX0ycoR",
        "HIxNESdIRyue",
        "g9k1pHkzWXtE",
        "RlSR_8-4N1WC",
        "G0gob8OHu6q6"
      ],
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}