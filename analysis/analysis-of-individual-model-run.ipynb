{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LAJHmSmF84"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eW4fMCshWBrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e68013f-2304-4748-d39d-3157c948c7fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for et_util (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install osfclient --quiet\n",
        "!pip install git+https://{cmikul}:{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}@github.com/jspsych/eyetracking-utils.git --quiet\n",
        "!pip install keras_cv --quiet\n",
        "!pip install plotnine --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install keras-hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SQRQZ3D5Vwqc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "import keras_hub\n",
        "from keras import ops\n",
        "import keras_cv\n",
        "from plotnine import ggplot, geom_point, aes, geom_line, scale_y_reverse, theme_void, scale_color_manual, ylab, xlab\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "import et_util.dataset_utils as dataset_utils\n",
        "import et_util.embedding_preprocessing as embed_pre\n",
        "import et_util.model_layers as model_layers\n",
        "from et_util import experiment_utils\n",
        "from et_util.custom_loss import normalized_weighted_euc_dist\n",
        "from et_util.model_analysis import plot_model_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8BUzHdPEc-ef"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
        "os.environ['OSF_USERNAME'] = userdata.get('osfusername')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9v9bxBcHc0"
      },
      "source": [
        "# Configure W&B experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SPGBdleCfbe",
        "outputId": "9446c626-217f-40bb-b49a-25950d078c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed constants\n",
        "MAX_TARGETS = 288\n",
        "\n",
        "# Config constants\n",
        "EMBEDDING_DIM = 200\n",
        "RIDGE_REGULARIZATION = 0.1\n",
        "TRAIN_EPOCHS = 30\n",
        "MIN_CAL_POINTS = 8\n",
        "MAX_CAL_POINTS = 40\n",
        "\n",
        "BACKBONE = \"densenet\"\n",
        "\n",
        "AUGMENTATION = False\n",
        "\n",
        "INITIAL_LEARNING_RATE = 0.00001\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 5\n",
        "TRAIN_EPOCHS = 40\n",
        "WARMUP_EPOCHS = 2\n",
        "DECAY_EPOCHS = TRAIN_EPOCHS - WARMUP_EPOCHS\n",
        "DECAY_ALPHA = 0.01"
      ],
      "metadata": {
        "id": "WQwNaBoP1T9O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeHGiFxtvo0"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset from OSF (Different from trick model set)"
      ],
      "metadata": {
        "id": "b2DUvhFF9oXr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tswV6s38WbtO",
        "outputId": "316a4f8b-2273-45fe-b0af-0bcc7fa018ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 388M/388M [00:02<00:00, 130Mbytes/s]\n"
          ]
        }
      ],
      "source": [
        "!osf -p uf2sh fetch single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgWOhWTt4iD"
      },
      "source": [
        "# Process raw data records into TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G4h8QhGvWhZf"
      },
      "outputs": [],
      "source": [
        "!mkdir single_eye_tfrecords\n",
        "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dZRcQgPadqtI"
      },
      "outputs": [],
      "source": [
        "def parse(element):\n",
        "    \"\"\"Process function that parses a tfr element in a raw dataset for process_tfr_to_tfds function.\n",
        "    Gets mediapipe landmarks, raw image, image width, image height, subject id, and xy labels.\n",
        "    Use for data generated with make_single_example_landmarks_and_jpg (i.e. data in\n",
        "    jpg_landmarks_tfrecords.tar.gz)\n",
        "\n",
        "    :param element: tfr element in raw dataset\n",
        "    :return: image, label(x,y), landmarks, subject_id\n",
        "    \"\"\"\n",
        "\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'phase': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    #landmarks = content['landmarks']\n",
        "    raw_image = content['eye_img']\n",
        "    width = content['img_width']\n",
        "    height = content['img_height']\n",
        "    phase = content['phase']\n",
        "    depth = 3\n",
        "    coords = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    # landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n",
        "    # landmarks = tf.reshape(landmarks, shape=(478, 3))\n",
        "\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, phase, coords, subject_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JWv57eb5XHVP"
      },
      "outputs": [],
      "source": [
        "test_data, _, _ = dataset_utils.process_tfr_to_tfds(\n",
        "    'single_eye_tfrecords/',\n",
        "    parse,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, phase, coords, subject_id: subject_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FULm58Cxu9oa"
      },
      "source": [
        "## Rescale the `x,y` coordinates to be 0-1 instead of 0-100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "apcLK9klHLQV"
      },
      "outputs": [],
      "source": [
        "def rescale_coords_map(img, phase, coords, id):\n",
        "  return img, phase, tf.divide(coords, tf.constant([100.])), id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b47CtWKqHg0B"
      },
      "outputs": [],
      "source": [
        "test_data_rescaled = test_data.map(rescale_coords_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepared masked dataset with phase calibration info"
      ],
      "metadata": {
        "id": "8KfcIpyMrybH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_masked_dataset(dataset, calibration_points=None, cal_phase=None):\n",
        "    # # Step 1: Group dataset by subject_id and batch all images\n",
        "    def group_by_subject(subject_id, ds):\n",
        "        return ds.batch(batch_size=MAX_TARGETS)\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, phase, coords, subject_id: subject_id,\n",
        "        reduce_func=group_by_subject,\n",
        "        window_size=MAX_TARGETS\n",
        "    )\n",
        "\n",
        "    # Step 2: Filter out subjects with not enough data points （288 total; 144 in each phase)\n",
        "    def filter_by_image_count(images, phase, coords, subject_ids):\n",
        "        total_image_count = tf.shape(images)[0] >= MAX_TARGETS\n",
        "        phase1_image_count = tf.reduce_sum(tf.cast(tf.equal(phase, 1), tf.int32)) >= 144\n",
        "        phase2_image_count = tf.reduce_sum(tf.cast(tf.equal(phase, 2), tf.int32)) >= 144\n",
        "        return tf.logical_and(total_image_count, tf.logical_and(phase1_image_count, phase2_image_count))\n",
        "\n",
        "   # grouped_dataset = grouped_dataset.filter(filter_by_image_count)\n",
        "\n",
        "    # Step 3: Transform each batch to include masks\n",
        "    def add_masks_to_batch(images, phase, coords, subject_ids):\n",
        "\n",
        "        actual_batch_size = tf.shape(images)[0]\n",
        "\n",
        "        # Create phase masks\n",
        "        phase1_mask = tf.cast(tf.equal(phase, 1), tf.int8)\n",
        "        phase2_mask = tf.cast(tf.equal(phase, 2), tf.int8)\n",
        "\n",
        "        cal_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n",
        "        target_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n",
        "\n",
        "        if calibration_points is None:\n",
        "          n_cal_images = tf.random.uniform(\n",
        "              shape=[],\n",
        "              minval=MIN_CAL_POINTS,\n",
        "              maxval=MAX_CAL_POINTS,\n",
        "              dtype=tf.int32\n",
        "          )\n",
        "          random_indices = tf.random.shuffle(tf.range(actual_batch_size))\n",
        "          cal_indices = random_indices[:n_cal_images]\n",
        "\n",
        "          cal_mask = tf.scatter_nd(\n",
        "              tf.expand_dims(cal_indices, 1),\n",
        "              tf.ones(n_cal_images, dtype=tf.int8),\n",
        "              [MAX_TARGETS]\n",
        "          )\n",
        "        else:\n",
        "          coords_xpand = tf.expand_dims(coords, axis=1)\n",
        "          cal_xpand = tf.expand_dims(calibration_points, axis=0)\n",
        "\n",
        "          # Check which points match calibration points\n",
        "          equality = tf.equal(coords_xpand, cal_xpand)\n",
        "          matches = tf.reduce_all(equality, axis=-1)\n",
        "          point_matches = tf.reduce_any(matches, axis=1)\n",
        "          cal_mask = tf.cast(point_matches, dtype=tf.int8)\n",
        "\n",
        "        if cal_phase == 1:\n",
        "          cal_mask = cal_mask * phase1_mask\n",
        "        elif cal_phase == 2:\n",
        "          cal_mask = cal_mask * phase2_mask\n",
        "\n",
        "        target_mask = (1 - cal_mask) * phase2_mask\n",
        "\n",
        "        padded_images = tf.pad(\n",
        "            tf.reshape(images, (-1, 36, 144, 1)),\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(\n",
        "            coords,\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n",
        "        )\n",
        "        padded_cal_mask = tf.pad(\n",
        "            cal_mask,\n",
        "            [[0, MAX_TARGETS - actual_batch_size]]\n",
        "        )\n",
        "        padded_target_mask = tf.pad(\n",
        "            target_mask,\n",
        "            [[0, MAX_TARGETS - actual_batch_size]]\n",
        "        )\n",
        "\n",
        "        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(padded_cal_mask, [MAX_TARGETS])\n",
        "        padded_target_mask = tf.ensure_shape(padded_target_mask, [MAX_TARGETS])\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids\n",
        "\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        lambda imgs, meshes, coords, subj_ids: add_masks_to_batch(imgs, meshes, coords, subj_ids),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return masked_dataset"
      ],
      "metadata": {
        "id": "jfdmO7MKba7M"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model_inputs(features, labels, subject_ids):\n",
        "    images, coords, cal_mask, target_mask = features\n",
        "\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": images,\n",
        "        \"Input_All_Coords\": coords,\n",
        "        \"Input_Calibration_Mask\": cal_mask,\n",
        "        \"Input_Target_Mask\": target_mask\n",
        "    }\n",
        "\n",
        "    return inputs, labels, target_mask"
      ],
      "metadata": {
        "id": "WntiqEBSCeOK"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "fgurOg1cq4T8"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_dataset_phase1_cal = prepare_masked_dataset(test_data_rescaled, scaled_cal_points, 1)\n",
        "\n",
        "masked_dataset_phase2_cal = prepare_masked_dataset(test_data_rescaled, scaled_cal_points, 2)"
      ],
      "metadata": {
        "id": "0gac_vkwbfZq"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_dataset_phase1_cal.element_spec"
      ],
      "metadata": {
        "id": "KCWOg8ORotm0",
        "outputId": "3bf90e12-3d5c-46e2-a3f1-2e176933d99c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(288, 36, 144, 1), dtype=tf.uint8, name=None),\n",
              "  TensorSpec(shape=(288, 2), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(288,), dtype=tf.int8, name=None),\n",
              "  TensorSpec(shape=(288,), dtype=tf.int8, name=None)),\n",
              " TensorSpec(shape=(288, 2), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(None,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds_phase1_cal = masked_dataset_phase1_cal.map(\n",
        "    prepare_model_inputs,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds_phase2_cal = masked_dataset_phase2_cal.map(\n",
        "    prepare_model_inputs,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "GlHsEEq0D1Jj"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds_phase2_cal.element_spec"
      ],
      "metadata": {
        "id": "NPNKTQZqrVS2",
        "outputId": "83f27887-a090-478a-949e-ae22028aa43c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Input_All_Images': TensorSpec(shape=(288, 36, 144, 1), dtype=tf.uint8, name=None),\n",
              "  'Input_All_Coords': TensorSpec(shape=(288, 2), dtype=tf.float32, name=None),\n",
              "  'Input_Calibration_Mask': TensorSpec(shape=(288,), dtype=tf.int8, name=None),\n",
              "  'Input_Target_Mask': TensorSpec(shape=(288,), dtype=tf.int8, name=None)},\n",
              " TensorSpec(shape=(288, 2), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(288,), dtype=tf.int8, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# should be the same for either phase cal choices\n",
        "INDIVIDUALS = 0\n",
        "for e in test_ds_phase2_cal.as_numpy_iterator():\n",
        "  INDIVIDUALS += 1"
      ],
      "metadata": {
        "id": "mzwFyg1cD3ZA"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This went down from 238 before to 97 now; worried that filtering in the prepare_mask_dataset function might be wrong\n",
        "INDIVIDUALS"
      ],
      "metadata": {
        "id": "GsI3d1_Kut8A",
        "outputId": "4903f030-f85c-41d1-8664-2d35babbe127",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "146"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgNRPZIhncRM"
      },
      "source": [
        "# Download the Model Config and Weights File from W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1p_UXl0Unfsc"
      },
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "\n",
        "run = api.run(\"vassar-cogsci-lab/eye-tracking-dense-full-data-set-single-eye/v0lc4agm\")\n",
        "\n",
        "config = run.config\n",
        "run.file(\"full_model.weights.h5\").download(exist_ok=True)\n",
        "\n",
        "EMBEDDING_DIM = config[\"embedding_dim\"]\n",
        "RIDGE_REGULARIZATION = config[\"ridge_regularization\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19yxxx_Ts8H"
      },
      "source": [
        "# Re-construct Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom layers"
      ],
      "metadata": {
        "id": "Ow5T7cNRCHb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTimeDistributed(keras.layers.Wrapper):\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.supports_masking = getattr(layer, 'supports_masking', False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not isinstance(input_shape, (tuple, list)) or len(input_shape) < 3:\n",
        "            raise ValueError(\n",
        "                \"`SimpleTimeDistributed` requires input with at least 3 dimensions\"\n",
        "            )\n",
        "\n",
        "        super().build((input_shape[0], *input_shape[2:]))\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        child_output_shape = self.layer.compute_output_shape((input_shape[0], *input_shape[2:]))\n",
        "        return (child_output_shape[0], input_shape[1], *child_output_shape[1:])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        time_steps = input_shape[1]\n",
        "\n",
        "        reshaped_inputs = ops.reshape(inputs, (-1, *input_shape[2:]))\n",
        "\n",
        "        outputs = self.layer.call(reshaped_inputs, training=training)\n",
        "\n",
        "        output_shape = ops.shape(outputs)\n",
        "\n",
        "        return ops.reshape(outputs, (batch_size, time_steps, *output_shape[1:]))"
      ],
      "metadata": {
        "id": "7eXv-aXgnmyt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jGk0aRz_ebqk"
      },
      "outputs": [],
      "source": [
        "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    def __init__(self, lambda_ridge, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        embeddings, coords, calibration_weights, cal_mask  = inputs\n",
        "\n",
        "        embeddings = ops.cast(embeddings, \"float32\")\n",
        "        coords = ops.cast(coords, \"float32\")\n",
        "        calibration_weights = ops.cast(calibration_weights, \"float32\")\n",
        "        cal_mask = ops.cast(cal_mask, \"float32\")\n",
        "\n",
        "        w = ops.squeeze(calibration_weights, axis=-1)\n",
        "\n",
        "        w_masked = w * cal_mask\n",
        "        w_sqrt = ops.sqrt(w_masked + self.epsilon)\n",
        "        w_sqrt = ops.expand_dims(w_sqrt, -1)\n",
        "\n",
        "        cal_mask_expand = ops.expand_dims(cal_mask, -1)\n",
        "        X = embeddings * cal_mask_expand\n",
        "\n",
        "        X_weighted = X * w_sqrt\n",
        "        y_weighted = coords * w_sqrt * cal_mask_expand\n",
        "\n",
        "        X_t = ops.transpose(X_weighted, axes=[0, 2, 1])\n",
        "        X_t_X = ops.matmul(X_t, X_weighted)\n",
        "\n",
        "        identity_matrix = ops.cast(ops.eye(ops.shape(embeddings)[-1]), \"float32\")\n",
        "        lhs = X_t_X + self.lambda_ridge * identity_matrix\n",
        "\n",
        "        rhs = ops.matmul(X_t, y_weighted)\n",
        "\n",
        "        kernel = tf.linalg.solve(lhs, rhs)\n",
        "\n",
        "        output = ops.matmul(embeddings, kernel)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        unknown_embeddings_shape, _, _, _ = input_shapes\n",
        "        return unknown_embeddings_shape[:-1] + (2,)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\"lambda_ridge\": self.lambda_ridge, \"epsilon\": self.epsilon})\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskInspectorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MaskInspectorLayer, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "    def call(self, inputs, mask=None):\n",
        "      tf.print(\"Layer mask:\", mask)\n",
        "      return inputs"
      ],
      "metadata": {
        "id": "rqTTR5I9n0de"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom loss"
      ],
      "metadata": {
        "id": "qCIOW0_pEIjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_weighted_euc_dist(y_true, y_pred):\n",
        "    x_weight = ops.convert_to_tensor([1.778, 1.0], dtype=\"float32\")\n",
        "\n",
        "    y_true_weighted = ops.multiply(x_weight, y_true)\n",
        "    y_pred_weighted = ops.multiply(x_weight, y_pred)\n",
        "\n",
        "    squared_diff = ops.square(y_pred_weighted - y_true_weighted)\n",
        "    squared_dist = ops.sum(squared_diff, axis=-1)\n",
        "    dist = ops.sqrt(squared_dist)\n",
        "\n",
        "    normalized_dist = ops.divide(dist, .0203992)\n",
        "\n",
        "    return normalized_dist"
      ],
      "metadata": {
        "id": "LO6UVbVvyZLM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuZobSgkgbW"
      },
      "source": [
        "## Embedding model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dense_net_backbone():\n",
        "  DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "  return keras_hub.models.DenseNetBackbone(\n",
        "      stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "      image_shape=(36, 144, 1),\n",
        "  )"
      ],
      "metadata": {
        "id": "ue5oH-yT4y20"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GnKs983UsyDj"
      },
      "outputs": [],
      "source": [
        "def create_embedding_model(BACKBONE):\n",
        "  image_shape = (36, 144, 1)\n",
        "  input_eyes = keras.layers.Input(shape=image_shape)\n",
        "\n",
        "  eyes_rescaled = keras.layers.Rescaling(scale=1./255)(input_eyes)\n",
        "\n",
        "  # Continue with the backbone\n",
        "  # backbone = create_rednet_backbone()\n",
        "\n",
        "\n",
        "  # backbone = keras.Sequential([\n",
        "  #     keras.layers.Flatten(),\n",
        "  #     keras.layers.Dense(10, activation=\"relu\")\n",
        "  # ])\n",
        "  if BACKBONE == \"densenet\":\n",
        "    backbone = create_dense_net_backbone()\n",
        "\n",
        "  # backbone = create_efficientnet_backbone()\n",
        "\n",
        "  # backbone = keras_hub.models.MiTBackbone(\n",
        "  #   image_shape=(36,144,1),\n",
        "  #   layerwise_depths=[2,2,2,2],\n",
        "  #   num_layers=4,\n",
        "  #   layerwise_num_heads=[1,2,5,8],\n",
        "  #   layerwise_sr_ratios=[8,4,2,1],\n",
        "  #   max_drop_path_rate=0.1,\n",
        "  #   layerwise_patch_sizes=[7,3,3,3],\n",
        "  #   layerwise_strides=[4,2,2,2],\n",
        "  #   hidden_dims=[32,64,160,256]\n",
        "  # )\n",
        "\n",
        "  backbone_encoder = backbone(eyes_rescaled)\n",
        "  flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "  eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "  embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "\n",
        "  return embedding_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtm4EUSZwXDC"
      },
      "source": [
        "## Full trainable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6JOA5GNqTUW-"
      },
      "outputs": [],
      "source": [
        "def create_masked_model():\n",
        "    input_all_images = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 36, 144, 1),\n",
        "        name=\"Input_All_Images\"\n",
        "    )\n",
        "\n",
        "    input_all_coords = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 2),\n",
        "        name=\"Input_All_Coords\"\n",
        "    )\n",
        "\n",
        "    input_cal_mask = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS,),\n",
        "        name=\"Input_Calibration_Mask\",\n",
        "    )\n",
        "\n",
        "    embedding_model = create_embedding_model(BACKBONE)\n",
        "\n",
        "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\"Image_Embeddings\")(input_all_images)\n",
        "\n",
        "    calibration_weights = keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\"sigmoid\",\n",
        "        name=\"Calibration_Weights\"\n",
        "    )(all_embeddings)\n",
        "\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
        "        RIDGE_REGULARIZATION,\n",
        "        name=\"Regression\"\n",
        "    )(\n",
        "        [\n",
        "            all_embeddings,\n",
        "            input_all_coords,\n",
        "            calibration_weights,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\"MaskedEyePredictionModel\"\n",
        "    )\n",
        "\n",
        "    return full_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "7xAFsV9x41BZ",
        "outputId": "c98ff9ff-8f79-4241-c3f4-e8b83ad3ea4e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"MaskedEyePredictionModel\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MaskedEyePredictionModel\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_All_Images          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m144\u001b[0m,   │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │ \u001b[38;5;34m1\u001b[0m)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Image_Embeddings          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │      \u001b[38;5;34m1,581,896\u001b[0m │ Input_All_Images[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mSimpleTimeDistributed\u001b[0m)   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_All_Coords          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Calibration_Weights       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │            \u001b[38;5;34m201\u001b[0m │ Image_Embeddings[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_Calibration_Mask    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Regression                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ Image_Embeddings[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mMaskedWeightedRidgeRegr…\u001b[0m │                        │                │ Input_All_Coords[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ Calibration_Weights[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ Input_Calibration_Mas… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_All_Images          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>,   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Image_Embeddings          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,581,896</span> │ Input_All_Images[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleTimeDistributed</span>)   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_All_Coords          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Calibration_Weights       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> │ Image_Embeddings[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_Calibration_Mask    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Regression                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Image_Embeddings[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedWeightedRidgeRegr…</span> │                        │                │ Input_All_Coords[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ Calibration_Weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ Input_Calibration_Mas… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,582,097\u001b[0m (6.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,582,097</span> (6.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,574,257\u001b[0m (6.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,257</span> (6.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,840\u001b[0m (30.62 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,840</span> (30.62 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "full_model = create_masked_model()\n",
        "full_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_model.load_weights('full_model.weights.h5')\n",
        "full_model.summary()\n",
        "\n",
        "full_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=normalized_weighted_euc_dist,\n",
        "    metrics=[normalized_weighted_euc_dist],\n",
        "    jit_compile = False\n",
        ")"
      ],
      "metadata": {
        "id": "dRGwIhvnXBOa",
        "outputId": "8268b429-74cc-45c3-b320-7d36d76a3840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"MaskedEyePredictionModel\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MaskedEyePredictionModel\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_All_Images          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m144\u001b[0m,   │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │ \u001b[38;5;34m1\u001b[0m)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Image_Embeddings          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │      \u001b[38;5;34m1,581,896\u001b[0m │ Input_All_Images[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mSimpleTimeDistributed\u001b[0m)   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_All_Coords          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Calibration_Weights       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │            \u001b[38;5;34m201\u001b[0m │ Image_Embeddings[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_Calibration_Mask    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Regression                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ Image_Embeddings[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mMaskedWeightedRidgeRegr…\u001b[0m │                        │                │ Input_All_Coords[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ Calibration_Weights[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ Input_Calibration_Mas… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_All_Images          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>,   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Image_Embeddings          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,581,896</span> │ Input_All_Images[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleTimeDistributed</span>)   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_All_Coords          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Calibration_Weights       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> │ Image_Embeddings[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_Calibration_Mask    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Regression                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Image_Embeddings[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedWeightedRidgeRegr…</span> │                        │                │ Input_All_Coords[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ Calibration_Weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ Input_Calibration_Mas… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,582,097\u001b[0m (6.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,582,097</span> (6.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,574,257\u001b[0m (6.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,257</span> (6.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,840\u001b[0m (30.62 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,840</span> (30.62 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzxl5IX0ycoR"
      },
      "source": [
        "# Get model predictions on val data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_phase_1 = full_model.predict(test_ds_phase1_cal.batch(BATCH_SIZE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQJEayJ_yQvy",
        "outputId": "a187e78c-d5e7-4b71-e2be-2150e1db2c44"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "collapsed": true,
        "id": "_OUDhetR5LdO"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store the batch losses\n",
        "batch_losses = []\n",
        "subject_ids = []\n",
        "\n",
        "for pred_batch, ds_batch in zip(predictions_phase_1, test_ds_phase1_cal.batch(1).as_numpy_iterator()):\n",
        "    # Extract ground truth and subject ID from dataset batch\n",
        "    y_true = ds_batch[1]\n",
        "    subject_id = ds_batch[2][0]\n",
        "\n",
        "    # Calculate losses for all points in this batch\n",
        "    batch_point_losses = normalized_weighted_euc_dist(y_true, pred_batch).numpy()\n",
        "\n",
        "    # Store average loss for this batch and subject ID\n",
        "    batch_losses.append(batch_point_losses)\n",
        "    subject_ids.append(subject_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subject_avg_losses = [np.mean(losses) for losses in batch_losses]"
      ],
      "metadata": {
        "id": "Jb8IhRMw1j9x",
        "collapsed": true
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "blgWDH3E5Swd",
        "outputId": "30238e18-bcdc-45f3-b414-6a7d91d41e6e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "n, bins, patches = plt.hist(subject_avg_losses, bins=np.arange(1,23, 1), color='#333', edgecolor='white')\n",
        "plt.title('Distribution of Model Error')\n",
        "plt.xlabel('Avg. Error - Proportion of Screen Diagonal')\n",
        "plt.ylabel('# subjects')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_phase_2 = full_model.predict(test_ds_phase2_cal.batch(BATCH_SIZE))"
      ],
      "metadata": {
        "id": "Mr68-eUzsSLt",
        "outputId": "7fa92696-4585-42a6-8c8b-5e820f8954cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "XSfQ2msf_DrL"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store the batch losses\n",
        "batch_losses_2 = []\n",
        "subject_ids_2 = []\n",
        "\n",
        "for pred_batch, ds_batch in zip(predictions_phase_2, test_ds_phase2_cal.batch(1).as_numpy_iterator()):\n",
        "    # Extract ground truth and subject ID from dataset batch\n",
        "    y_true = ds_batch[1]\n",
        "    subject_id = ds_batch[2][0]\n",
        "\n",
        "    # Calculate losses for all points in this batch\n",
        "    batch_point_losses = normalized_weighted_euc_dist(y_true, pred_batch).numpy()\n",
        "\n",
        "    # Store average loss for this batch and subject ID\n",
        "    batch_losses_2.append(batch_point_losses)\n",
        "    subject_ids_2.append(subject_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subject_avg_losses_2 = [np.mean(losses) for losses in batch_losses_2]"
      ],
      "metadata": {
        "id": "CQYwc_00W1Ku"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "n, bins, patches = plt.hist(subject_avg_losses_2, bins=np.arange(1,23,1), color='#333', edgecolor='white')\n",
        "plt.title('Distribution of Model Error')\n",
        "plt.xlabel('Avg. Error - Proportion of Screen Diagonal')\n",
        "plt.ylabel('# subjects')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "RqmBGZtcWyG_",
        "outputId": "81640410-e6bc-4688-9ef7-ca869408a3df"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIxNESdIRyue"
      },
      "source": [
        "# Explore model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjjMINThRkLa"
      },
      "outputs": [],
      "source": [
        "validation_subject_loss = pd.DataFrame(columns=[\"subject_id\", \"loss\"])\n",
        "validation_subject_loss[\"subject_id\"] = subject_ids\n",
        "validation_subject_loss[\"loss\"] = batch_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHH4IQMKSfHl"
      },
      "outputs": [],
      "source": [
        "validation_subject_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqx2ntqt-vKR"
      },
      "outputs": [],
      "source": [
        "np.median(validation_subject_loss['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw-OsZcNTMR_"
      },
      "outputs": [],
      "source": [
        "val_subject_id_to_explore = 69\n",
        "\n",
        "print(batch_losses[val_subject_id_to_explore])\n",
        "\n",
        "v_predictions = full_model.predict(v.skip(val_subject_id_to_explore).take(1).batch(1))[0]\n",
        "\n",
        "v_pts = []\n",
        "\n",
        "for e in v.skip(val_subject_id_to_explore).take(1).as_numpy_iterator():\n",
        "  v_pts = e[1]\n",
        "\n",
        "v_errors = np.linalg.norm(v_predictions - v_pts, axis=1)\n",
        "\n",
        "dfA = pd.DataFrame({\"sample\": range(len(v_pts)), \"x\": v_pts[:,0], \"y\": v_pts[:,1], \"error\": v_errors, \"point_type\": \"target\"})\n",
        "dfB = pd.DataFrame({\"sample\": range(len(v_pts)), \"x\": v_predictions[:,0], \"y\": v_predictions[:,1], \"error\": v_errors, \"point_type\": \"prediction\"})\n",
        "\n",
        "v_df = pd.concat([dfA,dfB])\n",
        "\n",
        "(ggplot(v_df, aes(x=\"x\", y=\"y\", color=\"point_type\", group=\"sample\"))+\n",
        "  geom_point()+\n",
        "  geom_line(color=\"darkred\")+\n",
        "  scale_y_reverse()+\n",
        "  scale_color_manual(values=[\"darkred\", \"gray\"], guide=False)+\n",
        "  theme_void())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6lJpVOVFLBT"
      },
      "outputs": [],
      "source": [
        "v_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiJ7vqyYIjv3"
      },
      "outputs": [],
      "source": [
        "def visualize_eye_pairs(eyes, coords, other_image, weights=None):\n",
        "\n",
        "  fig, ax = plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 0.2]})\n",
        "\n",
        "  for i in range(eyes.shape[0]):\n",
        "    image = np.flipud(np.fliplr(eyes[i]))\n",
        "\n",
        "    x, y = coords[i]\n",
        "\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "\n",
        "    ax[0].imshow(image, extent = [x-(width/100/10), x+(width/100/10), y-(height/100/10), y+(height/100/10)], cmap='gray', origin='upper')\n",
        "    if weights is not None:\n",
        "      ax[0].text(x, y-(height/100/10)-0.05, f'{weights[i]:.2f}', ha='center', va='top', color='red')\n",
        "\n",
        "  ax[0].set_xlim([-0.2,1.2])\n",
        "  ax[0].set_ylim([1,0])\n",
        "\n",
        "  ax[1].imshow(np.fliplr(other_image), cmap='gray', origin='upper')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN22r8JVKgVK"
      },
      "outputs": [],
      "source": [
        "z = [d for d in v.skip(val_subject_id_to_explore).take(1).as_numpy_iterator()][0]\n",
        "images = z[0][0]\n",
        "coords = z[0][1]\n",
        "target = z[0][2][135]\n",
        "visualize_eye_pairs(images, coords, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lSsb5gMUEQz"
      },
      "outputs": [],
      "source": [
        "reg_weights_output = keras.Model(inputs=full_model.input, outputs=full_model.get_layer(\"Calibration_Weights_Reshaped\").output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCZZZd3AUMuM"
      },
      "outputs": [],
      "source": [
        "ww = reg_weights_output.predict(v.skip(val_subject_id_to_explore).take(1).batch(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43JB8g3qURsr"
      },
      "outputs": [],
      "source": [
        "weights_for_cal_points = np.column_stack((z[0][1], ww[0]))\n",
        "\n",
        "df_weights = pd.DataFrame(weights_for_cal_points, columns=[\"x\", \"y\", \"weight\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2v7X62pdcpQ"
      },
      "outputs": [],
      "source": [
        "visualize_eye_pairs(images, coords, target, weights=ww[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9k1pHkzWXtE"
      },
      "source": [
        "# Effect of glasses on model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lid2QPQWbk_"
      },
      "outputs": [],
      "source": [
        "!osf -p yezk8 fetch glasses_info.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTsFHAl_vHfG"
      },
      "outputs": [],
      "source": [
        "glasses_df = pd.read_csv('glasses_info.csv')\n",
        "glasses_df['subject_b36'] = glasses_df['subject_id'].apply(lambda x: int(x, 36))\n",
        "glasses_df['wore_glasses'] = glasses_df['wore_glasses'].apply(lambda x: True if x == 'yes' else False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOn6XN6T4bhH"
      },
      "outputs": [],
      "source": [
        "losses_df = pd.DataFrame(columns=[\"subject_b36\", \"batch_loss\"])\n",
        "losses_df[\"subject_b36\"] = subject_ids\n",
        "losses_df['batch_loss'] = batch_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuvvpyOK5qrV"
      },
      "outputs": [],
      "source": [
        "validation_glasses_df = pd.merge(glasses_df, losses_df, on='subject_b36', how=\"inner\")\n",
        "validation_glasses_df = validation_glasses_df.drop(['subject_id'], axis=1)\n",
        "validation_glasses_only_df = validation_glasses_df[validation_glasses_df['wore_glasses'] == True]\n",
        "validation_no_glasses_df = validation_glasses_df[validation_glasses_df['wore_glasses'] == False]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_glasses_df"
      ],
      "metadata": {
        "id": "crcd-1hpI181"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TILTEIdJLI_O"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "n, bins, patches = plt.hist(\n",
        "    [validation_glasses_only_df['batch_loss'], validation_no_glasses_df['batch_loss']],\n",
        "    bins=np.arange(0.01,0.23, 0.01),\n",
        "    edgecolor='white',\n",
        "    label=['Wore glasses', 'Did not wear glasses'],\n",
        "    color=['#333', '#AAA'])\n",
        "plt.title('Effect of Wearing Glasses on Model Error')\n",
        "plt.xlabel('Avg. Error - Proportion of Screen Diagonal')\n",
        "plt.ylabel('# subjects')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLWqn8aV-JiV"
      },
      "source": [
        "# Effect of number of calibration points on model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4MgPOmF-sMe"
      },
      "outputs": [],
      "source": [
        "cols = [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95]\n",
        "rows = [5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95]\n",
        "\n",
        "cal_cols = [5, 35, 65, 95]\n",
        "cal_rows = [5, 27.5, 50, 72.5, 95]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_points = [[a, b] for a in cols for b in rows]\n",
        "cal_points = [[c, d] for c in cal_cols for d in cal_rows]"
      ],
      "metadata": {
        "id": "_5Tgm2ccx8S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-4wGeWpoTFJ"
      },
      "outputs": [],
      "source": [
        "cols_cols_indices = [\n",
        "    [0,15],\n",
        "    [0,7,15],\n",
        "    [0,5,10,15],\n",
        "    [0,4,8,12,15],\n",
        "    [0,3,6,9,12,15],\n",
        "    [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "]\n",
        "\n",
        "rows_rows_indices = [\n",
        "    [0,8],\n",
        "    [0,4,8],\n",
        "    [0,2,4,6,8],\n",
        "    [0,1,2,3,4,5,6,7,8]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFwAAmeJpBk0"
      },
      "outputs": [],
      "source": [
        "cols_rows_indices = [\n",
        "    [tf.constant([cols[x],rows[y]], dtype=tf.float32)\n",
        "    for x in cols_indices for y in rows_indices]\n",
        "    for cols_indices in cols_cols_indices for rows_indices in rows_rows_indices\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_rows_indices = sorted(cols_rows_indices, key=lambda x: len(x))\n",
        "for i, config in enumerate(cols_rows_indices):\n",
        "  if i > 0 and len(config) == len(cols_rows_indices[i-1]):\n",
        "    cols_rows_indices.pop(i)"
      ],
      "metadata": {
        "id": "vXZCnmZOSr_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaBN0liGrPvq"
      },
      "outputs": [],
      "source": [
        "cols_rows_indices_scaled = [tf.divide(config, tf.constant([100.])) for config in cols_rows_indices]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(zip(*all_points)))"
      ],
      "metadata": {
        "id": "oeJS-6yWyUPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y6AtvOFYH8W"
      },
      "outputs": [],
      "source": [
        "def visualize_points(points):\n",
        "  plt.figure(figsize=(16, 9))\n",
        "  x_coords, y_coords = list(zip(*points))\n",
        "  x_all, y_all = list(zip(*all_points))\n",
        "  plt.scatter(x_all, y_all, alpha=0.3)\n",
        "  plt.scatter(x_coords, y_coords, c='black')\n",
        "\n",
        "  # Set x and y ticks to be the rows and cols\n",
        "  plt.xticks(cols)\n",
        "  plt.yticks(rows)\n",
        "  plt.xlabel(\"% of screen width\", fontsize=16)\n",
        "  plt.ylabel(\"% of screen height\", fontsize=16)\n",
        "  plt.title(\"Calibration Points\", fontsize=20)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_points(cal_points)"
      ],
      "metadata": {
        "id": "4CIA6Xitxs8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_losses_across_cal = []\n",
        "for i, cal_config in enumerate(cols_rows_indices):\n",
        "  batch_losses = []\n",
        "  subject_ids = []\n",
        "  y_true = []\n",
        "  cal_points = cal_config\n",
        "  scaled_cal_points = cols_rows_indices_scaled[i]\n",
        "  v = validation_data_rescaled.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        "    ).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  for e in v.batch(1).as_numpy_iterator():\n",
        "    y_true.append(e[1])\n",
        "    subject_ids.append(e[2][0])\n",
        "\n",
        "  y_true = np.array(y_true).reshape(-1, 144, 2)\n",
        "  predictions = full_model.predict(v.batch(1))\n",
        "\n",
        "  for i in range(len(predictions)):\n",
        "    loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
        "    batch_losses.append(loss)\n",
        "\n",
        "  batch_losses = np.array(batch_losses)\n",
        "  batch_losses_across_cal.append([cal_config, np.mean(batch_losses, axis=1)])"
      ],
      "metadata": {
        "id": "iLEWktF8B8cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cal_points = [len(cal_config) for cal_config, _ in batch_losses_across_cal]\n",
        "errors = [batch_losses for _, batch_losses in batch_losses_across_cal]"
      ],
      "metadata": {
        "id": "uoHmXeE6Xz2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors_column = np.vstack(errors).reshape(-1)"
      ],
      "metadata": {
        "id": "iHf5cgpUw4p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points_column = [[x]*153 for x in num_cal_points]\n",
        "cal_points_column = np.array(cal_points_column).reshape(-1)"
      ],
      "metadata": {
        "id": "U8mdsdtixGK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points_data = pd.DataFrame({'cal_points': cal_points_column, 'error': errors_column})"
      ],
      "metadata": {
        "id": "klPfIwmGxUO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points_data.to_csv('cal_points_data.csv')"
      ],
      "metadata": {
        "id": "UttnzKQ0xrPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot([batch_losses for cal_config, batch_losses in batch_losses_across_cal], labels=[len(cal_config) for cal_config, batch_losses in batch_losses_across_cal])\n",
        "plt.title('Model Error Across Number of Calibration Points')\n",
        "plt.xlabel('# calibration points')\n",
        "plt.ylabel('Model error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qY6SfYzT8weB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!osf -p nry8g fetch cal_points_data.csv"
      ],
      "metadata": {
        "id": "oKwYnGnstAQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points_data = pd.read_csv('cal_points_data.csv')"
      ],
      "metadata": {
        "id": "gt7Z_14QuOw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points_data['mean_error'] = cal_points_data.groupby('cal_points')['error'].transform('mean')\n",
        "cal_points_data['median_error'] = cal_points_data.groupby('cal_points')['error'].transform('median')\n",
        "cal_points_data['min_error'] = cal_points_data.groupby('cal_points')['error'].transform('min')\n",
        "cal_points_data['max_error'] = cal_points_data.groupby('cal_points')['error'].transform('max')"
      ],
      "metadata": {
        "id": "orSdq8FEuUFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points_data['2_50 error'] = cal_points_data.groupby('cal_points')['error'].transform(lambda x: np.percentile(x, 2.5))\n",
        "cal_points_data['12_5 error'] = cal_points_data.groupby('cal_points')['error'].transform(lambda x: np.percentile(x, 12.5))\n",
        "cal_points_data['25_0 error'] = cal_points_data.groupby('cal_points')['error'].transform(lambda x: np.percentile(x, 25))\n",
        "cal_points_data['75_0 error'] = cal_points_data.groupby('cal_points')['error'].transform(lambda x: np.percentile(x, 75))\n",
        "cal_points_data['87_5 error'] = cal_points_data.groupby('cal_points')['error'].transform(lambda x: np.percentile(x, 87.5))\n",
        "cal_points_data['97_5 error'] = cal_points_data.groupby('cal_points')['error'].transform(lambda x: np.percentile(x, 97.5))"
      ],
      "metadata": {
        "id": "5yNBe53Uwr9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points_data"
      ],
      "metadata": {
        "id": "U2IOa72fA1PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import (ggplot, aes, geom_line, geom_vline, geom_ribbon, geom_text,\n",
        "                     stat_summary, theme, element_blank, element_line, element_text,\n",
        "                     scale_x_continuous, scale_fill_manual, labs)\n",
        "\n",
        "ribbon_map = {'2.5% to 97.5%': '#b8f7ff', '12.5% to 87.5%': '#8dc1fc', '25% to 75%': '#5d68de'}\n",
        "\n",
        "(\n",
        "    ggplot(cal_points_data, aes(x='cal_points')) +\n",
        "    geom_ribbon(aes(ymin='2_50 error', ymax='97_5 error', fill='\"2.5% to 97.5%\"'), alpha=0.6) +\n",
        "    geom_ribbon(aes(ymin='12_5 error', ymax='87_5 error', fill='\"12.5% to 87.5%\"'), alpha=0.6) +\n",
        "    geom_ribbon(aes(ymin='25_0 error', ymax='75_0 error', fill='\"25% to 75%\"'), alpha=0.6) +\n",
        "    geom_line(aes(y='median_error'), color='#333', size=1) +\n",
        "    geom_point(aes(y='median_error'), size=1, alpha=0.05, color='#333') +\n",
        "    geom_point(aes(x='cal_points', y='min_error'), size=0.5, alpha=0.05) +\n",
        "    geom_point(aes(x='cal_points', y='max_error'), size=0.5, alpha=0.05) +\n",
        "    scale_x_continuous(breaks=(4, 8, 12, 18, 25, 30, 36, 45, 54, 80), limits=(None, 80)) +\n",
        "    scale_fill_manual(\n",
        "        name=\"% of subjects\",\n",
        "        values=ribbon_map\n",
        "    ) +\n",
        "    labs(\n",
        "        x='# Calibration Points',\n",
        "        y='Model error',\n",
        "        title='Error Distribution across different # calibration points'\n",
        "    ) +\n",
        "    theme(\n",
        "        panel_grid=element_blank(),\n",
        "        panel_background=element_blank(),\n",
        "        plot_background=element_blank(),\n",
        "        axis_line=element_line(color='black', size=1),\n",
        "        axis_text_x=element_text(hjust=1),\n",
        "        axis_title=element_text(),\n",
        "        plot_title=element_text(),\n",
        "        legend_title=element_text(size=10),\n",
        "        legend_text=element_text(size=8),\n",
        "        legend_position=\"right\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "wo67Eu3ctEsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points"
      ],
      "metadata": {
        "id": "txbO72NcN4nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spread out vs focused calibration"
      ],
      "metadata": {
        "id": "RlSR_8-4N1WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "middle_cols = [29, 47, 53, 71]\n",
        "middle_rows = [27.5, 50, 72.5,]\n",
        "middle_cols_rows_indices = [tf.constant([x, y], dtype=tf.float32) for y in middle_rows for x in middle_cols]\n",
        "middle_cols_rows_indices_scaled = tf.divide(middle_cols_rows_indices, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "XFcoMY2yN4xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## filter before running predictions"
      ],
      "metadata": {
        "id": "G0gob8OHu6q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "middle_cols_full = [29, 35, 41, 47, 53, 59, 65, 71]\n",
        "middle_rows_full = [27.5, 38.75, 50, 61.25, 72.5,]\n",
        "middle_cols_rows_indices_full = [tf.constant([x, y], dtype=tf.float32) for y in middle_rows_full for x in middle_cols_full]\n",
        "middle_cols_rows_indices_full_scaled = tf.divide(middle_cols_rows_indices_full, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "m1GJOJk3j8X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def middle_filter_cal_points(image, mesh, coords, id):\n",
        "\n",
        "  return tf.reduce_any(tf.reduce_all(tf.equal(coords, middle_cols_rows_indices_full_scaled), axis=1))\n",
        "\n",
        "def middle_reducer_function_fixed_pts_with_id(subject_id, ds):\n",
        "\n",
        "  non_cal_points = ds.batch(144, drop_remainder=True).map(map_for_non_calibration_pts)\n",
        "\n",
        "  points = ds.filter(middle_filter_cal_points).batch(len(middle_cols_rows_indices_full)).map(map_for_calibration_pts).repeat()\n",
        "\n",
        "  merged = tf.data.Dataset.zip(points, non_cal_points)\n",
        "\n",
        "  return merged.map(lambda x, y: map_for_merged_with_id(x, y, subject_id))"
      ],
      "metadata": {
        "id": "lvhYDyUkjQZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## filter after running predictions"
      ],
      "metadata": {
        "id": "eAOupvvgvBVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = middle_cols_rows_indices\n",
        "scaled_cal_points = middle_cols_rows_indices_scaled\n",
        "v = validation_data_rescaled.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ").cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "vT5I0qBqW9lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "middle_batch_losses = []\n",
        "middle_subject_ids = []\n",
        "middle_y_true = []\n",
        "filtered_y_true = []\n",
        "\n",
        "for e in v.batch(1).as_numpy_iterator():\n",
        "    middle_y_true.append(e[1])\n",
        "    middle_subject_ids.append(e[2][0])\n",
        "\n",
        "middle_y_true = np.array(middle_y_true).reshape(-1, 144, 2)\n",
        "middle_predictions = full_model.predict(v.batch(1))\n",
        "\n",
        "for i in range(len(middle_predictions)): # 153\n",
        "  loss = normalized_weighted_euc_dist(middle_y_true[i], middle_predictions[i]).numpy()\n",
        "  new_loss = []\n",
        "  new_y_true = []\n",
        "  for j, trial_loss in enumerate(loss): # 144\n",
        "    if (middle_y_true[i][j][0] > 0.25 and\n",
        "        middle_y_true[i][j][0] < 0.75 and\n",
        "        middle_y_true[i][j][1] > 0.17 and\n",
        "        middle_y_true[i][j][1] < 0.83):\n",
        "      new_loss.append(trial_loss)\n",
        "      new_y_true.append(middle_y_true[i][j])\n",
        "  filtered_y_true.append(new_y_true)\n",
        "  middle_batch_losses.append(new_loss)\n",
        "\n",
        "middle_batch_losses = np.array(middle_batch_losses)\n",
        "\n",
        "# Get mean per subject\n",
        "middle_batch_losses = np.mean(middle_batch_losses, axis=1)"
      ],
      "metadata": {
        "id": "98MNDpwHa-q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outer_cols = [5, 17, 29, 41, 59, 71, 83, 95]\n",
        "outer_rows = [5, 27.25, 50, 72.5, 95]\n",
        "outer_cols_rows_indices = [tf.constant([x, y], dtype=tf.float32) for y in outer_rows for x in outer_cols]\n",
        "outer_cols_rows_indices_scaled = tf.divide(outer_cols_rows_indices, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "dtvXFGRUNPqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = outer_cols_rows_indices\n",
        "scaled_cal_points = outer_cols_rows_indices_scaled\n",
        "v = validation_data_rescaled.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ").cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "apmzSWmNXJ8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outer_batch_losses = []\n",
        "outer_subject_ids = []\n",
        "outer_y_true = []\n",
        "filtered_y_true = []\n",
        "\n",
        "for e in v.batch(1).as_numpy_iterator():\n",
        "    outer_y_true.append(e[1])\n",
        "    outer_subject_ids.append(e[2][0])\n",
        "\n",
        "outer_y_true = np.array(outer_y_true).reshape(-1, 144, 2)\n",
        "outer_predictions = full_model.predict(v.batch(1))\n",
        "\n",
        "for i in range(len(outer_predictions)): # 153\n",
        "  loss = normalized_weighted_euc_dist(outer_y_true[i], outer_predictions[i]).numpy()\n",
        "  new_loss = []\n",
        "  new_y_true = []\n",
        "  for j, trial_loss in enumerate(loss): # 144\n",
        "    if (outer_y_true[i][j][0] > 0.25 and\n",
        "        outer_y_true[i][j][0] < 0.75 and\n",
        "        outer_y_true[i][j][1] > 0.17 and\n",
        "        outer_y_true[i][j][1] < 0.83):\n",
        "      new_loss.append(trial_loss)\n",
        "      new_y_true.append(outer_y_true[i][j])\n",
        "  filtered_y_true.append(new_y_true)\n",
        "  outer_batch_losses.append(new_loss)\n",
        "\n",
        "outer_batch_losses = np.array(outer_batch_losses)\n",
        "\n",
        "# Get mean per subject\n",
        "outer_batch_losses = np.mean(outer_batch_losses, axis=1)"
      ],
      "metadata": {
        "id": "Z7IZ6U2pO_8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "n, bins, patches = plt.hist(\n",
        "    [middle_batch_losses, outer_batch_losses],\n",
        "    bins=np.arange(0.01,0.23, 0.01),\n",
        "    color=['#333', '#AAA'],\n",
        "    edgecolor='white',\n",
        "    label=['Focused calibration', 'Spread out calibration'])\n",
        "plt.title('Focused vs. Spread out calibration')\n",
        "plt.xlabel('Avg. Error - Proportion of Screen Diagonal')\n",
        "plt.ylabel('# subjects')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RaIqHYh-PEhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Horizontal AOIs vs Vertical AOIs"
      ],
      "metadata": {
        "id": "OY6NQx9zN5d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95]\n",
        "rows = [5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95]\n",
        "total_points = tf.constant([[col, row] for col in cols for row in rows], dtype=tf.float32)"
      ],
      "metadata": {
        "id": "AK0T-ivAe5Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "AyPwQj5gN7sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = validation_data_rescaled.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ").cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "5Av6_ZcIFXiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the batch losses\n",
        "subject_ids = []\n",
        "y_true = []\n",
        "\n",
        "for e in v.batch(1).as_numpy_iterator():\n",
        "\n",
        "    y_true.append(e[1])\n",
        "    subject_ids.append(e[2][0])\n",
        "\n",
        "y_true = np.array(y_true).reshape(-1, 144, 2)\n",
        "\n",
        "# this step is slower than expected. not sure what's going on.\n",
        "predictions = full_model.predict(v.batch(1))"
      ],
      "metadata": {
        "id": "y_K2bhsQOCuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_losses = []\n",
        "x_losses = []\n",
        "y_losses = []\n",
        "\n",
        "for i in range(len(predictions)): #153\n",
        "  loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
        "  subject_x_loss = []\n",
        "  subject_y_loss = []\n",
        "  for j in range(len(predictions[i])): #144\n",
        "    subject_trial_x_loss = abs(predictions[i][j][0] - y_true[i][j][0])\n",
        "    subject_trial_y_loss = abs(predictions[i][j][1] - y_true[i][j][1])\n",
        "    subject_x_loss.append(subject_trial_x_loss)\n",
        "    subject_y_loss.append(subject_trial_y_loss)\n",
        "    # subject_x_loss.append(subject_trial_x_loss if subject_trial_x_loss < 0.12 else float('nan'))\n",
        "    # subject_y_loss.append(subject_trial_y_loss if subject_trial_y_loss < 0.12 else float('nan'))\n",
        "  x_losses.append(subject_x_loss)\n",
        "  y_losses.append(subject_y_loss)\n",
        "  batch_losses.append(loss)\n",
        "\n",
        "batch_losses = np.array(batch_losses)\n",
        "x_losses = np.array(x_losses)\n",
        "y_losses = np.array(y_losses)"
      ],
      "metadata": {
        "id": "HGuTNVJdVa0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_y_losses = np.sqrt(np.add(np.square(y_losses.ravel()), np.square(x_losses.ravel())))"
      ],
      "metadata": {
        "id": "3xy2_Jfdi-F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_losses = x_losses.ravel()\n",
        "y_losses = y_losses.ravel()"
      ],
      "metadata": {
        "id": "fb-Nx0U2lB8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "n, bins, patches = plt.hist(y_losses, bins=np.arange(0,0.2, 0.001), edgecolor='white')\n",
        "plt.title('Histogram of Model Error')\n",
        "plt.xlabel('Avg. Error - Proportion of Screen Diagonal')\n",
        "plt.ylabel('# subjects')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vlmVH1aJjffs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_losses_sorted = []\n",
        "y_losses_sorted = []\n",
        "y_true_subject_sorted = []\n",
        "for y_true_subject, *loss_arrays in zip(y_true, *[x_losses, y_losses]):\n",
        "  sorted_indices = sorted(range(len(y_true_subject)), key=lambda i: (y_true_subject[i][0], y_true_subject[i][1]))\n",
        "  y_true_subject_sorted.append([y_true_subject[i] for i in sorted_indices])\n",
        "  x_losses_sorted.append([loss_arrays[0][i] for i in sorted_indices])\n",
        "  y_losses_sorted.append([loss_arrays[1][i] for i in sorted_indices])"
      ],
      "metadata": {
        "id": "-MGvPsm-wXYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_losses_avg = np.nanmean(x_losses_sorted, axis=0)\n",
        "y_losses_avg = np.nanmean(y_losses_sorted, axis=0)"
      ],
      "metadata": {
        "id": "9a3NZC7Bkphc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_losses_avg = x_losses_avg.reshape(9, 16)\n",
        "y_losses_avg = y_losses_avg.reshape(9, 16)"
      ],
      "metadata": {
        "id": "v62LQG8ImqfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(x_losses_avg, vmin=0, vmax=0.12, cmap=\"RdYlBu_r\")\n",
        "\n",
        "ax.set_xticks(np.arange(len(cols)), labels=cols)\n",
        "ax.set_yticks(np.arange(len(rows)), labels=rows)\n",
        "\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(rows)):\n",
        "    for j in range(len(cols)):\n",
        "        text = ax.text(j, i, np.round(x_losses_avg[i, j], decimals=2),\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"Horizontal Loss\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BFXF9S2xJL5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(y_losses_avg, vmin=0, vmax=0.12, cmap=\"RdYlBu_r\")\n",
        "\n",
        "ax.set_xticks(np.arange(len(cols)), labels=cols)\n",
        "ax.set_yticks(np.arange(len(rows)), labels=rows)\n",
        "\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(rows)):\n",
        "    for j in range(len(cols)):\n",
        "        text = ax.text(j, i, np.round(y_losses_avg[i, j], decimals=2),\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"Vertical Loss\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a6lcA34Sona7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_y_losses_avg = np.sqrt(np.add(np.square(y_losses_avg), np.square(x_losses_avg)))"
      ],
      "metadata": {
        "id": "JoukZgVtp-QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(x_y_losses_avg, vmin=0, vmax=0.12, cmap=\"RdYlBu_r\")\n",
        "\n",
        "ax.set_xticks(np.arange(len(cols)), labels=cols)\n",
        "ax.set_yticks(np.arange(len(rows)), labels=rows)\n",
        "\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(rows)):\n",
        "    for j in range(len(cols)):\n",
        "        text = ax.text(j, i, np.round(x_y_losses_avg[i, j], decimals=2),\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"Horizontal + Vertical Loss\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NUIawdEMf_k6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "G9LAJHmSmF84",
        "er9v9bxBcHc0",
        "wKeHGiFxtvo0",
        "kFgWOhWTt4iD",
        "YgNRPZIhncRM",
        "t19yxxx_Ts8H",
        "rzxl5IX0ycoR",
        "HIxNESdIRyue",
        "g9k1pHkzWXtE",
        "RlSR_8-4N1WC",
        "G0gob8OHu6q6"
      ],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}