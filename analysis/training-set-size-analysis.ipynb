{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqVpiZAOSGbr"
      },
      "source": [
        "# Training Set Size Analysis\n",
        "\n",
        "This notebook analyzes how the number of unique participants in the training set impacts the model's performance.\n",
        "\n",
        "The goal is to understand:\n",
        "- How model performance scales with training set size (number of unique participants)\n",
        "- Whether there are diminishing returns as we add more participants\n",
        "- What is the minimum viable training set size for acceptable performance\n",
        "\n",
        "## Plan:\n",
        "1. Load the training dataset and examine participant distribution\n",
        "2. Create function to sample subset of participants from the training set\n",
        "3. Train models on subsets of varying sizes (10%, 20%, ..., 100% of unique participants)\n",
        "4. Evaluate each model's performance on the fixed test set\n",
        "5. Log training runs in Weights & Biases\n",
        "6. Generate performance plots and histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPSVPgzuSGbu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-K9SUGk5SGbu",
        "outputId": "d06ab30c-cae6-45ed-9eb1-1e7eca6a5e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for et_util (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install osfclient --quiet\n",
        "!pip install git+https://github.com/jspsych/eyetracking-utils.git --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install --upgrade keras --quiet\n",
        "!pip install keras-hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lCO0sYzfSGbv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import keras\n",
        "import keras_hub\n",
        "from keras import ops\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import userdata\n",
        "\n",
        "import et_util.dataset_utils as dataset_utils\n",
        "import et_util.embedding_preprocessing as embed_pre\n",
        "import et_util.model_layers as model_layers\n",
        "from et_util import experiment_utils\n",
        "from et_util.custom_loss import normalized_weighted_euc_dist\n",
        "from et_util.model_analysis import plot_model_performance\n",
        "\n",
        "import random\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bPY68-PSGbv"
      },
      "outputs": [],
      "source": [
        "# Set environment variables for API access\n",
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
        "os.environ['OSF_USERNAME'] = userdata.get('osfusername')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SaL9Up6TSGbv",
        "outputId": "6b1a6269-1a0b-4d7e-ad78-d1479d546dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.10.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "keras.version()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yxUCOq_hSGbw"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rzXwi_7xSGbw",
        "outputId": "b84471f5-4060-45c3-ae9a-31198a0d7bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will train 10 models with different participant subset sizes\n",
            "Participant percentages: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n"
          ]
        }
      ],
      "source": [
        "# Fixed constants - matching the analysis notebook\n",
        "MAX_TARGETS = 288  # 144 per phase\n",
        "\n",
        "# Config constants - using same as original model for consistency\n",
        "EMBEDDING_DIM = 200\n",
        "RIDGE_REGULARIZATION = 0.1\n",
        "MIN_CAL_POINTS = 8\n",
        "MAX_CAL_POINTS = 40\n",
        "\n",
        "BACKBONE = \"densenet\"\n",
        "\n",
        "# Reduced training parameters for faster experimentation\n",
        "INITIAL_LEARNING_RATE = 0.00001\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 5\n",
        "TRAIN_EPOCHS = 20  # Reduced from 50 for faster iteration\n",
        "WARMUP_EPOCHS = 1  # Reduced from 2\n",
        "DECAY_EPOCHS = TRAIN_EPOCHS - WARMUP_EPOCHS\n",
        "DECAY_ALPHA = 0.01\n",
        "\n",
        "# Augmentation disabled for consistent comparison\n",
        "AUGMENTATION = False\n",
        "\n",
        "# Analysis-specific parameters\n",
        "PARTICIPANT_PERCENTAGES = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "print(f\"Will train {len(PARTICIPANT_PERCENTAGES)} models with different participant subset sizes\")\n",
        "print(f\"Participant percentages: {PARTICIPANT_PERCENTAGES}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N9S_yjupSGbw",
        "outputId": "8b745eae-5975-4d43-cfec-2230d0123eb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 2.50G/2.50G [02:36<00:00, 16.0Mbytes/s]\n"
          ]
        }
      ],
      "source": [
        "# Download training dataset from OSF (original training project)\n",
        "if not os.path.exists('training_single_eye_tfrecords.tar.gz'):\n",
        "    !osf -p 6b5cd fetch single_eye_tfrecords.tar.gz training_single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HVhoADt4SGbw",
        "outputId": "8a173285-1f77-41f0-fc2f-b85d6430443e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: osf fetch [-h] [-f] [-U] remote [local]\n",
            "osf fetch: error: Please set a username (run `osf -h` for details).\n"
          ]
        }
      ],
      "source": [
        "# Download test dataset from the correct OSF project (used in analysis notebook)\n",
        "if not os.path.exists('single_eye_tfrecords.tar.gz'):\n",
        "    !osf -p uf2sh fetch single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZmLyHnOSGbx"
      },
      "outputs": [],
      "source": [
        "# Extract training dataset\n",
        "if not os.path.exists('training_single_eye_tfrecords'):\n",
        "    !mkdir training_single_eye_tfrecords\n",
        "    !tar -xf training_single_eye_tfrecords.tar.gz -C training_single_eye_tfrecords\n",
        "\n",
        "# Extract test dataset\n",
        "if not os.path.exists('single_eye_tfrecords'):\n",
        "    !mkdir single_eye_tfrecords\n",
        "    !tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hb7xN_HSGbx"
      },
      "outputs": [],
      "source": [
        "def parse_training(element):\n",
        "    \"\"\"Process function for training data (no phase information).\"\"\"\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    landmarks = content['landmarks']\n",
        "    raw_image = content['eye_img']\n",
        "    label = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n",
        "    landmarks = tf.reshape(landmarks, shape=(478, 3))\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, landmarks, label, subject_id\n",
        "\n",
        "def parse_test(element):\n",
        "    \"\"\"Process function for test data (includes phase information).\"\"\"\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'phase': tf.io.FixedLenFeature([], tf.int64),  # Phase information for test data\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    raw_image = content['eye_img']\n",
        "    phase = content['phase']\n",
        "    coords = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, phase, coords, subject_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C76WnvfISGbx"
      },
      "outputs": [],
      "source": [
        "# Load the training dataset (for training different models)\n",
        "train_data, _, _ = dataset_utils.process_tfr_to_tfds(\n",
        "    'training_single_eye_tfrecords/',\n",
        "    parse_training,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, landmarks, coords, subject_id: subject_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeUvJ01_SGbx"
      },
      "outputs": [],
      "source": [
        "# Load the test dataset (this will be used for evaluation)\n",
        "test_data, _, _ = dataset_utils.process_tfr_to_tfds(\n",
        "    'single_eye_tfrecords/',\n",
        "    parse_test,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, phase, coords, subject_id: subject_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Iz86fZSGbx"
      },
      "outputs": [],
      "source": [
        "def rescale_coords_map(eyes, mesh, coords, id):\n",
        "    return eyes, mesh, tf.divide(coords, tf.constant([100.])), id\n",
        "\n",
        "train_data_rescaled = train_data.map(rescale_coords_map)\n",
        "test_data_rescaled = test_data.map(rescale_coords_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzC7qQyqSGbx"
      },
      "source": [
        "## Analyze Participant Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qXZwrzeSGbx"
      },
      "outputs": [],
      "source": [
        "# Collect all unique participant IDs from the training dataset\n",
        "participant_ids = set()\n",
        "participant_counts = defaultdict(int)\n",
        "\n",
        "print(\"Analyzing participant distribution in training data...\")\n",
        "for i, (image, landmarks, coords, subject_id) in enumerate(train_data_rescaled):\n",
        "    if i % 10000 == 0:\n",
        "        print(f\"Processed {i} samples...\")\n",
        "\n",
        "    subject_id_np = subject_id.numpy()\n",
        "    participant_ids.add(subject_id_np)\n",
        "    participant_counts[subject_id_np] += 1\n",
        "\n",
        "participant_ids = sorted(list(participant_ids))\n",
        "total_participants = len(participant_ids)\n",
        "total_samples = sum(participant_counts.values())\n",
        "\n",
        "print(f\"Training dataset statistics:\")\n",
        "print(f\"Total unique participants: {total_participants}\")\n",
        "print(f\"Total samples: {total_samples}\")\n",
        "print(f\"Average samples per participant: {total_samples / total_participants:.1f}\")\n",
        "print(f\"Min samples per participant: {min(participant_counts.values())}\")\n",
        "print(f\"Max samples per participant: {max(participant_counts.values())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSQlvQViSGby"
      },
      "outputs": [],
      "source": [
        "# Plot distribution of samples per participant\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "samples_per_participant = list(participant_counts.values())\n",
        "plt.hist(samples_per_participant, bins=30, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('Samples per Participant')\n",
        "plt.ylabel('Number of Participants')\n",
        "plt.title('Distribution of Samples per Participant')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(samples_per_participant)\n",
        "plt.ylabel('Samples per Participant')\n",
        "plt.title('Boxplot of Samples per Participant')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Median samples per participant: {np.median(samples_per_participant):.1f}\")\n",
        "print(f\"Standard deviation: {np.std(samples_per_participant):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jltZqGLaSGby"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ4216mFSGby"
      },
      "outputs": [],
      "source": [
        "def sample_participants(participant_ids, percentage, random_seed=None):\n",
        "    \"\"\"Sample a percentage of participants from the full list.\n",
        "\n",
        "    Args:\n",
        "        participant_ids: List of all participant IDs\n",
        "        percentage: Float between 0 and 1 indicating what percentage to sample\n",
        "        random_seed: Optional random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        List of sampled participant IDs\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "\n",
        "    n_participants = int(len(participant_ids) * percentage)\n",
        "    sampled_ids = random.sample(participant_ids, n_participants)\n",
        "\n",
        "    print(f\"Sampled {len(sampled_ids)} participants ({percentage*100:.1f}%) from {len(participant_ids)} total\")\n",
        "    return sampled_ids\n",
        "\n",
        "\n",
        "def filter_dataset_by_participants(dataset, participant_ids_to_keep):\n",
        "    \"\"\"Filter dataset to only include data from specified participants.\n",
        "\n",
        "    Args:\n",
        "        dataset: TensorFlow dataset\n",
        "        participant_ids_to_keep: Set of participant IDs to keep\n",
        "\n",
        "    Returns:\n",
        "        Filtered TensorFlow dataset\n",
        "    \"\"\"\n",
        "    participant_ids_set = set(participant_ids_to_keep)\n",
        "\n",
        "    def filter_func(image, landmarks, coords, subject_id):\n",
        "        return tf.py_function(\n",
        "            lambda sid: sid.numpy() in participant_ids_set,\n",
        "            [subject_id],\n",
        "            tf.bool\n",
        "        )\n",
        "\n",
        "    return dataset.filter(filter_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwAhXBEUSGby"
      },
      "outputs": [],
      "source": [
        "# Copy the dataset preparation functions from the original notebook\n",
        "def prepare_masked_dataset(dataset, calibration_points=None):\n",
        "    # Step 1: Group dataset by subject_id and batch all images\n",
        "    def group_by_subject(subject_id, ds):\n",
        "        return ds.batch(batch_size=MAX_TARGETS)\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, mesh, coords, subject_id: subject_id,\n",
        "        reduce_func=group_by_subject,\n",
        "        window_size=MAX_TARGETS\n",
        "    )\n",
        "\n",
        "    # Step 2: Filter out subjects with fewer than 144 images\n",
        "    def filter_by_image_count(images, meshes, coords, subject_ids):\n",
        "        return tf.shape(images)[0] >= 144\n",
        "\n",
        "    grouped_dataset = grouped_dataset.filter(filter_by_image_count)\n",
        "\n",
        "    # Step 3: Transform each batch to include masks\n",
        "    def add_masks_to_batch(images, meshes, coords, subject_ids):\n",
        "        actual_batch_size = tf.shape(images)[0]\n",
        "\n",
        "        cal_mask = tf.zeros(MAX_TARGETS, dtype=tf.int8)\n",
        "        target_mask = tf.zeros(MAX_TARGETS, dtype=tf.int8)\n",
        "\n",
        "        # Determine how many calibration images to use\n",
        "        if calibration_points is None:\n",
        "            n_cal_images = tf.random.uniform(\n",
        "                shape=[],\n",
        "                minval=MIN_CAL_POINTS,\n",
        "                maxval=MAX_CAL_POINTS,\n",
        "                dtype=tf.int32\n",
        "            )\n",
        "            random_indices = tf.random.shuffle(tf.range(actual_batch_size))\n",
        "            cal_indices = random_indices[:n_cal_images]\n",
        "\n",
        "            cal_mask = tf.scatter_nd(\n",
        "                tf.expand_dims(cal_indices, 1),\n",
        "                tf.ones(n_cal_images, dtype=tf.int8),\n",
        "                [MAX_TARGETS]\n",
        "            )\n",
        "        else:\n",
        "            coords_xpand = tf.expand_dims(coords, axis=1)\n",
        "            cal_xpand = tf.expand_dims(calibration_points, axis=0)\n",
        "            equality = tf.equal(coords_xpand, cal_xpand)\n",
        "            matches = tf.reduce_all(equality, axis=-1)\n",
        "            point_matches = tf.reduce_any(matches, axis=1)\n",
        "            cal_mask = tf.cast(point_matches, dtype=tf.int8)\n",
        "\n",
        "        target_mask = 1 - cal_mask\n",
        "\n",
        "        # Pad everything to fixed size\n",
        "        padded_images = tf.pad(\n",
        "            tf.reshape(images, (-1, 36, 144, 1)),\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(\n",
        "            coords,\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n",
        "        )\n",
        "\n",
        "        # Ensure all shapes are fixed\n",
        "        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(cal_mask, [MAX_TARGETS])\n",
        "        padded_target_mask = tf.ensure_shape(target_mask, [MAX_TARGETS])\n",
        "\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids\n",
        "\n",
        "    # Apply the transformation\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        lambda imgs, meshes, coords, subj_ids: add_masks_to_batch(imgs, meshes, coords, subj_ids),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return masked_dataset\n",
        "\n",
        "\n",
        "def prepare_model_inputs(features, labels, subject_ids):\n",
        "    \"\"\"Restructure the inputs for the model, using the mask-based approach.\"\"\"\n",
        "    images, coords, cal_mask, target_mask = features\n",
        "\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": images,\n",
        "        \"Input_All_Coords\": coords,\n",
        "        \"Input_Calibration_Mask\": cal_mask,\n",
        "    }\n",
        "\n",
        "    return inputs, labels, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5VcS5jKSGby"
      },
      "outputs": [],
      "source": [
        "# Copy the model architecture from the original notebook\n",
        "class SimpleTimeDistributed(keras.layers.Wrapper):\n",
        "    \"\"\"A simplified version of TimeDistributed that applies a layer to every temporal slice of an input.\"\"\"\n",
        "\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.supports_masking = getattr(layer, 'supports_masking', False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not isinstance(input_shape, (tuple, list)) or len(input_shape) < 3:\n",
        "            raise ValueError(\n",
        "                \"`SimpleTimeDistributed` requires input with at least 3 dimensions\"\n",
        "            )\n",
        "        super().build((input_shape[0], *input_shape[2:]))\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        child_output_shape = self.layer.compute_output_shape((input_shape[0], *input_shape[2:]))\n",
        "        return (child_output_shape[0], input_shape[1], *child_output_shape[1:])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        time_steps = input_shape[1]\n",
        "\n",
        "        reshaped_inputs = ops.reshape(inputs, (-1, *input_shape[2:]))\n",
        "        outputs = self.layer.call(reshaped_inputs, training=training)\n",
        "        output_shape = ops.shape(outputs)\n",
        "\n",
        "        return ops.reshape(outputs, (batch_size, time_steps, *output_shape[1:]))\n",
        "\n",
        "\n",
        "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    \"\"\"A custom layer that performs weighted ridge regression with proper masking support.\"\"\"\n",
        "\n",
        "    def __init__(self, lambda_ridge, epsilon=1e-7, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        embeddings, coords, calibration_weights, cal_mask = inputs\n",
        "\n",
        "        embeddings = ops.cast(embeddings, \"float32\")\n",
        "        coords = ops.cast(coords, \"float32\")\n",
        "        calibration_weights = ops.cast(calibration_weights, \"float32\")\n",
        "        cal_mask = ops.cast(cal_mask, \"float32\")\n",
        "\n",
        "        w = ops.squeeze(calibration_weights, axis=-1)\n",
        "        w_masked = w * cal_mask\n",
        "        w_sqrt = ops.sqrt(w_masked + self.epsilon)\n",
        "        w_sqrt = ops.expand_dims(w_sqrt, -1)\n",
        "\n",
        "        cal_mask_expand = ops.expand_dims(cal_mask, -1)\n",
        "        X = embeddings * cal_mask_expand\n",
        "        X_weighted = X * w_sqrt\n",
        "        y_weighted = coords * w_sqrt * cal_mask_expand\n",
        "\n",
        "        X_t = ops.transpose(X_weighted, axes=[0, 2, 1])\n",
        "        X_t_X = ops.matmul(X_t, X_weighted)\n",
        "\n",
        "        identity_matrix = ops.cast(ops.eye(ops.shape(embeddings)[-1]), \"float32\")\n",
        "        lhs = X_t_X + self.lambda_ridge * identity_matrix\n",
        "        rhs = ops.matmul(X_t, y_weighted)\n",
        "        kernel = ops.linalg.solve(lhs, rhs)\n",
        "        output = ops.matmul(embeddings, kernel)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        return (input_shapes[0][0], input_shapes[0][1], 2)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"lambda_ridge\": self.lambda_ridge,\n",
        "            \"epsilon\": self.epsilon\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "def create_dense_net_backbone():\n",
        "    DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "    return keras_hub.models.DenseNetBackbone(\n",
        "        stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "        image_shape=(36, 144, 1),\n",
        "    )\n",
        "\n",
        "\n",
        "def create_embedding_model(backbone_type):\n",
        "    image_shape = (36, 144, 1)\n",
        "    input_eyes = keras.layers.Input(shape=image_shape)\n",
        "    eyes_rescaled = keras.layers.Rescaling(scale=1./255)(input_eyes)\n",
        "\n",
        "    if backbone_type == \"densenet\":\n",
        "        backbone = create_dense_net_backbone()\n",
        "\n",
        "    backbone_encoder = backbone(eyes_rescaled)\n",
        "    flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "    eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "    embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "    return embedding_model\n",
        "\n",
        "\n",
        "def create_masked_model():\n",
        "    \"\"\"Create a model that uses masks to distinguish calibration and target images.\"\"\"\n",
        "    input_all_images = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 36, 144, 1),\n",
        "        name=\"Input_All_Images\"\n",
        "    )\n",
        "    input_all_coords = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 2),\n",
        "        name=\"Input_All_Coords\"\n",
        "    )\n",
        "    input_cal_mask = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS,),\n",
        "        name=\"Input_Calibration_Mask\",\n",
        "    )\n",
        "\n",
        "    embedding_model = create_embedding_model(BACKBONE)\n",
        "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\"Image_Embeddings\")(input_all_images)\n",
        "\n",
        "    calibration_weights = keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\"sigmoid\",\n",
        "        name=\"Calibration_Weights\"\n",
        "    )(all_embeddings)\n",
        "\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
        "        RIDGE_REGULARIZATION,\n",
        "        name=\"Regression\"\n",
        "    )([\n",
        "        all_embeddings,\n",
        "        input_all_coords,\n",
        "        calibration_weights,\n",
        "        input_cal_mask,\n",
        "    ])\n",
        "\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\"MaskedEyePredictionModel\"\n",
        "    )\n",
        "\n",
        "    return full_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFMVbA3wSGby"
      },
      "outputs": [],
      "source": [
        "# Copy the training dataset preparation functions from the original notebook\n",
        "def prepare_masked_dataset_training(dataset, calibration_points=None):\n",
        "    # Step 1: Group dataset by subject_id and batch all images\n",
        "    def group_by_subject(subject_id, ds):\n",
        "        return ds.batch(batch_size=144)  # Training uses 144 targets\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, landmarks, coords, subject_id: subject_id,\n",
        "        reduce_func=group_by_subject,\n",
        "        window_size=144\n",
        "    )\n",
        "\n",
        "    # Step 2: Filter out subjects with fewer than 144 images\n",
        "    def filter_by_image_count(images, landmarks, coords, subject_ids):\n",
        "        return tf.shape(images)[0] >= 144\n",
        "\n",
        "    grouped_dataset = grouped_dataset.filter(filter_by_image_count)\n",
        "\n",
        "    # Step 3: Transform each batch to include masks\n",
        "    def add_masks_to_batch(images, landmarks, coords, subject_ids):\n",
        "        actual_batch_size = tf.shape(images)[0]\n",
        "\n",
        "        cal_mask = tf.zeros(144, dtype=tf.int8)\n",
        "        target_mask = tf.zeros(144, dtype=tf.int8)\n",
        "\n",
        "        # Determine how many calibration images to use (random between min and max)\n",
        "        if calibration_points is None:\n",
        "            n_cal_images = tf.random.uniform(\n",
        "                shape=[],\n",
        "                minval=MIN_CAL_POINTS,\n",
        "                maxval=MAX_CAL_POINTS,\n",
        "                dtype=tf.int32\n",
        "            )\n",
        "            # Create random indices for calibration images\n",
        "            random_indices = tf.random.shuffle(tf.range(actual_batch_size))\n",
        "            cal_indices = random_indices[:n_cal_images]\n",
        "\n",
        "            # Create masks (1 = included, 0 = excluded)\n",
        "            cal_mask = tf.scatter_nd(\n",
        "                tf.expand_dims(cal_indices, 1),\n",
        "                tf.ones(n_cal_images, dtype=tf.int8),\n",
        "                [144]\n",
        "            )\n",
        "        else:\n",
        "            coords_xpand = tf.expand_dims(coords, axis=1)\n",
        "            cal_xpand = tf.expand_dims(calibration_points, axis=0)\n",
        "            equality = tf.equal(coords_xpand, cal_xpand)\n",
        "            matches = tf.reduce_all(equality, axis=-1)\n",
        "            point_matches = tf.reduce_any(matches, axis=1)\n",
        "            cal_mask = tf.cast(point_matches, dtype=tf.int8)\n",
        "\n",
        "        target_mask = 1 - cal_mask\n",
        "\n",
        "        # Pad everything to fixed size\n",
        "        padded_images = tf.pad(\n",
        "            tf.reshape(images, (-1, 36, 144, 1)),\n",
        "            [[0, 144 - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(\n",
        "            coords,\n",
        "            [[0, 144 - actual_batch_size], [0, 0]]\n",
        "        )\n",
        "\n",
        "        # Ensure all shapes are fixed\n",
        "        padded_images = tf.ensure_shape(padded_images, [144, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [144, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(cal_mask, [144])\n",
        "        padded_target_mask = tf.ensure_shape(target_mask, [144])\n",
        "\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids\n",
        "\n",
        "    # Apply the transformation\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        lambda imgs, landmarks, coords, subj_ids: add_masks_to_batch(imgs, landmarks, coords, subj_ids),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return masked_dataset\n",
        "\n",
        "\n",
        "def prepare_model_inputs_training(features, labels, subject_ids):\n",
        "    \"\"\"Restructure the inputs for the model, using the mask-based approach for training data.\"\"\"\n",
        "    images, coords, cal_mask, target_mask = features\n",
        "\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": images,\n",
        "        \"Input_All_Coords\": coords,\n",
        "        \"Input_Calibration_Mask\": cal_mask,\n",
        "    }\n",
        "\n",
        "    return inputs, labels, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a54TPxPWSGbz"
      },
      "outputs": [],
      "source": [
        "# Define fixed calibration points for test evaluation\n",
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))\n",
        "\n",
        "print(f\"Using {len(scaled_cal_points)} fixed calibration points for test evaluation\")\n",
        "\n",
        "# Prepare test dataset with phase-based calibration points (matching analysis notebook)\n",
        "test_masked_dataset = prepare_masked_test_dataset(\n",
        "    test_data_rescaled,\n",
        "    calibration_points=scaled_cal_points,\n",
        "    cal_phase=1  # Use phase 1 for calibration\n",
        ")\n",
        "\n",
        "test_ds = test_masked_dataset.map(\n",
        "    prepare_model_inputs_test,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Test dataset prepared with phase-based calibration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK9X9h5ASGbz"
      },
      "source": [
        "## Training Loop for Different Participant Subset Sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQqWASreSGbz"
      },
      "outputs": [],
      "source": [
        "def create_masked_model_training():\n",
        "    \\\"\\\"\\\"Create a model that uses masks to distinguish calibration and target images for training.\\\"\\\"\\\"\n",
        "    input_all_images = keras.layers.Input(\n",
        "        shape=(144, 36, 144, 1),  # Training uses 144 targets\n",
        "        name=\\\"Input_All_Images\\\"\n",
        "    )\n",
        "    input_all_coords = keras.layers.Input(\n",
        "        shape=(144, 2),\n",
        "        name=\\\"Input_All_Coords\\\"\n",
        "    )\n",
        "    input_cal_mask = keras.layers.Input(\n",
        "        shape=(144,),\n",
        "        name=\\\"Input_Calibration_Mask\\\",\n",
        "    )\n",
        "\n",
        "    embedding_model = create_embedding_model(BACKBONE)\n",
        "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\\\"Image_Embeddings\\\")(input_all_images)\n",
        "\n",
        "    calibration_weights = keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\\\"sigmoid\\\",\n",
        "        name=\\\"Calibration_Weights\\\"\n",
        "    )(all_embeddings)\n",
        "\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
        "        RIDGE_REGULARIZATION,\n",
        "        name=\\\"Regression\\\"\n",
        "    )([\n",
        "        all_embeddings,\n",
        "        input_all_coords,\n",
        "        calibration_weights,\n",
        "        input_cal_mask,\n",
        "    ])\n",
        "\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\\\"MaskedEyePredictionModel\\\"\n",
        "    )\n",
        "\n",
        "    return full_model\n",
        "\n",
        "\n",
        "def create_masked_model_test():\n",
        "    \\\"\\\"\\\"Create a model that uses masks to distinguish calibration and target images for testing.\\\"\\\"\\\"\n",
        "    input_all_images = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 36, 144, 1),  # Test uses 288 targets\n",
        "        name=\\\"Input_All_Images\\\"\n",
        "    )\n",
        "    input_all_coords = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 2),\n",
        "        name=\\\"Input_All_Coords\\\"\n",
        "    )\n",
        "    input_cal_mask = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS,),\n",
        "        name=\\\"Input_Calibration_Mask\\\",\n",
        "    )\n",
        "\n",
        "    embedding_model = create_embedding_model(BACKBONE)\n",
        "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\\\"Image_Embeddings\\\")(input_all_images)\n",
        "\n",
        "    calibration_weights = keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\\\"sigmoid\\\",\n",
        "        name=\\\"Calibration_Weights\\\"\n",
        "    )(all_embeddings)\n",
        "\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
        "        RIDGE_REGULARIZATION,\n",
        "        name=\\\"Regression\\\"\n",
        "    )([\n",
        "        all_embeddings,\n",
        "        input_all_coords,\n",
        "        calibration_weights,\n",
        "        input_cal_mask,\n",
        "    ])\n",
        "\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\\\"MaskedEyePredictionModel\\\"\n",
        "    )\n",
        "\n",
        "    return full_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaMkG-7gSGbz"
      },
      "outputs": [],
      "source": [
        "def prepare_masked_test_dataset(dataset, calibration_points=None, cal_phase=1):\n",
        "    \\\"\\\"\\\"\n",
        "    Prepare test dataset with phase-based calibration and target separation.\n",
        "    Uses phase 1 for calibration and phase 2 for targets by default.\n",
        "    \\\"\\\"\\\"\n",
        "    # Step 1: Group dataset by subject_id and batch all images\n",
        "    def group_by_subject(subject_id, ds):\n",
        "        return ds.batch(batch_size=MAX_TARGETS)\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, phase, coords, subject_id: subject_id,\n",
        "        reduce_func=group_by_subject,\n",
        "        window_size=MAX_TARGETS\n",
        "    )\n",
        "\n",
        "    # Step 2: Filter out subjects with insufficient data\n",
        "    def filter_by_image_count(images, phase, coords, subject_ids):\n",
        "        total_image_count = tf.shape(images)[0] >= MAX_TARGETS\n",
        "        phase1_image_count = tf.reduce_sum(tf.cast(tf.equal(phase, 1), tf.int32)) >= 144\n",
        "        phase2_image_count = tf.reduce_sum(tf.cast(tf.equal(phase, 2), tf.int32)) >= 144\n",
        "        return tf.logical_and(total_image_count, tf.logical_and(phase1_image_count, phase2_image_count))\n",
        "\n",
        "    grouped_dataset = grouped_dataset.filter(filter_by_image_count)\n",
        "\n",
        "    # Step 3: Transform each batch to include masks\n",
        "    def add_masks_to_batch(images, phase, coords, subject_ids):\n",
        "        actual_batch_size = tf.shape(images)[0]\n",
        "\n",
        "        # Create phase masks\n",
        "        phase1_mask = tf.cast(tf.equal(phase, 1), tf.int8)\n",
        "        phase2_mask = tf.cast(tf.equal(phase, 2), tf.int8)\n",
        "\n",
        "        cal_mask = tf.zeros(actual_batch_size, dtype=tf.int8)\n",
        "\n",
        "        if calibration_points is None:\n",
        "            raise ValueError(\\\"Need to specify calibration points for test evaluation\\\")\n",
        "        else:\n",
        "            coords_xpand = tf.expand_dims(coords, axis=1)\n",
        "            cal_xpand = tf.expand_dims(calibration_points, axis=0)\n",
        "            equality = tf.equal(coords_xpand, cal_xpand)\n",
        "            matches = tf.reduce_all(equality, axis=-1)\n",
        "            point_matches = tf.reduce_any(matches, axis=1)\n",
        "            cal_mask = tf.cast(point_matches, dtype=tf.int8)\n",
        "\n",
        "        # Target mask: ALL phase 2 points\n",
        "        target_mask = phase2_mask\n",
        "\n",
        "        # Apply phase restriction to calibration mask\n",
        "        if cal_phase == 1:\n",
        "            cal_mask = cal_mask * phase1_mask\n",
        "        elif cal_phase == 2:\n",
        "            cal_mask = cal_mask * phase2_mask\n",
        "\n",
        "        # Pad to fixed size\n",
        "        padded_images = tf.pad(\n",
        "            tf.reshape(images, (-1, 36, 144, 1)),\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(coords, [[0, MAX_TARGETS - actual_batch_size], [0, 0]])\n",
        "        padded_cal_mask = tf.pad(cal_mask, [[0, MAX_TARGETS - actual_batch_size]])\n",
        "        padded_target_mask = tf.pad(target_mask, [[0, MAX_TARGETS - actual_batch_size]])\n",
        "\n",
        "        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(padded_cal_mask, [MAX_TARGETS])\n",
        "        padded_target_mask = tf.ensure_shape(padded_target_mask, [MAX_TARGETS])\n",
        "\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids[0]\n",
        "\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        lambda imgs, phase, coords, subj_ids: add_masks_to_batch(imgs, phase, coords, subj_ids),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return masked_dataset\n",
        "\n",
        "\n",
        "def prepare_model_inputs_test(features, labels, subject_ids):\n",
        "    \\\"\\\"\\\"Restructure the inputs for the model, using the mask-based approach for test data.\\\"\\\"\\\"\n",
        "    images, coords, cal_mask, target_mask = features\n",
        "\n",
        "    inputs = {\n",
        "        \\\"Input_All_Images\\\": images,\n",
        "        \\\"Input_All_Coords\\\": coords,\n",
        "        \\\"Input_Calibration_Mask\\\": cal_mask,\n",
        "    }\n",
        "\n",
        "    return inputs, labels, target_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4KoliDKSGbz"
      },
      "source": [
        "## Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk8VuQFASGbz"
      },
      "outputs": [],
      "source": [
        "# Prepare test dataset with phase-based calibration points (matching analysis notebook)\n",
        "test_masked_dataset = prepare_masked_test_dataset(\n",
        "    test_data_rescaled,\n",
        "    calibration_points=scaled_cal_points,\n",
        "    cal_phase=1  # Use phase 1 for calibration\n",
        ")\n",
        "\n",
        "test_ds = test_masked_dataset.map(\n",
        "    prepare_model_inputs_test,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Test dataset prepared with phase-based calibration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzQ3zd7DSGbz"
      },
      "outputs": [],
      "source": [
        "# Extract data for plotting\n",
        "percentages = sorted(experiment_results.keys())\n",
        "n_participants_list = [experiment_results[p]['n_participants'] for p in percentages]\n",
        "mean_losses = [experiment_results[p]['mean_loss'] for p in percentages]\n",
        "std_losses = [experiment_results[p]['std_loss'] for p in percentages]\n",
        "median_losses = [experiment_results[p]['median_loss'] for p in percentages]\n",
        "\n",
        "print(\"Experiment Summary:\")\n",
        "print(\"Percentage | Participants | Mean Loss | Std Loss | Median Loss\")\n",
        "print(\"-\" * 65)\n",
        "for i, p in enumerate(percentages):\n",
        "    print(f\"{p*100:8.0f}%  |    {n_participants_list[i]:6d}    | {mean_losses[i]:8.3f}  | {std_losses[i]:7.3f}  | {median_losses[i]:10.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brAu8IgrSGbz"
      },
      "outputs": [],
      "source": [
        "# Plot 3: Learning curves comparison\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for p in selected_percentages:\n",
        "    history = experiment_results[p]['training_history']\n",
        "    if 'loss' in history:\n",
        "        plt.plot(history['loss'], label=f'{p*100:.0f}% participants', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Curves')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Plot final training loss vs test loss\n",
        "final_train_losses = []\n",
        "for p in percentages:\n",
        "    history = experiment_results[p]['training_history']\n",
        "    if 'loss' in history and len(history['loss']) > 0:\n",
        "        final_train_losses.append(history['loss'][-1])\n",
        "    else:\n",
        "        final_train_losses.append(np.nan)\n",
        "\n",
        "plt.scatter(final_train_losses, mean_losses, s=100, alpha=0.7)\n",
        "for i, p in enumerate(percentages):\n",
        "    if not np.isnan(final_train_losses[i]):\n",
        "        plt.annotate(f'{p*100:.0f}%',\n",
        "                    (final_train_losses[i], mean_losses[i]),\n",
        "                    xytext=(5, 5), textcoords='offset points')\n",
        "plt.xlabel('Final Training Loss')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.title('Training vs Test Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add diagonal line for reference\n",
        "min_val = min(min([x for x in final_train_losses if not np.isnan(x)]), min(mean_losses))\n",
        "max_val = max(max([x for x in final_train_losses if not np.isnan(x)]), max(mean_losses))\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='y=x')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6b_u2PoSGbz"
      },
      "source": [
        "## Key Findings and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUe0G8j8SGbz"
      },
      "outputs": [],
      "source": [
        "# Calculate key metrics and findings\n",
        "best_performance = min(mean_losses)\n",
        "best_percentage = percentages[mean_losses.index(best_performance)]\n",
        "best_n_participants = n_participants_list[mean_losses.index(best_performance)]\n",
        "\n",
        "# Find the \"knee\" point (point of diminishing returns)\n",
        "# Calculate improvement rate\n",
        "if len(mean_losses) > 1:\n",
        "    improvements = [(mean_losses[0] - loss) / mean_losses[0] * 100 for loss in mean_losses]\n",
        "\n",
        "    # Find where improvement rate starts to plateau (less than 2% improvement)\n",
        "    knee_point = None\n",
        "    for i in range(1, len(improvements)):\n",
        "        if i > 0 and (improvements[i] - improvements[i-1]) < 2.0:  # Less than 2% additional improvement\n",
        "            knee_point = i\n",
        "            break\n",
        "\n",
        "    if knee_point is None:\n",
        "        knee_point = len(percentages) - 1\n",
        "\n",
        "    knee_percentage = percentages[knee_point]\n",
        "    knee_participants = n_participants_list[knee_point]\n",
        "    knee_performance = mean_losses[knee_point]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY FINDINGS AND RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n📊 DATASET STATISTICS:\")\n",
        "print(f\"   • Total participants in full dataset: {total_participants}\")\n",
        "print(f\"   • Total training samples: {total_samples}\")\n",
        "print(f\"   • Average samples per participant: {total_samples/total_participants:.1f}\")\n",
        "\n",
        "print(f\"\\n🎯 PERFORMANCE RESULTS:\")\n",
        "print(f\"   • Best performance: {best_performance:.3f} (with {best_percentage*100:.0f}% participants = {best_n_participants} people)\")\n",
        "print(f\"   • Worst performance: {max(mean_losses):.3f} (with {percentages[mean_losses.index(max(mean_losses))]*100:.0f}% participants)\")\n",
        "print(f\"   • Total improvement: {((max(mean_losses) - best_performance) / max(mean_losses) * 100):.1f}%\")\n",
        "\n",
        "if len(mean_losses) > 1:\n",
        "    print(f\"\\n📈 DIMINISHING RETURNS ANALYSIS:\")\n",
        "    print(f\"   • Knee point (diminishing returns): {knee_percentage*100:.0f}% participants = {knee_participants} people\")\n",
        "    print(f\"   • Performance at knee point: {knee_performance:.3f}\")\n",
        "    print(f\"   • Performance gap from knee to best: {((knee_performance - best_performance) / best_performance * 100):.1f}%\")\n",
        "\n",
        "print(f\"\\n💡 RECOMMENDATIONS:\")\n",
        "if len(mean_losses) > 1:\n",
        "    if (best_performance - knee_performance) / best_performance < 0.05:  # Less than 5% improvement\n",
        "        print(f\"   • MINIMUM VIABLE: Use ~{knee_percentage*100:.0f}% of participants ({knee_participants} people)\")\n",
        "        print(f\"     - Achieves {((mean_losses[0] - knee_performance) / mean_losses[0] * 100):.1f}% of total possible improvement\")\n",
        "        print(f\"     - Cost-effective balance of performance vs data collection effort\")\n",
        "\n",
        "    print(f\"   • OPTIMAL: Use {best_percentage*100:.0f}% of participants ({best_n_participants} people) for best performance\")\n",
        "\n",
        "    # Check if there's still significant improvement at the end\n",
        "    if len(mean_losses) >= 2:\n",
        "        final_improvement = (mean_losses[-2] - mean_losses[-1]) / mean_losses[-2] * 100\n",
        "        if final_improvement > 2:  # More than 2% improvement in last step\n",
        "            print(f\"   • POTENTIAL: Consider collecting more data - still seeing {final_improvement:.1f}% improvement\")\n",
        "\n",
        "print(f\"\\n📋 DETAILED BREAKDOWN:\")\n",
        "for i, p in enumerate(percentages):\n",
        "    improvement = (mean_losses[0] - mean_losses[i]) / mean_losses[0] * 100\n",
        "    print(f\"   • {p*100:3.0f}% participants ({n_participants_list[i]:4d} people): {mean_losses[i]:.3f} loss ({improvement:5.1f}% improvement)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}