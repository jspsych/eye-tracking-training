{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LAJHmSmF84"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW4fMCshWBrD"
      },
      "outputs": [],
      "source": [
        "!pip install osfclient --quiet\n",
        "!pip install git+https://github.com/jspsych/eyetracking-utils.git --quiet\n",
        "!pip install keras_cv --quiet\n",
        "!pip install plotnine --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install albumentations --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQRQZ3D5Vwqc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import tensorflow.keras as keras\n",
        "import keras_cv\n",
        "from plotnine import ggplot, geom_point, aes, geom_line, scale_y_reverse, theme_void, scale_color_manual\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "import albumentations as A\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "import et_util.dataset_utils as dataset_utils\n",
        "import et_util.embedding_preprocessing as embed_pre\n",
        "import et_util.model_layers as model_layers\n",
        "from et_util import experiment_utils\n",
        "from et_util.custom_loss import normalized_weighted_euc_dist\n",
        "from et_util.model_analysis import plot_model_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BUzHdPEc-ef"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
        "os.environ['OSF_USERNAME'] = userdata.get('osfusername')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3M9X95jV8du"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9v9bxBcHc0"
      },
      "source": [
        "# Configure W&B experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SPGBdleCfbe"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWWKsFf6p1HJ"
      },
      "outputs": [],
      "source": [
        "# Fixed constants\n",
        "MAX_TARGETS = 144\n",
        "\n",
        "# Config constants\n",
        "EMBEDDING_DIM = 200\n",
        "RIDGE_REGULARIZATION = 0.1\n",
        "TRAIN_EPOCHS = 30\n",
        "MIN_CAL_POINTS = 8\n",
        "MAX_CAL_POINTS = 40\n",
        "DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "LEARNING_RATE = 0.001\n",
        "AUGMENTATION = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkHFv5kHfaWR"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"ridge_regularization\": RIDGE_REGULARIZATION,\n",
        "    \"train_epochs\": TRAIN_EPOCHS,\n",
        "    \"min_cal_points\": MIN_CAL_POINTS,\n",
        "    \"max_cal_points\": MAX_CAL_POINTS,\n",
        "    \"dense_net_stackwise_num_repeats\": DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"augmentation\": AUGMENTATION,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjDFWXNEddE9"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project='eye-tracking-dense-full-data-set-single-eye',\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeHGiFxtvo0"
      },
      "source": [
        "# Download dataset from OSF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tswV6s38WbtO"
      },
      "outputs": [],
      "source": [
        "!osf -p 6b5cd fetch single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgWOhWTt4iD"
      },
      "source": [
        "# Process raw data records into TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4h8QhGvWhZf"
      },
      "outputs": [],
      "source": [
        "!mkdir single_eye_tfrecords\n",
        "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZRcQgPadqtI"
      },
      "outputs": [],
      "source": [
        "def parse(element):\n",
        "    \"\"\"Process function that parses a tfr element in a raw dataset for process_tfr_to_tfds function.\n",
        "    Gets mediapipe landmarks, raw image, image width, image height, subject id, and xy labels.\n",
        "    Use for data generated with make_single_example_landmarks_and_jpg (i.e. data in\n",
        "    jpg_landmarks_tfrecords.tar.gz)\n",
        "\n",
        "    :param element: tfr element in raw dataset\n",
        "    :return: image, label(x,y), landmarks, subject_id\n",
        "    \"\"\"\n",
        "\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    landmarks = content['landmarks']\n",
        "    raw_image = content['eye_img']\n",
        "    width = content['img_width']\n",
        "    height = content['img_height']\n",
        "    depth = 3\n",
        "    label = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n",
        "    landmarks = tf.reshape(landmarks, shape=(478, 3))\n",
        "\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, landmarks, label, subject_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWv57eb5XHVP"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data, test_data = dataset_utils.process_tfr_to_tfds(\n",
        "    'single_eye_tfrecords/',\n",
        "    parse,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, landmarks, coords, z: z\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TeYbFSrdMcMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FULm58Cxu9oa"
      },
      "source": [
        "## Rescale the `x,y` coordinates to be 0-1 instead of 0-100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apcLK9klHLQV"
      },
      "outputs": [],
      "source": [
        "def rescale_coords_map(eyes, mesh, coords, id):\n",
        "  return eyes, mesh, tf.divide(coords, tf.constant([100.])), id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b47CtWKqHg0B"
      },
      "outputs": [],
      "source": [
        "train_data_rescaled = train_data.map(rescale_coords_map)\n",
        "validation_data_rescaled = validation_data.map(rescale_coords_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the weighted regression layer"
      ],
      "metadata": {
        "id": "0UfY_C8zMW8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    def __init__(self, lambda_ridge, embedding_dim, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        self.embedding_dim = embedding_dim\n",
        "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "        self._identity = tf.eye(embedding_dim, dtype=tf.float32)\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, inputs):\n",
        "        # Unpack inputs\n",
        "        all_embeddings, all_coords, calibration_weights, cal_mask, target_mask = inputs\n",
        "\n",
        "        # Cast all inputs to the same precision\n",
        "        embeddings = tf.cast(all_embeddings, tf.float32)  # (batch_size, max_images, embedding_dim)\n",
        "        coords = tf.cast(all_coords, tf.float32)          # (batch_size, max_images, 2)\n",
        "        w = tf.cast(calibration_weights, tf.float32)      # (batch_size, max_images)\n",
        "        w = tf.squeeze(w, axis=-1)\n",
        "        cal_mask = tf.cast(cal_mask, tf.float32)          # (batch_size, max_images)\n",
        "        target_mask = tf.cast(target_mask, tf.float32)    # (batch_size, max_images)\n",
        "\n",
        "        # Get batch size and shape constants\n",
        "        batch_size = tf.shape(embeddings)[0]\n",
        "        max_images = tf.shape(embeddings)[1]\n",
        "\n",
        "        # Create identity matrix broadcast to batch size\n",
        "        I = tf.broadcast_to(self._identity, [batch_size, self.embedding_dim, self.embedding_dim])\n",
        "\n",
        "        # Apply calibration mask to weights\n",
        "        w = w * cal_mask\n",
        "\n",
        "        # Reshape weights for broadcasting\n",
        "        w_sqrt = tf.sqrt(w)\n",
        "        w_sqrt = tf.reshape(w_sqrt, [batch_size, max_images, 1])  # For broadcasting\n",
        "\n",
        "        # Apply calibration mask to get calibration embeddings\n",
        "        cal_mask_expand = tf.reshape(cal_mask, [batch_size, max_images, 1])\n",
        "        X = embeddings * cal_mask_expand  # Zero out non-calibration embeddings\n",
        "\n",
        "        # Weight calibration embeddings and coordinates\n",
        "        X_weighted = X * w_sqrt\n",
        "        y_weighted = coords * w_sqrt\n",
        "\n",
        "        # Matrix multiplication\n",
        "        X_t = tf.transpose(X_weighted, perm=[0, 2, 1])  # (batch_size, embedding_dim, max_images)\n",
        "        X_t_X = tf.matmul(X_t, X_weighted)              # (batch_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Add regularization\n",
        "        ridge_term = tf.multiply(I, self.lambda_ridge + self.epsilon)\n",
        "        lhs = tf.add(X_t_X, ridge_term)                 # (batch_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Compute right-hand side\n",
        "        rhs = tf.matmul(X_t, y_weighted)                # (batch_size, embedding_dim, 2)\n",
        "\n",
        "        # Solve linear system with stability check\n",
        "        is_singular = tf.math.reduce_any(tf.math.is_nan(lhs))\n",
        "        kernel = tf.cond(\n",
        "            is_singular,\n",
        "            lambda: tf.zeros([batch_size, self.embedding_dim, 2], dtype=tf.float32),\n",
        "            lambda: tf.linalg.solve(lhs, rhs)\n",
        "        )  # Shape: [batch_size, embedding_dim, 2]\n",
        "\n",
        "        # Apply target mask to get target embeddings\n",
        "        target_mask_expand = tf.reshape(target_mask, [batch_size, max_images, 1])\n",
        "        target_embeddings = embeddings * target_mask_expand  # Zero out non-target embeddings\n",
        "\n",
        "        # Apply regression to target embeddings\n",
        "        output = tf.matmul(target_embeddings, kernel)   # (batch_size, max_images, 2)\n",
        "\n",
        "        # Mask the output (zeros for non-target points)\n",
        "        output = output * target_mask_expand\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        # Output shape is (batch_size, max_images, 2)\n",
        "        return (input_shapes[0][0], input_shapes[0][1], 2)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"lambda_ridge\": self.lambda_ridge,\n",
        "            \"epsilon\": self.epsilon,\n",
        "            \"embedding_dim\": self.embedding_dim\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "wbWzR7yfMZv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdQbBaKR6vLN"
      },
      "source": [
        "# Generate dataset that has calibration points, target point, and target output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCSmeKFBH5YQ"
      },
      "source": [
        "Cols = 5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95\n",
        "\n",
        "Rows = 5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP0_I2DUuEKo"
      },
      "source": [
        "Fixed Points as calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuCld98e7vAZ"
      },
      "outputs": [],
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnKs983UsyDj"
      },
      "outputs": [],
      "source": [
        "def create_embedding_model():\n",
        "    input_eyes = keras.layers.Input(shape=(36,144,1))\n",
        "\n",
        "    # Continue with the backbone\n",
        "    backbone = keras_cv.models.DenseNetBackbone(\n",
        "        include_rescaling=False,\n",
        "        input_shape=(36,144,1),\n",
        "        stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS\n",
        "    )\n",
        "    backbone_encoder = backbone(input_eyes)\n",
        "\n",
        "    flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "    eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "    embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "\n",
        "    return embedding_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_normalized_weighted_euc_dist(y_true, y_pred):\n",
        "    \"\"\"Custom loss function that calculates a weighted Euclidean distance between two sets of points,\n",
        "    respecting masks for padded sequences.\n",
        "\n",
        "    Weighting multiplies the x-coordinate of each input by 1.778 (derived from the 16:9 aspect ratio of most laptops)\n",
        "    to match the scale of the y-axis. For interpretability, the distances are normalized to the diagonal\n",
        "    such that the maximum distance between two points (corner to corner) is 100.\n",
        "\n",
        "    :param y_true (tensor): A tensor of shape (batch_size, seq_len, 2) containing ground-truth x- and y- coordinates\n",
        "    :param y_pred (tensor): A tensor of shape (batch_size, seq_len, 2) containing predicted x- and y- coordinates\n",
        "\n",
        "    :returns: A tensor of shape (1,) with the weighted and normalized euclidean distances between valid points.\n",
        "    \"\"\"\n",
        "    # Extract shape information\n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    seq_len = tf.shape(y_true)[1]\n",
        "\n",
        "    # Create a mask based on zero-padding in y_pred\n",
        "    # Assuming padded values are exactly 0 in both dimensions\n",
        "    mask = tf.reduce_any(tf.not_equal(y_pred, 0), axis=-1)\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Weighting treats y-axis as unit-scale and creates a rectangle that's 177.8x100 units.\n",
        "    x_weight = tf.constant([1.778, 1.0], dtype=tf.float32)\n",
        "\n",
        "    # Multiply x-coordinate by 16/9 = 1.778\n",
        "    y_true_weighted = tf.math.multiply(x_weight, y_true)\n",
        "    y_pred_weighted = tf.math.multiply(x_weight, y_pred)\n",
        "\n",
        "    # Calculate Euclidean distance with weighted coordinates\n",
        "    squared_diff = tf.math.square(y_pred_weighted - y_true_weighted)\n",
        "    squared_dist = tf.math.reduce_sum(squared_diff, axis=-1)\n",
        "    dist = tf.math.sqrt(squared_dist)\n",
        "\n",
        "    # Euclidean Distance from [0,0] to [177.8, 100] = 203.992\n",
        "    norm_scale = tf.constant(203.992, dtype=tf.float32)\n",
        "\n",
        "    # Normalizes loss values to the diagonal-- makes loss easier to interpret\n",
        "    normalized_dist = (dist / norm_scale) * 100\n",
        "\n",
        "    # Apply mask to only include valid points in the loss calculation\n",
        "    masked_dist = normalized_dist * mask\n",
        "\n",
        "    # Sum the losses and divide by the number of non-padded elements\n",
        "    num_valid = tf.maximum(tf.reduce_sum(mask), 1.0)  # Avoid division by zero\n",
        "    loss = tf.reduce_sum(masked_dist) / num_valid\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "y5Kghim-rVhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "MAX_IMAGES = MAX_TARGETS  # Total number of images in each batch\n",
        "MIN_CAL_IMAGES = MIN_CAL_POINTS  # Minimum number of calibration images\n",
        "MAX_CAL_IMAGES = MAX_CAL_POINTS  # Maximum number of calibration images\n",
        "\n",
        "def prepare_masked_dataset(dataset, batch_size=200):\n",
        "    \"\"\"\n",
        "    Creates a dataset where each batch contains all images for a subject,\n",
        "    and a random subset is marked as calibration images via a mask.\n",
        "\n",
        "    Args:\n",
        "        dataset: The base TF dataset containing eye images and coordinates\n",
        "        batch_size: Number of images to include in each batch\n",
        "\n",
        "    Returns:\n",
        "        A TF dataset with masked images ready for model training\n",
        "    \"\"\"\n",
        "    # Step 1: Group dataset by subject_id and batch all images\n",
        "    def group_by_subject(subject_id, ds):\n",
        "        return ds.batch(batch_size=MAX_TARGETS)\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, mesh, coords, subject_id: subject_id,\n",
        "        reduce_func=group_by_subject,\n",
        "        window_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Step 2: Transform each batch to include masks\n",
        "    def add_masks_to_batch(images, meshes, coords, subject_ids):\n",
        "        # Get actual batch size (number of images for this subject)\n",
        "        actual_batch_size = tf.shape(images)[0]\n",
        "\n",
        "        # Determine how many calibration images to use (random between min and max)\n",
        "        n_cal_images = tf.random.uniform(\n",
        "            shape=[],\n",
        "            minval=MIN_CAL_IMAGES,\n",
        "            maxval=MAX_CAL_IMAGES,\n",
        "            dtype=tf.int32\n",
        "        )\n",
        "\n",
        "        # Create random indices for calibration images\n",
        "        # NOTE: These will be different each time through the dataset\n",
        "        random_indices = tf.random.shuffle(tf.range(actual_batch_size))\n",
        "        cal_indices = random_indices[:n_cal_images]\n",
        "        target_indices = random_indices[n_cal_images:]\n",
        "\n",
        "        # Create masks (1 = included, 0 = excluded)\n",
        "        cal_mask = tf.zeros(actual_batch_size, dtype=tf.float32)\n",
        "        cal_mask = tf.tensor_scatter_nd_update(\n",
        "            cal_mask,\n",
        "            tf.expand_dims(cal_indices, 1),\n",
        "            tf.ones(n_cal_images, dtype=tf.float32)\n",
        "        )\n",
        "\n",
        "        target_mask = tf.zeros(actual_batch_size, dtype=tf.float32)\n",
        "        target_mask = tf.tensor_scatter_nd_update(\n",
        "            target_mask,\n",
        "            tf.expand_dims(target_indices, 1),\n",
        "            tf.ones(actual_batch_size - n_cal_images, dtype=tf.float32)\n",
        "        )\n",
        "\n",
        "        # Reshape images to expected format (batch, height, width, channels)\n",
        "        reshaped_images = tf.reshape(images, (-1, 36, 144, 1))\n",
        "\n",
        "        # Pad everything to fixed size\n",
        "        padded_images = tf.pad(\n",
        "            reshaped_images,\n",
        "            [[0, MAX_IMAGES - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(\n",
        "            coords,\n",
        "            [[0, MAX_IMAGES - actual_batch_size], [0, 0]]\n",
        "        )\n",
        "        padded_cal_mask = tf.pad(\n",
        "            cal_mask,\n",
        "            [[0, MAX_IMAGES - actual_batch_size]]\n",
        "        )\n",
        "        padded_target_mask = tf.pad(\n",
        "            target_mask,\n",
        "            [[0, MAX_IMAGES - actual_batch_size]]\n",
        "        )\n",
        "\n",
        "        # Ensure all shapes are fixed\n",
        "        padded_images = tf.ensure_shape(padded_images, [MAX_IMAGES, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [MAX_IMAGES, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(padded_cal_mask, [MAX_IMAGES])\n",
        "        padded_target_mask = tf.ensure_shape(padded_target_mask, [MAX_IMAGES])\n",
        "\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords\n",
        "\n",
        "    # Apply the transformation\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        lambda imgs, meshes, coords, subj_ids: add_masks_to_batch(imgs, meshes, coords, subj_ids),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Cache the results for faster training\n",
        "    masked_dataset = masked_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return masked_dataset\n",
        "\n",
        "# Modified model input preparation function\n",
        "def prepare_model_inputs(features, labels):\n",
        "    \"\"\"\n",
        "    Restructure the inputs for the model, using the mask-based approach.\n",
        "\n",
        "    Args:\n",
        "        features: Tuple of (images, coords, cal_mask, target_mask)\n",
        "        labels: The target coordinates\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of inputs for the model and the target labels\n",
        "    \"\"\"\n",
        "    images, coords, cal_mask, target_mask = features\n",
        "\n",
        "    # Use masking to create the model inputs\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": images,                   # All images (both cal and target)\n",
        "        \"Input_All_Coords\": coords,                   # All coordinates\n",
        "        \"Input_Calibration_Mask\": cal_mask,           # Mask indicating calibration images\n",
        "        \"Input_Target_Mask\": target_mask              # Mask indicating target images\n",
        "    }\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "# Modified model to work with the new masked input approach\n",
        "def create_masked_model():\n",
        "    \"\"\"Create a model that uses masks to distinguish calibration and target images\"\"\"\n",
        "\n",
        "    # Create the embedding model (same as before)\n",
        "    embedding_model = create_embedding_model()\n",
        "\n",
        "    # Define new inputs\n",
        "    input_all_images = keras.layers.Input(shape=(MAX_IMAGES, 36, 144, 1), name=\"Input_All_Images\")\n",
        "    input_all_coords = keras.layers.Input(shape=(MAX_IMAGES, 2), name=\"Input_All_Coords\")\n",
        "    input_cal_mask = keras.layers.Input(shape=(MAX_IMAGES,), name=\"Input_Calibration_Mask\")\n",
        "    input_target_mask = keras.layers.Input(shape=(MAX_IMAGES,), name=\"Input_Target_Mask\")\n",
        "\n",
        "    # Apply the embedding model to all images\n",
        "    all_embeddings = keras.layers.TimeDistributed(embedding_model)(input_all_images)\n",
        "\n",
        "    # Calculate importance weights for all points\n",
        "    calibration_weights = keras.layers.Dense(1, activation=\"sigmoid\", name=\"Calibration_Weights\")(all_embeddings)\n",
        "\n",
        "    # Apply the WeightedRidgeRegressionLayer as before\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(RIDGE_REGULARIZATION, embedding_dim=EMBEDDING_DIM)([\n",
        "        all_embeddings,\n",
        "        input_all_coords,\n",
        "        calibration_weights,\n",
        "        input_cal_mask,\n",
        "        input_target_mask\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Create the full model\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "            input_target_mask\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\"MaskedEyePredictionModel\"\n",
        "    )\n",
        "\n",
        "    return full_model\n",
        "\n",
        "# Usage example\n",
        "def prepare_and_train_with_masks():\n",
        "    \"\"\"Example of how to use the mask-based approach for training\"\"\"\n",
        "\n",
        "    # 1. Prepare the rescaled data\n",
        "    train_data_rescaled = train_data.map(rescale_coords_map).cache()\n",
        "\n",
        "    # 2. Create the masked dataset\n",
        "    masked_dataset = prepare_masked_dataset(train_data_rescaled)\n",
        "\n",
        "    # 3. Prepare the dataset for the model\n",
        "    train_ds_for_model = masked_dataset.map(\n",
        "        prepare_model_inputs,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # 4. Create and compile the model\n",
        "    mask_model = create_masked_model()\n",
        "    mask_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss=masked_normalized_weighted_euc_dist,\n",
        "        metrics=[normalized_weighted_euc_dist]\n",
        "    )\n",
        "\n",
        "    # 5. Train the model\n",
        "    history = mask_model.fit(\n",
        "        train_ds_for_model.batch(4),\n",
        "        epochs=TRAIN_EPOCHS,\n",
        "        callbacks=[\n",
        "            WandbMetricsLogger(),\n",
        "            keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return mask_model, history"
      ],
      "metadata": {
        "id": "OeXnPUbC08k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_dataset = prepare_masked_dataset(train_data_rescaled)"
      ],
      "metadata": {
        "id": "eTBbOOV52Uqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_for_model = masked_dataset.map(\n",
        "        prepare_model_inputs,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).take(10).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "21AN7OpI4ee2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_for_model.element_spec"
      ],
      "metadata": {
        "id": "kY8tCLq83Dgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model = create_masked_model()"
      ],
      "metadata": {
        "id": "fPT1PjTv4tNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model.summary()"
      ],
      "metadata": {
        "id": "RNxIV_1xIrHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=masked_normalized_weighted_euc_dist,\n",
        "    metrics=[normalized_weighted_euc_dist],\n",
        "    jit_compile=False\n",
        ")"
      ],
      "metadata": {
        "id": "S1SuTboRMtah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model.fit(train_ds_for_model.batch(4), epochs=10)"
      ],
      "metadata": {
        "id": "9QWxiUiBMmue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFQ-O5idYhS3"
      },
      "source": [
        "## Create augmentation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Xkt4gQYhS3"
      },
      "outputs": [],
      "source": [
        "augmentation_layers = [\n",
        "    #keras.layers.RandomBrightness(factor=0.1),\n",
        "    #keras.layers.RandomContrast(factor=0.1),\n",
        "    keras.layers.RandomRotation(factor=.05, fill_mode='constant', fill_value=0),\n",
        "    #keras.layers.RandomTranslation(height_factor=0.05, width_factor=0.05)\n",
        "]\n",
        "\n",
        "augmentation_model = keras.Sequential(augmentation_layers)\n",
        "\n",
        "@tf.function\n",
        "def apply_consistent_augmentations(inputs, augmentation_model):\n",
        "    \"\"\"Apply consistent augmentations to calibration and target images.\n",
        "\n",
        "    Args:\n",
        "        inputs: Tuple of (cal_imgs, cal_coords, cal_mask, target_imgs, target_mask)\n",
        "        augmentation_fn: Function created by create_augmentation_fn\n",
        "\n",
        "    Returns:\n",
        "        Tuple with augmented images in the same structure\n",
        "    \"\"\"\n",
        "    cal_imgs, cal_coords, cal_mask, target_imgs, target_mask = inputs\n",
        "\n",
        "    merged_cal_images = tf.transpose(cal_imgs, perm=[3, 1, 2, 0])\n",
        "    merged_target_images = tf.transpose(target_imgs, perm=[3, 1, 2, 0])\n",
        "\n",
        "    n_cal_images = MAX_CAL_POINTS\n",
        "    n_target_images = MAX_TARGETS\n",
        "\n",
        "    merged_all_images = tf.concat([merged_cal_images, merged_target_images], axis=-1)\n",
        "\n",
        "    merged_all_images_aug = augmentation_model(merged_all_images)\n",
        "\n",
        "    merged_cal_imgs_aug = merged_all_images_aug[..., :n_cal_images]\n",
        "    merged_target_imgs_aug = merged_all_images_aug[..., n_cal_images:]\n",
        "\n",
        "    cal_imgs_aug = tf.transpose(merged_cal_imgs_aug, perm=[3, 1, 2, 0])\n",
        "    target_imgs_aug = tf.transpose(merged_target_imgs_aug, perm=[3, 1, 2, 0])\n",
        "\n",
        "    return cal_imgs_aug, cal_coords, cal_mask, target_imgs_aug, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz_L2j0duJDw"
      },
      "outputs": [],
      "source": [
        "t0 = train_data_rescaled.cache()\n",
        "\n",
        "t1 = t0.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function,\n",
        "    window_size = 200\n",
        ")\n",
        "\n",
        "t2 = t0.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ")\n",
        "\n",
        "# Apply to dataset - ensure consistent shapes\n",
        "train_ds = t1.map(\n",
        "    lambda x, y: (apply_consistent_augmentations(x, augmentation_model), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Update the batching step to use a custom padded_batch function\n",
        "def prepare_batch_for_model(features, labels):\n",
        "    \"\"\"Ensure inputs are correctly formatted for the model\"\"\"\n",
        "    cal_imgs, cal_coords, cal_mask, target_imgs, target_mask = features\n",
        "\n",
        "    # Prepare inputs as a dictionary matching the model's expected inputs\n",
        "    inputs = {\n",
        "        \"Input_Calibration_Eyes\": cal_imgs,\n",
        "        \"Input_Calibration_Points\": cal_coords,\n",
        "        \"Input_Calibration_Mask\": cal_mask,\n",
        "        \"Input_Target_Eyes\": target_imgs,\n",
        "        \"Input_Target_Mask\": target_mask\n",
        "    }\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "# Use the updated version with proper input preparation\n",
        "train_ds_for_model = train_ds.map(prepare_batch_for_model).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.element_spec"
      ],
      "metadata": {
        "id": "RKeCKJ8jliC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORk_yuzTYhS3"
      },
      "source": [
        "## Visualize augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeTXbm3mYhS3"
      },
      "outputs": [],
      "source": [
        "def visualize_augmentations(dataset, num_examples=5):\n",
        "    \"\"\"Visualize augmentations from a tf.data.Dataset.\"\"\"\n",
        "\n",
        "    # Get a sample element from the dataset\n",
        "    for element in dataset.take(1):\n",
        "        # Access the calibration images from the element\n",
        "        images = element[0][0]  # Access all calibration images\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    num_images = images.shape[0]  # Get the number of images in the batch\n",
        "    grid_cols = 5  # Number of columns in the grid\n",
        "    grid_rows = (num_images + grid_cols - 1) // grid_cols  # Calculate number of rows\n",
        "\n",
        "    # Create a figure to display results\n",
        "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(15, 3 * grid_rows))  # Create subplots in a grid\n",
        "\n",
        "    # Display all images\n",
        "    for i in range(num_images):\n",
        "        row = i // grid_cols\n",
        "        col = i % grid_cols\n",
        "        axes[row, col].imshow(images[i].numpy().squeeze(), cmap='gray')  # Display image\n",
        "        axes[row, col].set_title(f\"Image {i + 1}\")  # Set title for each subplot\n",
        "        axes[row, col].axis('off')  # Turn off axis\n",
        "\n",
        "    # Hide empty subplots if any\n",
        "    for i in range(num_images, grid_rows * grid_cols):\n",
        "        row = i // grid_cols\n",
        "        col = i % grid_cols\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Use this to check your augmentations\n",
        "visualize_augmentations(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19yxxx_Ts8H"
      },
      "source": [
        "# Construct Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuZobSgkgbW"
      },
      "source": [
        "## Eye image processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62VuVwq6lvd"
      },
      "source": [
        "## Regression Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGk0aRz_ebqk"
      },
      "outputs": [],
      "source": [
        "class WeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    def __init__(self, lambda_ridge, embedding_dim, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        self.embedding_dim = embedding_dim  # Required static embedding dimension\n",
        "        super(WeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "        # Create identity matrix constant during initialization\n",
        "        # This ensures it's created only once and is available immediately\n",
        "        self._identity = tf.eye(embedding_dim, dtype=tf.float32)\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, inputs):\n",
        "        unknown_embeddings, calibration_embeddings, calibration_coords, weights, cal_mask, target_mask = inputs\n",
        "\n",
        "        # Explicitly set shapes where possible (assuming batch_size can remain dynamic)\n",
        "        batch_size = tf.shape(calibration_embeddings)[0]\n",
        "\n",
        "        # Force shapes to match the static embedding dimension\n",
        "        calibration_embeddings = tf.ensure_shape(\n",
        "            calibration_embeddings, [None, None, self.embedding_dim]\n",
        "        )\n",
        "        unknown_embeddings = tf.ensure_shape(\n",
        "            unknown_embeddings, [None, None, self.embedding_dim]\n",
        "        )\n",
        "\n",
        "        # Cast all inputs to the same precision\n",
        "        X = tf.cast(calibration_embeddings, tf.float32)  # (batch_size, n_calibration, embedding_dim)\n",
        "        y = tf.cast(calibration_coords, tf.float32)      # (batch_size, n_calibration, 2)\n",
        "        w = tf.cast(weights, tf.float32)                 # (batch_size, n_calibration)\n",
        "        cal_mask = tf.cast(cal_mask, tf.float32)         # (batch_size, n_calibration)\n",
        "        target_mask = tf.cast(target_mask, tf.float32)   # (batch_size, n_target)\n",
        "\n",
        "        # Ensure shapes are consistent\n",
        "        n_calibration = tf.shape(X)[1]\n",
        "        n_target = tf.shape(unknown_embeddings)[1]\n",
        "\n",
        "        # Broadcast the identity matrix to batch size\n",
        "        I = tf.broadcast_to(self._identity, [batch_size, self.embedding_dim, self.embedding_dim])\n",
        "\n",
        "        # Apply mask to weights - ensure this is broadcastable\n",
        "        w = w * cal_mask\n",
        "\n",
        "        # Apply weights to X and y with explicit reshaping\n",
        "        w_sqrt = tf.sqrt(w)\n",
        "        w_sqrt = tf.reshape(w_sqrt, [batch_size, n_calibration, 1])  # Reshape for broadcasting\n",
        "\n",
        "        X_weighted = X * w_sqrt  # (batch_size, n_calibration, embedding_dim)\n",
        "        y_weighted = y * w_sqrt  # (batch_size, n_calibration, 2)\n",
        "\n",
        "        # Matrix multiplication with explicit transpose\n",
        "        X_t = tf.transpose(X_weighted, perm=[0, 2, 1])  # (batch_size, embedding_dim, n_calibration)\n",
        "        X_t_X = tf.matmul(X_t, X_weighted)  # (batch_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Add regularization with stable epsilon\n",
        "        ridge_term = tf.multiply(I, self.lambda_ridge + self.epsilon)\n",
        "        lhs = tf.add(X_t_X, ridge_term)  # (batch_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Compute right-hand side\n",
        "        rhs = tf.matmul(X_t, y_weighted)  # (batch_size, embedding_dim, 2)\n",
        "\n",
        "        # Solve linear system\n",
        "        # Add stability check for better XLA compatibility\n",
        "        is_singular = tf.math.reduce_any(tf.math.is_nan(lhs))\n",
        "        kernel = tf.cond(\n",
        "            is_singular,\n",
        "            lambda: tf.zeros([batch_size, self.embedding_dim, 2], dtype=tf.float32),\n",
        "            lambda: tf.linalg.solve(lhs, rhs)\n",
        "        )  # Shape: [batch_size, embedding_dim, 2]\n",
        "\n",
        "        # Apply regression to unknown point with explicit masking\n",
        "        unknown_embeddings = tf.cast(unknown_embeddings, tf.float32)  # Shape: [batch_size, n_target, embedding_dim]\n",
        "        target_mask_expanded = tf.reshape(target_mask, [batch_size, n_target, 1])\n",
        "        unknown_embeddings_masked = unknown_embeddings * target_mask_expanded\n",
        "\n",
        "        # Matrix multiplication with proper dimensions:\n",
        "        # unknown_embeddings_masked: [batch_size, n_target, embedding_dim]\n",
        "        # kernel:                    [batch_size, embedding_dim, 2]\n",
        "        # Result:                    [batch_size, n_target, 2]\n",
        "        output = tf.matmul(unknown_embeddings_masked, kernel)  # Shape: [batch_size, n_target, 2]\n",
        "\n",
        "        # Apply final mask\n",
        "        output = output * target_mask_expanded\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        unknown_embeddings_shape, _, _, _, _, _ = input_shapes\n",
        "        return (unknown_embeddings_shape[0], unknown_embeddings_shape[1], 2)\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        # This method is called once before the first call() to build the layer\n",
        "        # We already handle initialization in __init__, but this is a good place to validate shapes\n",
        "        unknown_shape, cal_shape, coords_shape, weights_shape, cal_mask_shape, target_mask_shape = input_shapes\n",
        "\n",
        "        if unknown_shape[-1] != self.embedding_dim:\n",
        "            raise ValueError(f\"Unknown embeddings dimension {unknown_shape[-1]} doesn't match specified embedding_dim {self.embedding_dim}\")\n",
        "\n",
        "        if cal_shape[-1] != self.embedding_dim:\n",
        "            raise ValueError(f\"Calibration embeddings dimension {cal_shape[-1]} doesn't match specified embedding_dim {self.embedding_dim}\")\n",
        "\n",
        "        # We don't need to create any weights in build() since we don't have trainable parameters\n",
        "        super(WeightedRidgeRegressionLayer, self).build(input_shapes)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(WeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"lambda_ridge\": self.lambda_ridge,\n",
        "            \"epsilon\": self.epsilon,\n",
        "            \"embedding_dim\": self.embedding_dim\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JOA5GNqTUW-"
      },
      "outputs": [],
      "source": [
        "def create_full_model():\n",
        "\n",
        "  embedding_model = create_embedding_model()\n",
        "\n",
        "  input_calibration_eyes = keras.layers.Input(shape=(MAX_CAL_POINTS, 36, 144, 1), name=\"Input_Calibration_Eyes\")\n",
        "  input_calibration_points = keras.layers.Input(shape=(MAX_CAL_POINTS, 2), name=\"Input_Calibration_Points\")\n",
        "  input_calibration_mask = keras.layers.Input(shape=(MAX_CAL_POINTS,), name=\"Input_Calibration_Mask\")\n",
        "\n",
        "  input_target_eyes = keras.layers.Input(shape=(MAX_TARGETS, 36, 144, 1), name=\"Input_Target_Eyes\")\n",
        "  input_target_mask = keras.layers.Input(shape=(MAX_TARGETS,), name=\"Input_Target_Mask\")\n",
        "\n",
        "  # Apply the embedding model using masking\n",
        "  target_embedding = keras.layers.TimeDistributed(embedding_model)(input_target_eyes)\n",
        "  calibration_embeddings = keras.layers.TimeDistributed(embedding_model)(input_calibration_eyes)\n",
        "\n",
        "  calibration_weights = keras.layers.Dense(1, activation=\"sigmoid\", name=\"Calibration_Weights\")(calibration_embeddings)\n",
        "  calibration_weights_reshaped = keras.layers.Reshape((-1,), name=\"Calibration_Weights_Reshaped\")(calibration_weights)\n",
        "\n",
        "  ridge = WeightedRidgeRegressionLayer(RIDGE_REGULARIZATION, embedding_dim=EMBEDDING_DIM)([\n",
        "    target_embedding,\n",
        "    calibration_embeddings,\n",
        "    input_calibration_points,\n",
        "    calibration_weights_reshaped,\n",
        "    input_calibration_mask,\n",
        "    input_target_mask\n",
        "  ])\n",
        "\n",
        "  full_model = keras.Model(inputs=[\n",
        "    input_calibration_eyes,\n",
        "    input_calibration_points,\n",
        "    input_calibration_mask,\n",
        "    input_target_eyes,\n",
        "    input_target_mask\n",
        "  ], outputs=ridge, name=\"FullEyePredictionModel\")\n",
        "\n",
        "  return full_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtm4EUSZwXDC"
      },
      "source": [
        "## Full trainable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xAFsV9x41BZ"
      },
      "outputs": [],
      "source": [
        "full_model = create_full_model()\n",
        "\n",
        "full_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "full_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=masked_normalized_weighted_euc_dist,\n",
        "    metrics=[normalized_weighted_euc_dist],  # Keep the original for comparison if needed\n",
        "    jit_compile=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FnXA0JueNHm"
      },
      "outputs": [],
      "source": [
        "def lr_schedule(epoch):\n",
        "  if epoch < 15:\n",
        "    return LEARNING_RATE\n",
        "  else:\n",
        "    return LEARNING_RATE * 0.1\n",
        "\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzxl5IX0ycoR"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t15bFcCV4_yw"
      },
      "outputs": [],
      "source": [
        "train_history = full_model.fit(\n",
        "    train_ds_for_model.batch(4),\n",
        "    epochs=TRAIN_EPOCHS,\n",
        "    callbacks=[WandbMetricsLogger(), lr_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVk3s8vQRFpl"
      },
      "outputs": [],
      "source": [
        "full_model.save('full_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9SocMbzC5p"
      },
      "outputs": [],
      "source": [
        "full_model.save_weights('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZy4iASiRJvi"
      },
      "outputs": [],
      "source": [
        "wandb.save('full_model.keras')\n",
        "wandb.save('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_OUDhetR5LdO"
      },
      "outputs": [],
      "source": [
        "# Update the prediction code\n",
        "t2_processed = t2.map(\n",
        "    lambda x, y, id: (prepare_batch_for_model((x[0], x[1], x[2], x[3], x[4]), y)[0], y, id)\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Initialize a list to store the batch losses\n",
        "batch_losses = []\n",
        "subject_ids = []\n",
        "y_true = []\n",
        "\n",
        "for e in t2_processed.batch(1).as_numpy_iterator():\n",
        "    inputs = e[0]\n",
        "    y_true.append(e[1])\n",
        "    subject_ids.append(e[2][0])\n",
        "\n",
        "y_true = np.array(y_true).reshape(-1, 144, 2)\n",
        "\n",
        "# this step is slower than expected. not sure what's going on.\n",
        "predictions = full_model.predict(t2_processed.batch(1))\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
        "  batch_losses.append(loss)\n",
        "\n",
        "batch_losses = np.array(batch_losses)\n",
        "\n",
        "# Get mean per subject\n",
        "batch_losses = np.mean(batch_losses, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FdB2Anc88Qz"
      },
      "outputs": [],
      "source": [
        "final_loss_table = wandb.Table(data=[[s, l] for (s, l) in zip(subject_ids, batch_losses)], columns=[\"subject\", \"scores\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvKzpmrI9PBO"
      },
      "outputs": [],
      "source": [
        "final_loss_hist = wandb.plot.histogram(final_loss_table, \"scores\", title=\"Normalized Euclidean Distance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9iK_69wdrZ0"
      },
      "outputs": [],
      "source": [
        "final_loss_mean = np.mean(batch_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk8z0iV4DV2d"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"final_val_loss_table\": final_loss_table, \"final_val_loss_hist\": final_loss_hist, \"final_loss_mean\": final_loss_mean})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blgWDH3E5Swd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(batch_losses, bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Batch Losses')\n",
        "plt.xlabel('Loss')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXAdVnFKMjg7"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}