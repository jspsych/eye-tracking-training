{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LAJHmSmF84"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW4fMCshWBrD"
      },
      "outputs": [],
      "source": [
        "!pip install osfclient --quiet\n",
        "!pip install git+https://github.com/jspsych/eyetracking-utils.git --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install --upgrade keras --quiet\n",
        "!pip install keras-hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQRQZ3D5Vwqc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import keras_hub\n",
        "from keras import ops\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "import et_util.dataset_utils as dataset_utils\n",
        "import et_util.embedding_preprocessing as embed_pre\n",
        "import et_util.model_layers as model_layers\n",
        "from et_util import experiment_utils\n",
        "from et_util.custom_loss import normalized_weighted_euc_dist\n",
        "from et_util.model_analysis import plot_model_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BUzHdPEc-ef"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
        "os.environ['OSF_USERNAME'] = userdata.get('osfusername')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.version()"
      ],
      "metadata": {
        "id": "FKOfeTxzYAKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3M9X95jV8du"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9v9bxBcHc0"
      },
      "source": [
        "# Configure W&B experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SPGBdleCfbe"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWWKsFf6p1HJ"
      },
      "outputs": [],
      "source": [
        "# Fixed constants\n",
        "MAX_TARGETS = 144\n",
        "\n",
        "# Config constants\n",
        "EMBEDDING_DIM = 200\n",
        "RIDGE_REGULARIZATION = 0\n",
        "TRAIN_EPOCHS = 30\n",
        "MIN_CAL_POINTS = 8\n",
        "MAX_CAL_POINTS = 40\n",
        "DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "LEARNING_RATE = 0.001\n",
        "AUGMENTATION = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkHFv5kHfaWR"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"ridge_regularization\": RIDGE_REGULARIZATION,\n",
        "    \"train_epochs\": TRAIN_EPOCHS,\n",
        "    \"min_cal_points\": MIN_CAL_POINTS,\n",
        "    \"max_cal_points\": MAX_CAL_POINTS,\n",
        "    \"dense_net_stackwise_num_repeats\": DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"augmentation\": AUGMENTATION,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjDFWXNEddE9"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project='eye-tracking-dense-full-data-set-single-eye',\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeHGiFxtvo0"
      },
      "source": [
        "# Download dataset from OSF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tswV6s38WbtO"
      },
      "outputs": [],
      "source": [
        "!osf -p 6b5cd fetch single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgWOhWTt4iD"
      },
      "source": [
        "# Process raw data records into TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4h8QhGvWhZf"
      },
      "outputs": [],
      "source": [
        "!mkdir single_eye_tfrecords\n",
        "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZRcQgPadqtI"
      },
      "outputs": [],
      "source": [
        "def parse(element):\n",
        "    \"\"\"Process function that parses a tfr element in a raw dataset for process_tfr_to_tfds function.\n",
        "    Gets mediapipe landmarks, raw image, image width, image height, subject id, and xy labels.\n",
        "    Use for data generated with make_single_example_landmarks_and_jpg (i.e. data in\n",
        "    jpg_landmarks_tfrecords.tar.gz)\n",
        "\n",
        "    :param element: tfr element in raw dataset\n",
        "    :return: image, label(x,y), landmarks, subject_id\n",
        "    \"\"\"\n",
        "\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    landmarks = content['landmarks']\n",
        "    raw_image = content['eye_img']\n",
        "    width = content['img_width']\n",
        "    height = content['img_height']\n",
        "    depth = 3\n",
        "    label = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n",
        "    landmarks = tf.reshape(landmarks, shape=(478, 3))\n",
        "\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, landmarks, label, subject_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWv57eb5XHVP"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data, test_data = dataset_utils.process_tfr_to_tfds(\n",
        "    'single_eye_tfrecords/',\n",
        "    parse,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, landmarks, coords, z: z\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FULm58Cxu9oa"
      },
      "source": [
        "## Rescale the `x,y` coordinates to be 0-1 instead of 0-100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apcLK9klHLQV"
      },
      "outputs": [],
      "source": [
        "def rescale_coords_map(eyes, mesh, coords, id):\n",
        "  return eyes, mesh, tf.divide(coords, tf.constant([100.])), id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b47CtWKqHg0B"
      },
      "outputs": [],
      "source": [
        "train_data_rescaled = train_data.map(rescale_coords_map)\n",
        "validation_data_rescaled = validation_data.map(rescale_coords_map)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_masked_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Creates a dataset where each batch contains all images for a subject,\n",
        "    and a random subset is marked as calibration images via a mask.\n",
        "\n",
        "    Args:\n",
        "        dataset: The base TF dataset containing eye images and coordinates\n",
        "        batch_size: Number of images to include in each batch\n",
        "\n",
        "    Returns:\n",
        "        A TF dataset with masked images ready for model training\n",
        "    \"\"\"\n",
        "    # Step 1: Group dataset by subject_id and batch all images\n",
        "    def group_by_subject(subject_id, ds):\n",
        "        return ds.batch(batch_size=MAX_TARGETS)\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, mesh, coords, subject_id: subject_id,\n",
        "        reduce_func=group_by_subject,\n",
        "        window_size=MAX_TARGETS\n",
        "    )\n",
        "\n",
        "    # Step 2: Filter out subjects with fewer than 72 images\n",
        "    def filter_by_image_count(images, meshes, coords, subject_ids):\n",
        "        return tf.shape(images)[0] >= 72\n",
        "\n",
        "    grouped_dataset = grouped_dataset.filter(filter_by_image_count)\n",
        "\n",
        "    # Step 3: Transform each batch to include masks\n",
        "    def add_masks_to_batch(images, meshes, coords, subject_ids):\n",
        "\n",
        "        actual_batch_size = tf.shape(images)[0]\n",
        "\n",
        "        cal_mask = tf.zeros(MAX_TARGETS, dtype=tf.int8)\n",
        "        target_mask = tf.zeros(MAX_TARGETS, dtype=tf.int8)\n",
        "\n",
        "        # Determine how many calibration images to use (random between min and max)\n",
        "        n_cal_images = tf.random.uniform(\n",
        "            shape=[],\n",
        "            minval=MIN_CAL_POINTS,\n",
        "            maxval=MAX_CAL_POINTS,\n",
        "            dtype=tf.int32\n",
        "        )\n",
        "\n",
        "        # Create random indices for calibration images\n",
        "        # NOTE: These will be different each time through the dataset\n",
        "        random_indices = tf.random.shuffle(tf.range(actual_batch_size))\n",
        "        cal_indices = random_indices[:n_cal_images]\n",
        "\n",
        "        # Create masks (1 = included, 0 = excluded)\n",
        "        cal_mask = tf.scatter_nd(\n",
        "            tf.expand_dims(cal_indices, 1),\n",
        "            tf.ones(n_cal_images, dtype=tf.int8),\n",
        "            [MAX_TARGETS]\n",
        "        )\n",
        "        target_mask = 1 - cal_mask\n",
        "\n",
        "        # Pad everything to fixed size\n",
        "\n",
        "        padded_images = tf.pad(\n",
        "            tf.reshape(images, (-1, 36, 144, 1)),\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(\n",
        "            coords,\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n",
        "        )\n",
        "\n",
        "        # Ensure all shapes are fixed\n",
        "        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(cal_mask, [MAX_TARGETS])\n",
        "        padded_target_mask = tf.ensure_shape(target_mask, [MAX_TARGETS])\n",
        "\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords\n",
        "\n",
        "    # Apply the transformation\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        lambda imgs, meshes, coords, subj_ids: add_masks_to_batch(imgs, meshes, coords, subj_ids),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return masked_dataset\n",
        "\n",
        "# Modified model input preparation function\n",
        "def prepare_model_inputs(features, labels):\n",
        "    \"\"\"\n",
        "    Restructure the inputs for the model, using the mask-based approach.\n",
        "\n",
        "    Args:\n",
        "        features: Tuple of (images, coords, cal_mask, target_mask)\n",
        "        labels: The target coordinates\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of inputs for the model and the target labels\n",
        "    \"\"\"\n",
        "    images, coords, cal_mask, target_mask = features\n",
        "\n",
        "    # Use masking to create the model inputs\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": images,                   # All images (both cal and target)\n",
        "        \"Input_All_Coords\": coords,                   # All coordinates\n",
        "        \"Input_Calibration_Mask\": cal_mask,           # Mask indicating calibration images\n",
        "        \"Input_Target_Mask\": target_mask              # Mask indicating target images\n",
        "    }\n",
        "\n",
        "    return inputs, labels"
      ],
      "metadata": {
        "id": "2-AX95Hj7_mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_dataset = prepare_masked_dataset(train_data_rescaled)"
      ],
      "metadata": {
        "id": "eTBbOOV52Uqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_for_model = masked_dataset.map(\n",
        "        prepare_model_inputs,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "21AN7OpI4ee2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import HTML\n",
        "import base64\n",
        "\n",
        "def visualize_eye_tracking_data(dataset, interval=100, figsize=(10, 6)):\n",
        "    \"\"\"\n",
        "    Visualize eye tracking data by animating through a subject's images and displaying\n",
        "    where they are looking with a red dot. Returns an HTML animation for direct display\n",
        "    in a Colab notebook.\n",
        "\n",
        "    Args:\n",
        "        dataset: A TensorFlow dataset containing a single batch of one subject's data\n",
        "                Expected format: ({\"Input_All_Images\": images, \"Input_All_Coords\": coords,\n",
        "                                 \"Input_Calibration_Mask\": cal_mask, \"Input_Target_Mask\": target_mask}, labels)\n",
        "                OR: Your model's dataset with one subject batch\n",
        "        interval: Time between frames in milliseconds\n",
        "        figsize: Size of the figure (width, height)\n",
        "\n",
        "    Returns:\n",
        "        IPython.display.HTML object with the animation\n",
        "\n",
        "    Example usage in a Colab notebook:\n",
        "    ```python\n",
        "    # For a dataset from a single subject\n",
        "    subject_dataset = train_ds_for_model.take(1)\n",
        "\n",
        "    # Or you can create a dataset for just one subject (example from your notebook):\n",
        "    # This line extracts data for one subject from the complete dataset\n",
        "    single_subject = train_ds_for_model.filter(\n",
        "        lambda inputs, labels: tf.equal(inputs[\"Input_Subject_ID\"][0], subject_id)\n",
        "    ).take(1)\n",
        "\n",
        "    # Display the animation directly in the notebook\n",
        "    from IPython.display import display\n",
        "    animation = visualize_eye_tracking_data(single_subject)\n",
        "    display(animation)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    # Extract data from the dataset\n",
        "    for inputs, labels in dataset.take(1):\n",
        "        images = inputs[\"Input_All_Images\"]\n",
        "        coords = inputs[\"Input_All_Coords\"]\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        valid_images = images.numpy()\n",
        "        valid_coords = coords.numpy()\n",
        "\n",
        "        # Sort by X coordinate, then by Y coordinate\n",
        "        sorted_indices = np.lexsort((valid_coords[:, 0], valid_coords[:, 1]))\n",
        "        valid_images = valid_images[sorted_indices]\n",
        "        valid_coords = valid_coords[sorted_indices]\n",
        "\n",
        "        num_images = valid_images.shape[0]\n",
        "\n",
        "    # Set up the figure\n",
        "    plt.ioff()  # Turn off interactive mode to avoid displaying during generation\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Create a function that draws each frame\n",
        "    def draw_frame(frame_num):\n",
        "        ax.clear()\n",
        "\n",
        "        # Set up the 16:9 coordinate space\n",
        "        ax.set_xlim(0, 100)\n",
        "        ax.set_ylim(100, 0)  # 16:9 aspect ratio\n",
        "\n",
        "        # Get the current image and coordinates\n",
        "        img = valid_images[frame_num].squeeze()\n",
        "        x, y = valid_coords[frame_num]\n",
        "\n",
        "        # Draw rectangle for the screen\n",
        "        rect = plt.Rectangle((0, 0), 100, 100, fill=False, color='black', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Add image display in the top right corner\n",
        "        img_display = ax.inset_axes([0.35, 0.35, 0.3, 0.3], transform=ax.transAxes)\n",
        "        img_display.imshow(np.fliplr(img), cmap='gray')\n",
        "        img_display.axis('off')\n",
        "\n",
        "        # Draw red dot at the coordinate - make it bigger for visibility\n",
        "        ax.scatter(x * 100, y * 100, color='red', s=150, zorder=5,\n",
        "                  edgecolor='white', linewidth=1.5)\n",
        "\n",
        "        # Draw crosshair\n",
        "        ax.axhline(y * 100, color='gray', linestyle='--', alpha=0.5, zorder=1)\n",
        "        ax.axvline(x * 100, color='gray', linestyle='--', alpha=0.5, zorder=1)\n",
        "\n",
        "        # Set title\n",
        "        ax.set_title('Eye Tracking Visualization')\n",
        "\n",
        "        # Set axis labels\n",
        "        ax.set_xlabel('X Coordinate')\n",
        "        ax.set_ylabel('Y Coordinate')\n",
        "\n",
        "    # Create the animation\n",
        "    anim = animation.FuncAnimation(fig, draw_frame, frames=num_images, interval=interval)\n",
        "\n",
        "  # Use animation's to_jshtml method which directly generates HTML\n",
        "    html_animation = anim.to_jshtml()\n",
        "\n",
        "    # Clean up\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Create an HTML object that can be displayed in the notebook\n",
        "    return HTML(html_animation)\n"
      ],
      "metadata": {
        "id": "glQLYELqaOg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_eye_tracking_data(train_ds_for_model.take(1))"
      ],
      "metadata": {
        "id": "oB2m-VExaRSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the weighted regression layer"
      ],
      "metadata": {
        "id": "0UfY_C8zMW8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTimeDistributed(keras.layers.Layer):\n",
        "    \"\"\"A simplified version of TimeDistributed that applies a layer to every temporal slice of an input.\n",
        "\n",
        "    This implementation avoids for loops by using reshape operations to apply the wrapped layer\n",
        "    to all time steps at once.\n",
        "\n",
        "    Args:\n",
        "        layer: a `keras.layers.Layer` instance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer = layer\n",
        "        self.supports_masking = getattr(layer, 'supports_masking', False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Validate input shape has at least 3 dimensions (batch, time, ...)\n",
        "        if not isinstance(input_shape, (tuple, list)) or len(input_shape) < 3:\n",
        "            raise ValueError(\n",
        "                \"`SimpleTimeDistributed` requires input with at least 3 dimensions\"\n",
        "            )\n",
        "\n",
        "        # Build the wrapped layer with shape excluding the time dimension\n",
        "        self.layer.build((input_shape[0], *input_shape[2:]))\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # Get output shape by applying the layer to a single time slice\n",
        "        child_output_shape = self.layer.compute_output_shape((input_shape[0], *input_shape[2:]))\n",
        "        # Include time dimension in the result\n",
        "        return (child_output_shape[0], input_shape[1], *child_output_shape[1:])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        time_steps = input_shape[1]\n",
        "\n",
        "        # Reshape inputs to combine batch and time dimensions: (batch*time, ...)\n",
        "        reshaped_inputs = ops.reshape(inputs, (-1, *input_shape[2:]))\n",
        "\n",
        "        # Apply the layer to all time steps at once\n",
        "        outputs = self.layer(reshaped_inputs, training=training)\n",
        "\n",
        "        # Get output dimensions\n",
        "        output_shape = ops.shape(outputs)\n",
        "\n",
        "        # Reshape back to include the separate batch and time dimensions: (batch, time, ...)\n",
        "        return ops.reshape(outputs, (batch_size, time_steps, *output_shape[1:]))"
      ],
      "metadata": {
        "id": "JCx-whu4HlDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuCld98e7vAZ"
      },
      "outputs": [],
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom layer that performs weighted ridge regression with proper masking support.\n",
        "\n",
        "    This layer takes embeddings, coordinates, weights, and calibration mask as explicit inputs,\n",
        "    while using Keras' masking system to handle target masking. This separation allows more\n",
        "    precise control over calibration points while leveraging Keras' built-in mask propagation\n",
        "    for target predictions.\n",
        "\n",
        "    Args:\n",
        "        lambda_ridge (float): Regularization parameter for ridge regression\n",
        "        embedding_dim (int): Dimension of the embedding vectors\n",
        "        epsilon (float): Small constant for numerical stability\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_ridge, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        The forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "            inputs: A list containing:\n",
        "                - embeddings: Embeddings for all points (batch_size, n_points, embedding_dim)\n",
        "                - coords: Coordinates for all points (batch_size, n_points, 2)\n",
        "                - calibration_weights: Importance weights (batch_size, n_points, 1)\n",
        "                - cal_mask: Mask for calibration points (batch_size, n_points) [EXPLICIT]\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            Predicted coordinates for the target points (batch_size, n_points, 2)\n",
        "        \"\"\"\n",
        "        # Unpack inputs\n",
        "        embeddings, coords, calibration_weights, cal_mask  = inputs\n",
        "\n",
        "        embeddings = ops.cast(embeddings, \"float32\")\n",
        "        coords = ops.cast(coords, \"float32\")\n",
        "        calibration_weights = ops.cast(calibration_weights, \"float32\")\n",
        "        cal_mask = ops.cast(cal_mask, \"float32\")\n",
        "\n",
        "        # reshape weights to (batch, calibration)\n",
        "        w = ops.squeeze(calibration_weights, axis=-1)\n",
        "\n",
        "        # Pre-compute masked weights for calibration points\n",
        "        w_masked = w * cal_mask\n",
        "        w_sqrt = ops.sqrt(w_masked)\n",
        "        w_sqrt = ops.expand_dims(w_sqrt, -1)  # More efficient than reshape\n",
        "\n",
        "        # Apply calibration mask to embeddings\n",
        "        cal_mask_expand = ops.expand_dims(cal_mask, -1)\n",
        "        X = embeddings * cal_mask_expand\n",
        "\n",
        "        # Weight calibration embeddings and coordinates\n",
        "        X_weighted = X * w_sqrt\n",
        "        y_weighted = coords * w_sqrt\n",
        "\n",
        "        # Matrix operations\n",
        "        X_t = ops.transpose(X_weighted, axes=[0, 2, 1])\n",
        "        X_t_X = ops.matmul(X_t, X_weighted)\n",
        "\n",
        "        # Add regularization\n",
        "        lhs = X_t_X + (self.lambda_ridge + self.epsilon) * ops.eye(embeddings.shape[-1])\n",
        "\n",
        "        # Compute RHS\n",
        "        rhs = ops.matmul(X_t, y_weighted)\n",
        "\n",
        "        # Solve the system\n",
        "        kernel = ops.linalg.solve(lhs, rhs)\n",
        "\n",
        "        # Apply regression\n",
        "        output = ops.matmul(embeddings, kernel)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        \"\"\"\n",
        "        Computes the output shape of the layer.\n",
        "\n",
        "        Args:\n",
        "            input_shapes: List of input shapes\n",
        "\n",
        "        Returns:\n",
        "            Output shape tuple\n",
        "        \"\"\"\n",
        "        return (input_shapes[0][0], input_shapes[0][1], 2)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration of the layer for serialization.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing the layer configuration\n",
        "        \"\"\"\n",
        "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"lambda_ridge\": self.lambda_ridge,\n",
        "            \"epsilon\": self.epsilon\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "wbWzR7yfMZv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdQbBaKR6vLN"
      },
      "source": [
        "# Generate dataset that has calibration points, target point, and target output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Stuff"
      ],
      "metadata": {
        "id": "Y3a-BpZNsvUE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCSmeKFBH5YQ"
      },
      "source": [
        "Cols = 5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95\n",
        "\n",
        "Rows = 5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP0_I2DUuEKo"
      },
      "source": [
        "Fixed Points as calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnKs983UsyDj"
      },
      "outputs": [],
      "source": [
        "def create_embedding_model():\n",
        "  image_shape = (36, 144, 1)\n",
        "  input_eyes = keras.layers.Input(shape=image_shape)\n",
        "\n",
        "  # Continue with the backbone\n",
        "  backbone = keras_hub.models.DenseNetBackbone(\n",
        "      stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "      image_shape=image_shape,\n",
        "  )\n",
        "  backbone_encoder = backbone(input_eyes)\n",
        "\n",
        "  # backbone = keras.Sequential([\n",
        "  #     keras.layers.Flatten(),\n",
        "  #     keras.layers.Dense(10, activation=\"relu\")\n",
        "  # ])\n",
        "\n",
        "  # backbone_encoder = backbone(input_eyes)\n",
        "\n",
        "  # backbone = keras_hub.models.MiTBackbone(\n",
        "  #   image_shape=(36,144,1),\n",
        "  #   layerwise_depths=[2,2,2,2],\n",
        "  #   num_layers=4,\n",
        "  #   layerwise_num_heads=[1,2,5,8],\n",
        "  #   layerwise_sr_ratios=[8,4,2,1],\n",
        "  #   max_drop_path_rate=0.1,\n",
        "  #   layerwise_patch_sizes=[7,3,3,3],\n",
        "  #   layerwise_strides=[4,2,2,2],\n",
        "  #   hidden_dims=[32,64,160,256]\n",
        "  # )\n",
        "  # backbone_encoder = backbone(input_eyes)\n",
        "\n",
        "  flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "  eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "  embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "\n",
        "  return embedding_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_normalized_weighted_euc_dist(y_true, y_pred):\n",
        "    \"\"\"Custom loss function that calculates a weighted Euclidean distance between two sets of points,\n",
        "    respecting masks for padded sequences.\n",
        "\n",
        "    Weighting multiplies the x-coordinate of each input by 1.778 (derived from the 16:9 aspect ratio of most laptops)\n",
        "    to match the scale of the y-axis. For interpretability, the distances are normalized to the diagonal\n",
        "    such that the maximum distance between two points (corner to corner) is 100.\n",
        "\n",
        "    :param y_true (tensor): A tensor of shape (batch_size, seq_len, 2) containing ground-truth x- and y- coordinates\n",
        "    :param y_pred (tensor): A tensor of shape (batch_size, seq_len, 2) containing predicted x- and y- coordinates\n",
        "\n",
        "    :returns: A tensor of shape (1,) with the weighted and normalized euclidean distances between valid points.\n",
        "    \"\"\"\n",
        "    y_true = ops.cast(y_true, \"float32\")\n",
        "    y_pred = ops.cast(y_pred, \"float32\")\n",
        "\n",
        "    # Weighting treats y-axis as unit-scale and creates a rectangle that's 177.8x100 units.\n",
        "    x_weight = ops.convert_to_tensor([1.778, 1.0], dtype=\"float32\")\n",
        "\n",
        "    # Multiply x-coordinate by 16/9 = 1.778\n",
        "    y_true_weighted = ops.multiply(x_weight, y_true)\n",
        "    y_pred_weighted = ops.multiply(x_weight, y_pred)\n",
        "\n",
        "    # Calculate Euclidean distance with weighted coordinates\n",
        "    squared_diff = ops.square(y_pred_weighted - y_true_weighted)\n",
        "    squared_dist = ops.sum(squared_diff, axis=-1)\n",
        "    dist = ops.sqrt(squared_dist)\n",
        "\n",
        "    # Euclidean Distance from [0,0] to [177.8, 100] = 203.992\n",
        "    norm_scale = ops.convert_to_tensor(203.992, dtype=\"float32\")\n",
        "\n",
        "    # Normalizes loss values to the diagonal-- makes loss easier to interpret\n",
        "    normalized_dist = (dist / norm_scale) * 100\n",
        "\n",
        "    return normalized_dist"
      ],
      "metadata": {
        "id": "y5Kghim-rVhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskInspectorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MaskInspectorLayer, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "    def call(self, inputs, mask=None):\n",
        "      # Print mask information\n",
        "      tf.print(\"Layer mask:\", mask)\n",
        "      return inputs"
      ],
      "metadata": {
        "id": "vt7x7bN6EzGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Modified model to work with the new masked input approach\n",
        "def create_masked_model():\n",
        "    \"\"\"\n",
        "    Create a model that uses masks to distinguish calibration and target images.\n",
        "\n",
        "    This implementation:\n",
        "    - Uses explicit calibration mask as a regular input\n",
        "    - Integrates target masking with Keras' masking system\n",
        "    - Properly propagates the target mask through the network\n",
        "    \"\"\"\n",
        "    # Create the embedding model\n",
        "    embedding_model = create_embedding_model()\n",
        "\n",
        "    # Define inputs\n",
        "    input_all_images = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 36, 144, 1),\n",
        "        name=\"Input_All_Images\"\n",
        "    )\n",
        "\n",
        "    input_all_coords = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 2),\n",
        "        name=\"Input_All_Coords\"\n",
        "    )\n",
        "\n",
        "    # Calibration mask is a regular input (not using Keras masking)\n",
        "    input_cal_mask = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS,),\n",
        "        name=\"Input_Calibration_Mask\",\n",
        "        dtype=\"int8\"\n",
        "    )\n",
        "\n",
        "    # Target mask gets proper Keras masking\n",
        "    input_target_mask = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS,),\n",
        "        name=\"Input_Target_Mask\",\n",
        "        dtype=\"int8\"\n",
        "    )\n",
        "\n",
        "    # Create a Keras masking layer specifically for target mask\n",
        "    # This layer generates a proper Keras mask from the target_mask values\n",
        "    #masked_target = keras.layers.Masking(mask_value=0.0)(input_target_mask)\n",
        "\n",
        "    # Apply the embedding model to all images\n",
        "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\"Image_Embeddings\")(input_all_images)\n",
        "\n",
        "    # Calculate importance weights for calibration points\n",
        "    calibration_weights = keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\"sigmoid\",\n",
        "        name=\"Calibration_Weights\"\n",
        "    )(all_embeddings)\n",
        "\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
        "        RIDGE_REGULARIZATION,\n",
        "        name=\"Regression\"\n",
        "    )(\n",
        "        # Inputs to the regression layer\n",
        "        [\n",
        "            all_embeddings,           # Embeddings for all points\n",
        "            input_all_coords,         # Coordinates for all points\n",
        "            calibration_weights,      # Weights for calibration importance\n",
        "            input_cal_mask,            # Explicit calibration mask\n",
        "        ],\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "    reshaped_target_mask = keras.layers.Reshape((-1,1), name=\"Reshaped_Target_Mask\")(input_target_mask)\n",
        "    mask = keras.layers.Masking(mask_value=0, name=\"Mask\")(reshaped_target_mask)\n",
        "    output = keras.layers.Multiply(name=\"Masked_Predictions\")([ridge, mask])\n",
        "    # mask_inspector = MaskInspectorLayer(name=\"Mask_Inspector\")\n",
        "    # output = mask_inspector(output)\n",
        "\n",
        "    # Create the full model with proper masking connections\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "            input_target_mask\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\"MaskedEyePredictionModel\"\n",
        "    )\n",
        "\n",
        "    return full_model"
      ],
      "metadata": {
        "id": "OeXnPUbC08k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build stuff"
      ],
      "metadata": {
        "id": "Lptb4zfQsywN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model = create_masked_model()"
      ],
      "metadata": {
        "id": "fPT1PjTv4tNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model.summary()"
      ],
      "metadata": {
        "id": "RNxIV_1xIrHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True\n",
        ")"
      ],
      "metadata": {
        "id": "S1SuTboRMtah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_model = keras.Model(inputs=mask_model.input, outputs=mask_model.get_layer(\"Mask\").output)"
      ],
      "metadata": {
        "id": "lrcfdrqxOmDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model.fit(train_ds_for_model.batch(1), epochs=3)"
      ],
      "metadata": {
        "id": "9QWxiUiBMmue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, z, r = mask_model.predict(train_ds_for_model.take(1).batch(1))\n",
        "#y = sub_model.predict(train_ds_for_model.take(1).batch(1))"
      ],
      "metadata": {
        "id": "zVho1DmlHyyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y*r"
      ],
      "metadata": {
        "id": "Iyr0127XBZxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the new layers\n"
      ],
      "metadata": {
        "id": "IFkHLca_yRXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------------------------\n",
        "# Set up synthetic test data for two batches.\n",
        "# ---------------------------\n",
        "batch_size = 2\n",
        "n_points = 5\n",
        "embedding_dim = 3\n",
        "\n",
        "# Define a true regression kernel (shared across batches)\n",
        "kernel_true = np.array([[1, 2],\n",
        "                        [3, 4],\n",
        "                        [5, 6]], dtype=np.float32)\n",
        "\n",
        "# Batch 0 embeddings:\n",
        "embeddings_batch0 = np.array([\n",
        "    [1, 0, 0],   # calibration point (index 0)\n",
        "    [0, 1, 0],   # calibration point (index 1)\n",
        "    [0, 0, 1],   # calibration point (index 2)\n",
        "    [1, 1, 0],   # target point (index 3)\n",
        "    [0, 1, 1]    # target point (index 4)\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Batch 1 embeddings:\n",
        "embeddings_batch1 = np.array([\n",
        "    [0, 1, 0],    # calibration point (index 0)\n",
        "    [1, 0, 0],    # calibration point (index 1)\n",
        "    [0, 0, 1],    # calibration point (index 2)\n",
        "    [1, 1, 1],    # calibration point (index 3)\n",
        "    [0.5, 0.5, 0.5]  # target point (index 4)\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Stack embeddings for both batches: shape (2, 5, 3)\n",
        "embeddings = np.stack([embeddings_batch0, embeddings_batch1], axis=0)\n",
        "\n",
        "# Compute coordinates for each batch using the true kernel.\n",
        "# Shape (2, 5, 2)\n",
        "coords = np.matmul(embeddings, kernel_true)\n",
        "\n",
        "# Calibration weights: use ones for both batches, shape (2, 5, 1)\n",
        "calibration_weights = np.ones((batch_size, n_points, 1), dtype=np.float32)\n",
        "\n",
        "# ---------------------------\n",
        "# Define disjoint calibration and target masks.\n",
        "# For Batch 0, calibration points: indices 0, 1, 2; target points: indices 3, 4.\n",
        "# For Batch 1, calibration points: indices 0, 1, 2, 3; target point: index 4.\n",
        "cal_mask = np.array([\n",
        "    [1, 1, 1, 0, 0],  # Batch 0 calibration mask\n",
        "    [1, 1, 1, 1, 0]   # Batch 1 calibration mask\n",
        "], dtype=np.float32)\n",
        "\n",
        "target_mask = np.array([\n",
        "    [0, 0, 0, 1, 1],  # Batch 0 target mask\n",
        "    [0, 0, 0, 0, 1]   # Batch 1 target mask\n",
        "], dtype=np.float32)\n",
        "\n",
        "# ---------------------------\n",
        "# Build and run the model.\n",
        "# ---------------------------\n",
        "input_embeddings = keras.layers.Input(shape=(n_points, embedding_dim), name=\"embeddings\")\n",
        "input_coords = keras.layers.Input(shape=(n_points, 2), name=\"coords\")\n",
        "input_calibration_weights = keras.layers.Input(shape=(n_points, 1), name=\"calibration_weights\")\n",
        "input_cal_mask = keras.layers.Input(shape=(n_points,), name=\"cal_mask\")\n",
        "input_target_mask = keras.layers.Input(shape=(n_points,), name=\"target_mask\")\n",
        "\n",
        "# Use lambda_ridge=0.0 to recover the true kernel exactly.\n",
        "output = MaskedWeightedRidgeRegressionLayer(lambda_ridge=0.0, embedding_dim=embedding_dim)(\n",
        "    [input_embeddings, input_coords, input_calibration_weights, input_cal_mask, input_target_mask],\n",
        "    # mask=keras.layers.Masking(mask_value=0.0).compute_mask(input_target_mask)\n",
        ")\n",
        "\n",
        "model = keras.models.Model(\n",
        "    inputs=[input_embeddings, input_coords, input_calibration_weights, input_cal_mask, input_target_mask],\n",
        "    outputs=output\n",
        ")\n",
        "\n",
        "model.compile(loss=masked_normalized_weighted_euc_dist)\n",
        "\n",
        "# Train the model.\n",
        "# Run prediction.\n",
        "result = model.predict([embeddings, coords, calibration_weights, cal_mask, target_mask])\n",
        "\n",
        "print(\"Predicted coordinates for both batches:\")\n",
        "print(result)\n",
        "\n",
        "# ---------------------------\n",
        "# Expected Output:\n",
        "#\n",
        "# Batch 0:\n",
        "#   Calibration indices: 0,1,2 (used to compute the kernel)\n",
        "#   Target indices: 3 and 4.\n",
        "#   For index 3: [1,1,0] dot kernel_true = [1*1 + 1*3 + 0*5, 1*2 + 1*4 + 0*6] = [4,6].\n",
        "#   For index 4: [0,1,1] dot kernel_true = [0*1 + 1*3 + 1*5, 0*2 + 1*4 + 1*6] = [8,10].\n",
        "#\n",
        "# Batch 1:\n",
        "#   Calibration indices: 0,1,2,3 (used to compute the kernel)\n",
        "#   Target index: 4.\n",
        "#   For index 4: [0.5,0.5,0.5] dot kernel_true = [0.5*(1+3+5), 0.5*(2+4+6)] = [4.5,6].\n",
        "#\n",
        "expected_output_batch0 = np.array([\n",
        "    [0, 0],\n",
        "    [0, 0],\n",
        "    [0, 100],\n",
        "    [4, 6],\n",
        "    [8, 10]\n",
        "], dtype=np.float32)\n",
        "\n",
        "expected_output_batch1 = np.array([\n",
        "    [100, 0],\n",
        "    [0, 0],\n",
        "    [0, 0],\n",
        "    [0, 100],\n",
        "    [4.4999, 6]\n",
        "], dtype=np.float32)\n",
        "\n",
        "expected_output = np.stack([expected_output_batch0, expected_output_batch1], axis=0)\n",
        "\n",
        "print(\"\\nExpected coordinates for both batches:\")\n",
        "print(expected_output)\n",
        "\n",
        "x = model.evaluate([embeddings, coords, calibration_weights, cal_mask, target_mask], expected_output, batch_size=1)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "zFZEX35f1XF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def test_masked_normalized_weighted_euc_dist():\n",
        "    \"\"\"Test suite for masked_normalized_weighted_euc_dist function\"\"\"\n",
        "\n",
        "    print(\"Running tests for masked_normalized_weighted_euc_dist:\")\n",
        "\n",
        "    # Test 1: Basic functionality - identical inputs should have zero distance\n",
        "    y_true = jnp.array([[[1.0, 2.0], [3.0, 4.0]]])\n",
        "    y_pred = jnp.array([[[1.0, 2.0], [3.0, 4.0]]])\n",
        "    loss = masked_normalized_weighted_euc_dist(y_true, y_pred)\n",
        "    print(f\"Test 1 - Identical inputs: {float(loss):.6f}\")\n",
        "    assert np.isclose(float(loss), 0.0), \"Identical inputs should have zero distance\"\n",
        "\n",
        "    # Test 2: Masking - verify padded points are ignored\n",
        "    y_true = jnp.array([[[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]])\n",
        "    y_pred = jnp.array([[[1.1, 2.1], [0.0, 0.0], [5.1, 6.1]]])  # Middle point is padded\n",
        "    loss = masked_normalized_weighted_euc_dist(y_true, y_pred)\n",
        "    print(f\"Test 2 - Masked sequence: {float(loss):.6f}\")\n",
        "\n",
        "    # Count non-padded elements (should be 2)\n",
        "    mask = jnp.any(jnp.not_equal(y_pred, 0), axis=-1).astype(jnp.float32)\n",
        "    num_valid = float(jnp.sum(mask))\n",
        "    print(f\"  Number of valid (non-padded) points: {num_valid}\")\n",
        "    assert num_valid == 2, \"Expected 2 non-padded points\"\n",
        "\n",
        "    # Test 3: Batch processing with mixed padding\n",
        "    y_true = jnp.array([\n",
        "        [[1.0, 2.0], [3.0, 4.0]],  # Sample 1\n",
        "        [[5.0, 6.0], [7.0, 8.0]]   # Sample 2\n",
        "    ])\n",
        "    y_pred = jnp.array([\n",
        "        [[1.0, 2.0], [0.0, 0.0]],  # Sample 1 with second point padded\n",
        "        [[5.1, 6.1], [7.1, 8.1]]   # Sample 2 with no padding\n",
        "    ])\n",
        "\n",
        "    loss = masked_normalized_weighted_euc_dist(y_true, y_pred)\n",
        "    print(f\"Test 3 - Batch handling: {float(loss):.6f}\")\n",
        "\n",
        "    # Count non-padded elements (should be 3: 1 from first batch, 2 from second)\n",
        "    mask = jnp.any(jnp.not_equal(y_pred, 0), axis=-1).astype(jnp.float32)\n",
        "    num_valid = float(jnp.sum(mask))\n",
        "    print(f\"  Number of valid (non-padded) points: {num_valid}\")\n",
        "    assert num_valid == 3, \"Expected 3 non-padded points in batch\"\n",
        "\n",
        "    # Test 4: Edge case - all points padded (should not divide by zero)\n",
        "    y_true = jnp.array([[[1.0, 2.0], [3.0, 4.0]]])\n",
        "    y_pred = jnp.array([[[0.0, 0.0], [0.0, 0.0]]])  # All padded\n",
        "    loss = masked_normalized_weighted_euc_dist(y_true, y_pred)\n",
        "    print(f\"Test 4 - All padded: {float(loss):.6f}\")\n",
        "    assert np.isclose(float(loss), 0.0), \"All padded points should result in zero loss\"\n",
        "\n",
        "    # Test 5: Verify mask application by comparing with manual calculation\n",
        "    y_true = jnp.array([\n",
        "        [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n",
        "    ])\n",
        "    y_pred = jnp.array([\n",
        "        [[1.1, 2.1], [0.0, 0.0], [5.1, 6.1]]  # Middle point padded\n",
        "    ])\n",
        "\n",
        "    # Manual calculation to verify masking\n",
        "    manual_mask = jnp.array([[1.0, 0.0, 1.0]])  # Only first and third points are valid\n",
        "\n",
        "    # Apply function with automatic masking\n",
        "    auto_masked_loss = masked_normalized_weighted_euc_dist(y_true, y_pred)\n",
        "\n",
        "    # Manual calculation of distance for comparison\n",
        "    y_true_float = y_true.astype(jnp.float32)\n",
        "    y_pred_float = y_pred.astype(jnp.float32)\n",
        "\n",
        "    x_weight = jnp.array([1.778, 1.0], dtype=jnp.float32)\n",
        "    y_true_weighted = y_true_float * x_weight\n",
        "    y_pred_weighted = y_pred_float * x_weight\n",
        "\n",
        "    squared_diff = jnp.square(y_pred_weighted - y_true_weighted)\n",
        "    squared_dist = jnp.sum(squared_diff, axis=-1)\n",
        "    dist = jnp.sqrt(squared_dist)\n",
        "\n",
        "    normalized_dist = (dist / jnp.array(203.992, dtype=jnp.float32)) * 100\n",
        "\n",
        "    # Apply manual mask and calculate mean\n",
        "    masked_dist = normalized_dist * manual_mask\n",
        "    manual_loss = jnp.sum(masked_dist) / jnp.sum(manual_mask)\n",
        "\n",
        "    print(f\"Test 5 - Masking verification:\")\n",
        "    print(f\"  Auto-masked loss: {float(auto_masked_loss):.6f}\")\n",
        "    print(f\"  Manual masked loss: {float(manual_loss):.6f}\")\n",
        "\n",
        "    assert np.isclose(float(auto_masked_loss), float(manual_loss), rtol=1e-5), \\\n",
        "        \"Auto-masking should match manual masking calculation\"\n",
        "\n",
        "    print(\"All tests passed successfully!\")"
      ],
      "metadata": {
        "id": "Tu63N2kqVc7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_masked_normalized_weighted_euc_dist()"
      ],
      "metadata": {
        "id": "keF1wFG7VeHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def test_simple_time_distributed():\n",
        "    batch_size = 5\n",
        "    time_steps = 4\n",
        "    feature_dim = 5\n",
        "    output_dim = 3\n",
        "\n",
        "    # Use a Lambda layer that adds 1 to each element\n",
        "    wrapped_layer = keras.layers.Lambda(lambda x: x + 1)\n",
        "    time_distributed_layer = SimpleTimeDistributed(wrapped_layer)\n",
        "\n",
        "    # Create structured input\n",
        "    inputs = tf.constant(np.arange(batch_size * time_steps * feature_dim).reshape(batch_size, time_steps, feature_dim), dtype=tf.float32)\n",
        "\n",
        "    # Apply the layer\n",
        "    outputs = time_distributed_layer(inputs)\n",
        "\n",
        "    # Expected output\n",
        "    expected_outputs = inputs + 1\n",
        "\n",
        "    # Check if shape is preserved\n",
        "    shape_correct = outputs.shape == inputs.shape\n",
        "    print(\"Shape preserved:\", shape_correct)\n",
        "\n",
        "    # Check if elements are in correct order\n",
        "    values_correct = np.array_equal(jnp.array(outputs), jnp.array(expected_outputs))\n",
        "    print(\"Order preserved:\", values_correct)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nInput:\\n\", inputs.numpy())\n",
        "    print(\"\\nOutput:\\n\", jnp.array(outputs))\n",
        "    print(\"\\nExpected Output:\\n\", expected_outputs.numpy())\n",
        "\n",
        "# Run the test\n",
        "test_simple_time_distributed()"
      ],
      "metadata": {
        "id": "rnV_WQZqGGwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import unittest\n",
        "\n",
        "class TestSimpleTimeDistributed(unittest.TestCase):\n",
        "\n",
        "    def test_basic_functionality(self):\n",
        "        \"\"\"Test the layer with a simple Dense layer.\"\"\"\n",
        "        # Create a model with SimpleTimeDistributed\n",
        "        inputs = keras.Input(shape=(3, 4))  # (batch, time, features)\n",
        "        dense_layer = keras.layers.Dense(5, activation='relu')\n",
        "        x = SimpleTimeDistributed(dense_layer)(inputs)\n",
        "        model = keras.Model(inputs, x)\n",
        "\n",
        "        # Generate sample input\n",
        "        batch_size = 2\n",
        "        sample_input = np.random.random((batch_size, 3, 4))\n",
        "\n",
        "        # Get the output\n",
        "        output = model.predict(sample_input)\n",
        "\n",
        "        # Verify output shape\n",
        "        self.assertEqual(output.shape, (batch_size, 3, 5))\n",
        "\n",
        "        # Manually apply the dense layer to each time step and verify results\n",
        "        manual_output = np.zeros((batch_size, 3, 5))\n",
        "        for b in range(batch_size):\n",
        "            for t in range(3):\n",
        "                manual_output[b, t] = dense_layer(np.expand_dims(sample_input[b, t], 0)).numpy()\n",
        "\n",
        "        # Check outputs are the same\n",
        "        np.testing.assert_allclose(output, manual_output, rtol=1e-5)\n",
        "\n",
        "    def test_shape_handling(self):\n",
        "        \"\"\"Test that the layer correctly handles different input shapes.\"\"\"\n",
        "        # Test with different input shapes\n",
        "        shapes = [\n",
        "            (3, 5),      # (time, features)\n",
        "            (3, 5, 6),   # (time, height, width)\n",
        "            (3, 5, 6, 2) # (time, height, width, channels)\n",
        "        ]\n",
        "\n",
        "        for shape in shapes:\n",
        "            # Create a model with SimpleTimeDistributed\n",
        "            inputs = keras.Input(shape=shape)\n",
        "\n",
        "            # Choose appropriate layer based on input dimensions\n",
        "            if len(shape) == 2:\n",
        "                layer = keras.layers.Dense(4)\n",
        "                expected_output_shape = (None, shape[0], 4)\n",
        "            elif len(shape) == 3:\n",
        "                layer = keras.layers.Conv1D(4, 3, padding='same')\n",
        "                expected_output_shape = (None, shape[0], shape[1], 4)\n",
        "            else:\n",
        "                layer = keras.layers.Conv2D(4, 3, padding='same')\n",
        "                expected_output_shape = (None, shape[0], shape[1], shape[2], 4)\n",
        "\n",
        "            x = SimpleTimeDistributed(layer)(inputs)\n",
        "            model = keras.Model(inputs, x)\n",
        "\n",
        "            # Verify output shape\n",
        "            self.assertEqual(model.output_shape, expected_output_shape)\n",
        "\n",
        "    def test_complex_layer(self):\n",
        "        \"\"\"Test with a more complex layer like Conv2D.\"\"\"\n",
        "        # Create a model with SimpleTimeDistributed wrapping Conv2D\n",
        "        inputs = keras.Input(shape=(10, 28, 28, 3))  # (batch, time, height, width, channels)\n",
        "        conv_layer = keras.layers.Conv2D(16, 3, padding='same', activation='relu')\n",
        "        x = SimpleTimeDistributed(conv_layer)(inputs)\n",
        "        model = keras.Model(inputs, x)\n",
        "\n",
        "        # Generate sample input\n",
        "        batch_size = 2\n",
        "        sample_input = np.random.random((batch_size, 10, 28, 28, 3))\n",
        "\n",
        "        # Get the output\n",
        "        output = model.predict(sample_input)\n",
        "\n",
        "        # Verify output shape\n",
        "        self.assertEqual(output.shape, (batch_size, 10, 28, 28, 16))\n",
        "\n",
        "        # Manually apply the conv layer to each time step and verify results\n",
        "        manual_output = np.zeros((batch_size, 10, 28, 28, 16))\n",
        "        for b in range(batch_size):\n",
        "            for t in range(10):\n",
        "                manual_output[b, t] = conv_layer(np.expand_dims(sample_input[b, t], 0)).numpy()\n",
        "\n",
        "        # Check outputs are the same\n",
        "        np.testing.assert_allclose(output, manual_output, rtol=1e-5)\n",
        "\n",
        "    def test_training_mode(self):\n",
        "        \"\"\"Test that training mode is correctly passed to the wrapped layer.\"\"\"\n",
        "        # Create a dropout layer which behaves differently in training vs. inference\n",
        "        dropout_layer = keras.layers.Dropout(0.5)\n",
        "\n",
        "        # Create a model with SimpleTimeDistributed\n",
        "        inputs = keras.Input(shape=(5, 10))\n",
        "        x = SimpleTimeDistributed(dropout_layer)(inputs)\n",
        "        model = keras.Model(inputs, x)\n",
        "\n",
        "        # Generate sample input\n",
        "        sample_input = np.ones((1, 5, 10))\n",
        "\n",
        "        # Output during training (with dropout applied) should not be equal to the input\n",
        "        output_training = model(sample_input, training=True).numpy()\n",
        "        # Since dropout is random, we can't guarantee outputs will differ, but it's highly likely\n",
        "        # Instead of directly comparing, we'll check if any element was dropped (set to 0)\n",
        "        self.assertTrue(np.any(output_training == 0))\n",
        "\n",
        "        # Output during inference (no dropout) should be equal to the input\n",
        "        output_inference = model(sample_input, training=False).numpy()\n",
        "        np.testing.assert_array_equal(output_inference, sample_input)\n",
        "\n",
        "    def test_error_handling(self):\n",
        "        \"\"\"Test that appropriate errors are raised for invalid inputs.\"\"\"\n",
        "        # Test with invalid input shape (less than 3 dimensions)\n",
        "        with self.assertRaises(ValueError):\n",
        "            layer = SimpleTimeDistributed(keras.layers.Dense(10))\n",
        "            layer.build((None, 10))  # Missing time dimension\n",
        "\n",
        "    def test_variable_time_steps(self):\n",
        "        \"\"\"Test that the layer correctly handles variable time steps.\"\"\"\n",
        "        # Create a model with variable time steps\n",
        "        inputs = keras.Input(shape=(None, 10))  # Variable time steps\n",
        "        x = SimpleTimeDistributed(keras.layers.Dense(5))(inputs)\n",
        "        model = keras.Model(inputs, x)\n",
        "\n",
        "        # Test with different sequence lengths\n",
        "        for seq_len in [3, 5, 7]:\n",
        "            sample_input = np.random.random((2, seq_len, 10))\n",
        "            output = model.predict(sample_input)\n",
        "            self.assertEqual(output.shape, (2, seq_len, 5))\n",
        "\n",
        "    def test_multiple_layer_stacking(self):\n",
        "        \"\"\"Test that the layer works when stacked with other layers.\"\"\"\n",
        "        # Create a model with multiple SimpleTimeDistributed layers\n",
        "        inputs = keras.Input(shape=(10, 20))\n",
        "        x = SimpleTimeDistributed(keras.layers.Dense(15))(inputs)\n",
        "        x = SimpleTimeDistributed(keras.layers.Dense(10))(x)\n",
        "        x = SimpleTimeDistributed(keras.layers.Dense(5))(x)\n",
        "        model = keras.Model(inputs, x)\n",
        "\n",
        "        # Generate sample input\n",
        "        sample_input = np.random.random((2, 10, 20))\n",
        "\n",
        "        # Get the output\n",
        "        output = model.predict(sample_input)\n",
        "\n",
        "        # Verify output shape\n",
        "        self.assertEqual(output.shape, (2, 10, 5))\n",
        "\n",
        "    def test_masking_support_inheritance(self):\n",
        "        \"\"\"Test that SimpleTimeDistributed correctly inherits masking support.\"\"\"\n",
        "        # Create layers with and without masking support\n",
        "        lstm_layer = keras.layers.LSTM(10, return_sequences=True)  # Supports masking\n",
        "        dense_layer = keras.layers.Dense(10)  # Does not support masking by default\n",
        "\n",
        "        # Check that SimpleTimeDistributed inherits masking support correctly\n",
        "        time_distributed_lstm = SimpleTimeDistributed(lstm_layer)\n",
        "        time_distributed_dense = SimpleTimeDistributed(dense_layer)\n",
        "\n",
        "        self.assertTrue(time_distributed_lstm.supports_masking)\n",
        "        self.assertFalse(time_distributed_dense.supports_masking)\n",
        "\n",
        "    def test_comparison_with_keras_time_distributed(self):\n",
        "        \"\"\"Compare outputs with Keras's built-in TimeDistributed.\"\"\"\n",
        "        # Create a dense layer with fixed weights for deterministic comparison\n",
        "        dense_layer = keras.layers.Dense(10,\n",
        "                                        kernel_initializer='ones',\n",
        "                                        bias_initializer='zeros',\n",
        "                                        use_bias=True)\n",
        "\n",
        "        # Create a model with SimpleTimeDistributed\n",
        "        inputs1 = keras.Input(shape=(5, 8))\n",
        "        x1 = SimpleTimeDistributed(dense_layer)(inputs1)\n",
        "        model1 = keras.Model(inputs1, x1)\n",
        "\n",
        "        # Create a model with Keras TimeDistributed\n",
        "        # Here we'll need to clone the dense layer to avoid weight sharing\n",
        "        dense_layer_clone = keras.layers.Dense(10,\n",
        "                                             kernel_initializer='ones',\n",
        "                                             bias_initializer='zeros',\n",
        "                                             use_bias=True)\n",
        "        inputs2 = keras.Input(shape=(5, 8))\n",
        "        x2 = keras.layers.TimeDistributed(dense_layer_clone)(inputs2)\n",
        "        model2 = keras.Model(inputs2, x2)\n",
        "\n",
        "        # Ensure weights are the same\n",
        "        dense_layer_clone.set_weights(dense_layer.get_weights())\n",
        "\n",
        "        # Generate sample input\n",
        "        sample_input = np.random.random((2, 5, 8))\n",
        "\n",
        "        # Get outputs\n",
        "        output1 = model1.predict(sample_input)\n",
        "        output2 = model2.predict(sample_input)\n",
        "\n",
        "        # Check outputs are the same\n",
        "        np.testing.assert_allclose(output1, output2, rtol=1e-5)\n",
        "\n",
        "    def test_weights_training(self):\n",
        "        \"\"\"Test that the weights of the wrapped layer are updated correctly during training.\"\"\"\n",
        "        # Create a dense layer with specific initializers for easy tracking\n",
        "        dense_layer = keras.layers.Dense(1,\n",
        "                                        kernel_initializer='ones',\n",
        "                                        bias_initializer='zeros')\n",
        "\n",
        "        # Create a model with SimpleTimeDistributed\n",
        "        inputs = keras.Input(shape=(3, 2))\n",
        "        x = SimpleTimeDistributed(dense_layer)(inputs)\n",
        "        model = keras.Model(inputs, x)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer='sgd', loss='mse')\n",
        "\n",
        "        # Get initial weights\n",
        "        initial_weights = [w.copy() for w in dense_layer.get_weights()]\n",
        "\n",
        "        # Generate some training data\n",
        "        x_train = np.ones((10, 3, 2))\n",
        "        y_train = np.zeros((10, 3, 1))  # Target: opposite of what initial weights would produce\n",
        "\n",
        "        # Train the model for a few epochs\n",
        "        model.fit(x_train, y_train, epochs=5, verbose=0)\n",
        "\n",
        "        # Get updated weights\n",
        "        updated_weights = dense_layer.get_weights()\n",
        "\n",
        "        # Check that weights have been updated\n",
        "        for i in range(len(initial_weights)):\n",
        "            self.assertFalse(np.array_equal(initial_weights[i], updated_weights[i]))"
      ],
      "metadata": {
        "id": "P008CZSgyTqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_lVtTpHyZyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFQ-O5idYhS3"
      },
      "source": [
        "## Create augmentation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Xkt4gQYhS3"
      },
      "outputs": [],
      "source": [
        "augmentation_layers = [\n",
        "    #keras.layers.RandomBrightness(factor=0.1),\n",
        "    #keras.layers.RandomContrast(factor=0.1),\n",
        "    keras.layers.RandomRotation(factor=.05, fill_mode='constant', fill_value=0),\n",
        "    #keras.layers.RandomTranslation(height_factor=0.05, width_factor=0.05)\n",
        "]\n",
        "\n",
        "augmentation_model = keras.Sequential(augmentation_layers)\n",
        "\n",
        "@tf.function\n",
        "def apply_consistent_augmentations(inputs, augmentation_model):\n",
        "    \"\"\"Apply consistent augmentations to calibration and target images.\n",
        "\n",
        "    Args:\n",
        "        inputs: Tuple of (cal_imgs, cal_coords, cal_mask, target_imgs, target_mask)\n",
        "        augmentation_fn: Function created by create_augmentation_fn\n",
        "\n",
        "    Returns:\n",
        "        Tuple with augmented images in the same structure\n",
        "    \"\"\"\n",
        "    cal_imgs, cal_coords, cal_mask, target_imgs, target_mask = inputs\n",
        "\n",
        "    merged_cal_images = tf.transpose(cal_imgs, perm=[3, 1, 2, 0])\n",
        "    merged_target_images = tf.transpose(target_imgs, perm=[3, 1, 2, 0])\n",
        "\n",
        "    n_cal_images = MAX_CAL_POINTS\n",
        "    n_target_images = MAX_TARGETS\n",
        "\n",
        "    merged_all_images = tf.concat([merged_cal_images, merged_target_images], axis=-1)\n",
        "\n",
        "    merged_all_images_aug = augmentation_model(merged_all_images)\n",
        "\n",
        "    merged_cal_imgs_aug = merged_all_images_aug[..., :n_cal_images]\n",
        "    merged_target_imgs_aug = merged_all_images_aug[..., n_cal_images:]\n",
        "\n",
        "    cal_imgs_aug = tf.transpose(merged_cal_imgs_aug, perm=[3, 1, 2, 0])\n",
        "    target_imgs_aug = tf.transpose(merged_target_imgs_aug, perm=[3, 1, 2, 0])\n",
        "\n",
        "    return cal_imgs_aug, cal_coords, cal_mask, target_imgs_aug, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz_L2j0duJDw"
      },
      "outputs": [],
      "source": [
        "t0 = train_data_rescaled.cache()\n",
        "\n",
        "t1 = t0.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function,\n",
        "    window_size = 200\n",
        ")\n",
        "\n",
        "t2 = t0.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ")\n",
        "\n",
        "# Apply to dataset - ensure consistent shapes\n",
        "train_ds = t1.map(\n",
        "    lambda x, y: (apply_consistent_augmentations(x, augmentation_model), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Update the batching step to use a custom padded_batch function\n",
        "def prepare_batch_for_model(features, labels):\n",
        "    \"\"\"Ensure inputs are correctly formatted for the model\"\"\"\n",
        "    cal_imgs, cal_coords, cal_mask, target_imgs, target_mask = features\n",
        "\n",
        "    # Prepare inputs as a dictionary matching the model's expected inputs\n",
        "    inputs = {\n",
        "        \"Input_Calibration_Eyes\": cal_imgs,\n",
        "        \"Input_Calibration_Points\": cal_coords,\n",
        "        \"Input_Calibration_Mask\": cal_mask,\n",
        "        \"Input_Target_Eyes\": target_imgs,\n",
        "        \"Input_Target_Mask\": target_mask\n",
        "    }\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "# Use the updated version with proper input preparation\n",
        "train_ds_for_model = train_ds.map(prepare_batch_for_model).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.element_spec"
      ],
      "metadata": {
        "id": "RKeCKJ8jliC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORk_yuzTYhS3"
      },
      "source": [
        "## Visualize augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeTXbm3mYhS3"
      },
      "outputs": [],
      "source": [
        "def visualize_augmentations(dataset, num_examples=5):\n",
        "    \"\"\"Visualize augmentations from a tf.data.Dataset.\"\"\"\n",
        "\n",
        "    # Get a sample element from the dataset\n",
        "    for element in dataset.take(1):\n",
        "        # Access the calibration images from the element\n",
        "        images = element[0][0]  # Access all calibration images\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    num_images = images.shape[0]  # Get the number of images in the batch\n",
        "    grid_cols = 5  # Number of columns in the grid\n",
        "    grid_rows = (num_images + grid_cols - 1) // grid_cols  # Calculate number of rows\n",
        "\n",
        "    # Create a figure to display results\n",
        "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(15, 3 * grid_rows))  # Create subplots in a grid\n",
        "\n",
        "    # Display all images\n",
        "    for i in range(num_images):\n",
        "        row = i // grid_cols\n",
        "        col = i % grid_cols\n",
        "        axes[row, col].imshow(images[i].numpy().squeeze(), cmap='gray')  # Display image\n",
        "        axes[row, col].set_title(f\"Image {i + 1}\")  # Set title for each subplot\n",
        "        axes[row, col].axis('off')  # Turn off axis\n",
        "\n",
        "    # Hide empty subplots if any\n",
        "    for i in range(num_images, grid_rows * grid_cols):\n",
        "        row = i // grid_cols\n",
        "        col = i % grid_cols\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Use this to check your augmentations\n",
        "visualize_augmentations(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19yxxx_Ts8H"
      },
      "source": [
        "# Construct Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuZobSgkgbW"
      },
      "source": [
        "## Eye image processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62VuVwq6lvd"
      },
      "source": [
        "## Regression Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGk0aRz_ebqk"
      },
      "outputs": [],
      "source": [
        "class WeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    def __init__(self, lambda_ridge, embedding_dim, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        self.embedding_dim = embedding_dim  # Required static embedding dimension\n",
        "        super(WeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "        # Create identity matrix constant during initialization\n",
        "        # This ensures it's created only once and is available immediately\n",
        "        self._identity = tf.eye(embedding_dim, dtype=tf.float32)\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, inputs):\n",
        "        unknown_embeddings, calibration_embeddings, calibration_coords, weights, cal_mask, target_mask = inputs\n",
        "\n",
        "        # Explicitly set shapes where possible (assuming batch_size can remain dynamic)\n",
        "        batch_size = tf.shape(calibration_embeddings)[0]\n",
        "\n",
        "        # Force shapes to match the static embedding dimension\n",
        "        calibration_embeddings = tf.ensure_shape(\n",
        "            calibration_embeddings, [None, None, self.embedding_dim]\n",
        "        )\n",
        "        unknown_embeddings = tf.ensure_shape(\n",
        "            unknown_embeddings, [None, None, self.embedding_dim]\n",
        "        )\n",
        "\n",
        "        # Cast all inputs to the same precision\n",
        "        X = tf.cast(calibration_embeddings, tf.float32)  # (batch_size, n_calibration, embedding_dim)\n",
        "        y = tf.cast(calibration_coords, tf.float32)      # (batch_size, n_calibration, 2)\n",
        "        w = tf.cast(weights, tf.float32)                 # (batch_size, n_calibration)\n",
        "        cal_mask = tf.cast(cal_mask, tf.float32)         # (batch_size, n_calibration)\n",
        "        target_mask = tf.cast(target_mask, tf.float32)   # (batch_size, n_target)\n",
        "\n",
        "        # Ensure shapes are consistent\n",
        "        n_calibration = tf.shape(X)[1]\n",
        "        n_target = tf.shape(unknown_embeddings)[1]\n",
        "\n",
        "        # Broadcast the identity matrix to batch size\n",
        "        I = tf.broadcast_to(self._identity, [batch_size, self.embedding_dim, self.embedding_dim])\n",
        "\n",
        "        # Apply mask to weights - ensure this is broadcastable\n",
        "        w = w * cal_mask\n",
        "\n",
        "        # Apply weights to X and y with explicit reshaping\n",
        "        w_sqrt = tf.sqrt(w)\n",
        "        w_sqrt = tf.reshape(w_sqrt, [batch_size, n_calibration, 1])  # Reshape for broadcasting\n",
        "\n",
        "        X_weighted = X * w_sqrt  # (batch_size, n_calibration, embedding_dim)\n",
        "        y_weighted = y * w_sqrt  # (batch_size, n_calibration, 2)\n",
        "\n",
        "        # Matrix multiplication with explicit transpose\n",
        "        X_t = tf.transpose(X_weighted, perm=[0, 2, 1])  # (batch_size, embedding_dim, n_calibration)\n",
        "        X_t_X = tf.matmul(X_t, X_weighted)  # (batch_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Add regularization with stable epsilon\n",
        "        ridge_term = tf.multiply(I, self.lambda_ridge + self.epsilon)\n",
        "        lhs = tf.add(X_t_X, ridge_term)  # (batch_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Compute right-hand side\n",
        "        rhs = tf.matmul(X_t, y_weighted)  # (batch_size, embedding_dim, 2)\n",
        "\n",
        "        # Solve linear system\n",
        "        # Add stability check for better XLA compatibility\n",
        "        is_singular = tf.math.reduce_any(tf.math.is_nan(lhs))\n",
        "        kernel = tf.cond(\n",
        "            is_singular,\n",
        "            lambda: tf.zeros([batch_size, self.embedding_dim, 2], dtype=tf.float32),\n",
        "            lambda: tf.linalg.solve(lhs, rhs)\n",
        "        )  # Shape: [batch_size, embedding_dim, 2]\n",
        "\n",
        "        # Apply regression to unknown point with explicit masking\n",
        "        unknown_embeddings = tf.cast(unknown_embeddings, tf.float32)  # Shape: [batch_size, n_target, embedding_dim]\n",
        "        target_mask_expanded = tf.reshape(target_mask, [batch_size, n_target, 1])\n",
        "        unknown_embeddings_masked = unknown_embeddings * target_mask_expanded\n",
        "\n",
        "        # Matrix multiplication with proper dimensions:\n",
        "        # unknown_embeddings_masked: [batch_size, n_target, embedding_dim]\n",
        "        # kernel:                    [batch_size, embedding_dim, 2]\n",
        "        # Result:                    [batch_size, n_target, 2]\n",
        "        output = tf.matmul(unknown_embeddings_masked, kernel)  # Shape: [batch_size, n_target, 2]\n",
        "\n",
        "        # Apply final mask\n",
        "        output = output * target_mask_expanded\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        unknown_embeddings_shape, _, _, _, _, _ = input_shapes\n",
        "        return (unknown_embeddings_shape[0], unknown_embeddings_shape[1], 2)\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        # This method is called once before the first call() to build the layer\n",
        "        # We already handle initialization in __init__, but this is a good place to validate shapes\n",
        "        unknown_shape, cal_shape, coords_shape, weights_shape, cal_mask_shape, target_mask_shape = input_shapes\n",
        "\n",
        "        if unknown_shape[-1] != self.embedding_dim:\n",
        "            raise ValueError(f\"Unknown embeddings dimension {unknown_shape[-1]} doesn't match specified embedding_dim {self.embedding_dim}\")\n",
        "\n",
        "        if cal_shape[-1] != self.embedding_dim:\n",
        "            raise ValueError(f\"Calibration embeddings dimension {cal_shape[-1]} doesn't match specified embedding_dim {self.embedding_dim}\")\n",
        "\n",
        "        # We don't need to create any weights in build() since we don't have trainable parameters\n",
        "        super(WeightedRidgeRegressionLayer, self).build(input_shapes)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(WeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"lambda_ridge\": self.lambda_ridge,\n",
        "            \"epsilon\": self.epsilon,\n",
        "            \"embedding_dim\": self.embedding_dim\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JOA5GNqTUW-"
      },
      "outputs": [],
      "source": [
        "def create_full_model():\n",
        "\n",
        "  embedding_model = create_embedding_model()\n",
        "\n",
        "  input_calibration_eyes = keras.layers.Input(shape=(MAX_CAL_POINTS, 36, 144, 1), name=\"Input_Calibration_Eyes\")\n",
        "  input_calibration_points = keras.layers.Input(shape=(MAX_CAL_POINTS, 2), name=\"Input_Calibration_Points\")\n",
        "  input_calibration_mask = keras.layers.Input(shape=(MAX_CAL_POINTS,), name=\"Input_Calibration_Mask\")\n",
        "\n",
        "  input_target_eyes = keras.layers.Input(shape=(MAX_TARGETS, 36, 144, 1), name=\"Input_Target_Eyes\")\n",
        "  input_target_mask = keras.layers.Input(shape=(MAX_TARGETS,), name=\"Input_Target_Mask\")\n",
        "\n",
        "  # Apply the embedding model using masking\n",
        "  target_embedding = keras.layers.TimeDistributed(embedding_model)(input_target_eyes)\n",
        "  calibration_embeddings = keras.layers.TimeDistributed(embedding_model)(input_calibration_eyes)\n",
        "\n",
        "  calibration_weights = keras.layers.Dense(1, activation=\"sigmoid\", name=\"Calibration_Weights\")(calibration_embeddings)\n",
        "  calibration_weights_reshaped = keras.layers.Reshape((-1,), name=\"Calibration_Weights_Reshaped\")(calibration_weights)\n",
        "\n",
        "  ridge = WeightedRidgeRegressionLayer(RIDGE_REGULARIZATION, embedding_dim=EMBEDDING_DIM)([\n",
        "    target_embedding,\n",
        "    calibration_embeddings,\n",
        "    input_calibration_points,\n",
        "    calibration_weights_reshaped,\n",
        "    input_calibration_mask,\n",
        "    input_target_mask\n",
        "  ])\n",
        "\n",
        "  full_model = keras.Model(inputs=[\n",
        "    input_calibration_eyes,\n",
        "    input_calibration_points,\n",
        "    input_calibration_mask,\n",
        "    input_target_eyes,\n",
        "    input_target_mask\n",
        "  ], outputs=ridge, name=\"FullEyePredictionModel\")\n",
        "\n",
        "  return full_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtm4EUSZwXDC"
      },
      "source": [
        "## Full trainable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xAFsV9x41BZ"
      },
      "outputs": [],
      "source": [
        "full_model = create_full_model()\n",
        "\n",
        "full_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "full_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=masked_normalized_weighted_euc_dist,\n",
        "    metrics=[normalized_weighted_euc_dist],  # Keep the original for comparison if needed\n",
        "    jit_compile=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FnXA0JueNHm"
      },
      "outputs": [],
      "source": [
        "def lr_schedule(epoch):\n",
        "  if epoch < 15:\n",
        "    return LEARNING_RATE\n",
        "  else:\n",
        "    return LEARNING_RATE * 0.1\n",
        "\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzxl5IX0ycoR"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t15bFcCV4_yw"
      },
      "outputs": [],
      "source": [
        "train_history = full_model.fit(\n",
        "    train_ds_for_model.batch(4),\n",
        "    epochs=TRAIN_EPOCHS,\n",
        "    callbacks=[WandbMetricsLogger(), lr_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVk3s8vQRFpl"
      },
      "outputs": [],
      "source": [
        "full_model.save('full_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9SocMbzC5p"
      },
      "outputs": [],
      "source": [
        "full_model.save_weights('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZy4iASiRJvi"
      },
      "outputs": [],
      "source": [
        "wandb.save('full_model.keras')\n",
        "wandb.save('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_OUDhetR5LdO"
      },
      "outputs": [],
      "source": [
        "# Update the prediction code\n",
        "t2_processed = t2.map(\n",
        "    lambda x, y, id: (prepare_batch_for_model((x[0], x[1], x[2], x[3], x[4]), y)[0], y, id)\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Initialize a list to store the batch losses\n",
        "batch_losses = []\n",
        "subject_ids = []\n",
        "y_true = []\n",
        "\n",
        "for e in t2_processed.batch(1).as_numpy_iterator():\n",
        "    inputs = e[0]\n",
        "    y_true.append(e[1])\n",
        "    subject_ids.append(e[2][0])\n",
        "\n",
        "y_true = np.array(y_true).reshape(-1, 144, 2)\n",
        "\n",
        "# this step is slower than expected. not sure what's going on.\n",
        "predictions = full_model.predict(t2_processed.batch(1))\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
        "  batch_losses.append(loss)\n",
        "\n",
        "batch_losses = np.array(batch_losses)\n",
        "\n",
        "# Get mean per subject\n",
        "batch_losses = np.mean(batch_losses, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FdB2Anc88Qz"
      },
      "outputs": [],
      "source": [
        "final_loss_table = wandb.Table(data=[[s, l] for (s, l) in zip(subject_ids, batch_losses)], columns=[\"subject\", \"scores\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvKzpmrI9PBO"
      },
      "outputs": [],
      "source": [
        "final_loss_hist = wandb.plot.histogram(final_loss_table, \"scores\", title=\"Normalized Euclidean Distance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9iK_69wdrZ0"
      },
      "outputs": [],
      "source": [
        "final_loss_mean = np.mean(batch_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk8z0iV4DV2d"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"final_val_loss_table\": final_loss_table, \"final_val_loss_hist\": final_loss_hist, \"final_loss_mean\": final_loss_mean})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blgWDH3E5Swd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(batch_losses, bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Batch Losses')\n",
        "plt.xlabel('Loss')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXAdVnFKMjg7"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}