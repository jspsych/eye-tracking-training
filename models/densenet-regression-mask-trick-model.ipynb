{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9LAJHmSmF84"
   },
   "source": [
    "# Setup"
   ],
   "id": "b2215d98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW4fMCshWBrD"
   },
   "outputs": [],
   "source": [
    "!pip install osfclient --quiet\n",
    "!pip install git+https://github.com/jspsych/eyetracking-utils.git --quiet\n",
    "!pip install wandb --quiet\n",
    "!pip install --upgrade keras --quiet\n",
    "!pip install keras-hub --quiet"
   ],
   "id": "0786ca21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQRQZ3D5Vwqc"
   },
   "outputs": [],
   "source": "# ============================================================================\n# Backend Configuration - Must be set before importing Keras\n# ============================================================================\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # JAX backend for TPU compatibility\n\n# ============================================================================\n# Core ML/Deep Learning Libraries\n# ============================================================================\nimport tensorflow as tf  # Used for data pipeline (tf.data, tf.io)\nimport numpy as np\nimport math\nimport keras\nimport keras_hub\nfrom keras import ops  # Backend-agnostic operations\n\n# ============================================================================\n# Experiment Tracking\n# ============================================================================\nimport wandb\nfrom wandb.integration.keras import WandbMetricsLogger\n\n# ============================================================================\n# Visualization\n# ============================================================================\nimport matplotlib.pyplot as plt\n\n# ============================================================================\n# Google Colab Utilities\n# ============================================================================\nfrom google.colab import userdata\n\n# ============================================================================\n# Project-Specific Utilities\n# ============================================================================\nimport et_util.dataset_utils as dataset_utils\nimport et_util.embedding_preprocessing as embed_pre\nimport et_util.model_layers as model_layers\nfrom et_util import experiment_utils\nfrom et_util.custom_loss import normalized_weighted_euc_dist\nfrom et_util.model_analysis import plot_model_performance",
   "id": "676c2ba3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BUzHdPEc-ef"
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
    "os.environ['OSF_USERNAME'] = userdata.get('osfusername')"
   ],
   "id": "be3997eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKOfeTxzYAKy"
   },
   "outputs": [],
   "source": [
    "keras.version()"
   ],
   "id": "52c090cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3M9X95jV8du"
   },
   "outputs": [],
   "source": "keras.mixed_precision.set_global_policy('bfloat16')",
   "id": "1fd0ae30"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er9v9bxBcHc0"
   },
   "source": [
    "# Configure W&B experiment"
   ],
   "id": "5fd907c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SPGBdleCfbe"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ],
   "id": "7e4a274f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWWKsFf6p1HJ"
   },
   "outputs": [],
   "source": "# =============================================================================\n# EXPERIMENT CONFIGURATION\n# =============================================================================\n\n# -----------------------------------------------------------------------------\n# Dataset Parameters\n# -----------------------------------------------------------------------------\nMAX_TARGETS = 144  # Maximum images per subject (for padding)\n\n# -----------------------------------------------------------------------------\n# Model Architecture\n# -----------------------------------------------------------------------------\nEMBEDDING_DIM = 200  # Size of embedding vector\nRIDGE_REGULARIZATION = 0.1  # Ridge regression lambda\nMIN_CAL_POINTS = 8  # Minimum calibration images\nMAX_CAL_POINTS = 40  # Maximum calibration images\n\nBACKBONE = \"densenet\"  # Options: \"densenet\", \"vit\"\n\n# -----------------------------------------------------------------------------\n# DenseNet Configuration (when BACKBONE=\"densenet\")\n# -----------------------------------------------------------------------------\nDENSENET_STACKWISE_NUM_REPEATS = [4, 4, 4]  # Number of dense blocks per stack\n                                             # [4,4,4] = 3 dense blocks with 4 layers each\n\n# -----------------------------------------------------------------------------\n# Vision Transformer Configuration (when BACKBONE=\"vit\")\n# -----------------------------------------------------------------------------\nVIT_PATCH_SIZE = 4  # Patch size (4x4 → 324 patches from 36×144 image)\nVIT_HIDDEN_DIM = 256  # Hidden dimension (TPU-friendly)\nVIT_NUM_LAYERS = 6  # Number of transformer layers\nVIT_NUM_HEADS = 8  # Number of attention heads\nVIT_MLP_DIM = 1024  # MLP dimension in transformer blocks\nVIT_DROPOUT = 0.1  # Dropout rate\nVIT_ATTENTION_DROPOUT = 0.0  # Attention dropout rate\nVIT_USE_CLS_TOKEN = True  # Use CLS token for global representation\n\n# -----------------------------------------------------------------------------\n# Data Augmentation\n# -----------------------------------------------------------------------------\nAUGMENTATION = True  # Enable/disable augmentation\n\n# Augmentation strategy\nPER_IMAGE_AUGMENTATION = True  # If True, each image gets random augmentation\n                               # If False, same augmentation per sequence\n\n# Blur augmentation\nBLUR_PROB = 0.3  # Probability of applying blur\nBLUR_SIGMA_RANGE = (0.5, 1.5)  # Gaussian blur sigma range\n\n# Color augmentations\nBRIGHTNESS_RANGE = 0.3  # Brightness adjustment range\nCONTRAST_RANGE = (0.7, 1.4)  # Contrast adjustment range\nGAMMA_RANGE = (0.8, 1.2)  # Gamma correction range\n\n# Noise augmentation\nNOISE_PROB = 0.4  # Probability of adding noise\nNOISE_STD_RANGE = (0.01, 0.05)  # Noise standard deviation range\n\n# -----------------------------------------------------------------------------\n# Training Hyperparameters\n# -----------------------------------------------------------------------------\nBATCH_SIZE = 5  # Number of subjects per batch\nTRAIN_EPOCHS = 50  # Total training epochs\n\n# Learning rate schedule\nINITIAL_LEARNING_RATE = 0.00001  # Starting learning rate (warmup)\nLEARNING_RATE = 0.01  # Target learning rate (after warmup)\nWARMUP_EPOCHS = 2  # Number of warmup epochs\nDECAY_EPOCHS = TRAIN_EPOCHS - WARMUP_EPOCHS  # Cosine decay epochs\nDECAY_ALPHA = 0.01  # Final learning rate multiplier",
   "id": "a79af7ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkHFv5kHfaWR"
   },
   "outputs": [],
   "source": "config = {\n    \"embedding_dim\": EMBEDDING_DIM,\n    \"ridge_regularization\": RIDGE_REGULARIZATION,\n    \"train_epochs\": TRAIN_EPOCHS,\n    \"min_cal_points\": MIN_CAL_POINTS,\n    \"max_cal_points\": MAX_CAL_POINTS,\n    \"backbone\": BACKBONE,\n    \"batch_size\": BATCH_SIZE,\n    \"initial_learning_rate\": INITIAL_LEARNING_RATE,\n    \"learning_rate\": LEARNING_RATE,\n    \"augmentation\": AUGMENTATION,\n    \"per_image_augmentation\": PER_IMAGE_AUGMENTATION,\n    \"blur_prob\": BLUR_PROB,\n    \"blur_sigma_range\": BLUR_SIGMA_RANGE,\n    \"brightness_range\": BRIGHTNESS_RANGE,\n    \"contrast_range\": CONTRAST_RANGE,\n    \"gamma_range\": GAMMA_RANGE,\n    \"noise_prob\": NOISE_PROB,\n    \"noise_std_range\": NOISE_STD_RANGE,\n    \"warmup_epochs\": WARMUP_EPOCHS,\n    \"decay_epochs\": DECAY_EPOCHS,\n    \"decay_alpha\": DECAY_ALPHA,\n    \n    # Add DenseNet-specific config when using DenseNet\n    **({\"densenet_stackwise_num_repeats\": DENSENET_STACKWISE_NUM_REPEATS}\n      if BACKBONE == \"densenet\" else {}),\n    \n    # Add ViT-specific config when using ViT\n    **({\"vit_patch_size\": VIT_PATCH_SIZE,\n        \"vit_hidden_dim\": VIT_HIDDEN_DIM,\n        \"vit_num_layers\": VIT_NUM_LAYERS,\n        \"vit_num_heads\": VIT_NUM_HEADS,\n        \"vit_mlp_dim\": VIT_MLP_DIM,\n        \"vit_dropout\": VIT_DROPOUT,\n        \"vit_attention_dropout\": VIT_ATTENTION_DROPOUT,\n        \"vit_use_cls_token\": VIT_USE_CLS_TOKEN}\n      if BACKBONE == \"vit\" else {})\n}",
   "id": "c60ff2b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjDFWXNEddE9"
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='eye-tracking-dense-full-data-set-single-eye',\n",
    "    config=config\n",
    ")"
   ],
   "id": "445fd85a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERTCx1sP0XGv"
   },
   "source": [
    "# Dataset preparation"
   ],
   "id": "df7d0f52"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKeHGiFxtvo0"
   },
   "source": [
    "## Download dataset from OSF"
   ],
   "id": "bf9b4823"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tswV6s38WbtO"
   },
   "outputs": [],
   "source": [
    "!osf -p 6b5cd fetch single_eye_tfrecords.tar.gz"
   ],
   "id": "27df7f04"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFgWOhWTt4iD"
   },
   "source": [
    "## Process raw data records into TF Dataset"
   ],
   "id": "c62720bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4h8QhGvWhZf"
   },
   "outputs": [],
   "source": [
    "!mkdir single_eye_tfrecords\n",
    "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
   ],
   "id": "da7b74e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZRcQgPadqtI"
   },
   "outputs": [],
   "source": "def parse(element):\n    \"\"\"Process function that parses a tfr element in a raw dataset for process_tfr_to_tfds function.\n    Gets mediapipe landmarks, raw image, image width, image height, subject id, and xy labels.\n    Use for data generated with make_single_example_landmarks_and_jpg (i.e. data in\n    jpg_landmarks_tfrecords.tar.gz)\n\n    :param element: tfr element in raw dataset\n    :return: image, label(x,y), landmarks, subject_id\n    \"\"\"\n\n    data_structure = {\n        'landmarks': tf.io.FixedLenFeature([], tf.string),\n        'img_width': tf.io.FixedLenFeature([], tf.int64),\n        'img_height': tf.io.FixedLenFeature([], tf.int64),\n        'x': tf.io.FixedLenFeature([], tf.float32),\n        'y': tf.io.FixedLenFeature([], tf.float32),\n        'eye_img': tf.io.FixedLenFeature([], tf.string),\n        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n    }\n\n    content = tf.io.parse_single_example(element, data_structure)\n\n    landmarks = content['landmarks']\n    raw_image = content['eye_img']\n    width = content['img_width']\n    height = content['img_height']\n    depth = 3\n    label = [content['x'], content['y']]\n    subject_id = content['subject_id']\n\n    landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n    landmarks = ops.reshape(landmarks, shape=(478, 3))\n\n    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n\n    return image, landmarks, label, subject_id",
   "id": "2599da1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWv57eb5XHVP"
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = dataset_utils.process_tfr_to_tfds(\n",
    "    'single_eye_tfrecords/',\n",
    "    parse,\n",
    "    train_split=1.0,\n",
    "    val_split=0.0,\n",
    "    test_split=0.0,\n",
    "    random_seed=12604,\n",
    "    group_function=lambda img, landmarks, coords, z: z\n",
    ")"
   ],
   "id": "a6568fd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apcLK9klHLQV"
   },
   "outputs": [],
   "source": "def rescale_coords_map(eyes, mesh, coords, id):\n  return eyes, mesh, coords / 100.0, id",
   "id": "1f42ab02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b47CtWKqHg0B"
   },
   "outputs": [],
   "source": [
    "train_data_rescaled = train_data.map(rescale_coords_map)\n",
    "validation_data_rescaled = validation_data.map(rescale_coords_map)"
   ],
   "id": "0ed29287"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-AX95Hj7_mD"
   },
   "outputs": [],
   "source": "def prepare_masked_dataset(dataset, calibration_points=None):\n\n    # Step 1: Group dataset by subject_id and batch all images\n    def group_by_subject(subject_id, ds):\n        return ds.batch(batch_size=MAX_TARGETS)\n\n    grouped_dataset = dataset.group_by_window(\n        key_func=lambda img, mesh, coords, subject_id: subject_id,\n        reduce_func=group_by_subject,\n        window_size=MAX_TARGETS\n    )\n\n    # Step 2: Filter out subjects with fewer than 72 images\n    def filter_by_image_count(images, meshes, coords, subject_ids):\n        return ops.shape(images)[0] >= 144\n\n    grouped_dataset = grouped_dataset.filter(filter_by_image_count)\n\n    # Step 3: Transform each batch to include masks\n    def add_masks_to_batch(images, meshes, coords, subject_ids):\n\n        actual_batch_size = ops.shape(images)[0]\n\n        cal_mask = tf.zeros(MAX_TARGETS, dtype=\"int8\")\n        target_mask = tf.zeros(MAX_TARGETS, dtype=\"int8\")\n\n        # Determine how many calibration images to use (random between min and max)\n        if calibration_points is None:\n          n_cal_images = tf.random.uniform(\n              shape=[],\n              minval=MIN_CAL_POINTS,\n              maxval=MAX_CAL_POINTS,\n              dtype=\"int32\"\n          )\n          # Create random indices for calibration images\n          # NOTE: These will be different each time through the dataset\n          random_indices = tf.random.shuffle(ops.arange(actual_batch_size))\n          cal_indices = random_indices[:n_cal_images]\n\n          # Create masks (1 = included, 0 = excluded)\n          cal_mask = tf.scatter_nd(\n              ops.expand_dims(cal_indices, 1),\n              tf.ones(n_cal_images, dtype=\"int8\"),\n              [MAX_TARGETS]\n          )\n        else:\n          coords_xpand = ops.expand_dims(coords, axis=1)\n          cal_xpand = ops.expand_dims(calibration_points, axis=0)\n          equality = ops.equal(coords_xpand, cal_xpand)\n          matches = ops.all(equality, axis=-1)\n          point_matches = ops.any(matches, axis=1)\n          cal_mask = ops.cast(point_matches, dtype=\"int8\")\n\n\n\n        target_mask = 1 - cal_mask\n\n        # Pad everything to fixed size\n\n        padded_images = tf.pad(\n            ops.reshape(images, (-1, 36, 144, 1)),\n            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n        )\n        padded_coords = tf.pad(\n            coords,\n            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n        )\n\n        # Ensure all shapes are fixed\n        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n        padded_cal_mask = tf.ensure_shape(cal_mask, [MAX_TARGETS])\n        padded_target_mask = tf.ensure_shape(target_mask, [MAX_TARGETS])\n        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids\n\n    # Apply the transformation\n    masked_dataset = grouped_dataset.map(\n        lambda imgs, meshes, coords, subj_ids: add_masks_to_batch(imgs, meshes, coords, subj_ids),\n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n\n    return masked_dataset\n\n# Modified model input preparation function\ndef prepare_model_inputs(features, labels, subject_ids):\n    \"\"\"\n    Restructure the inputs for the model, using the mask-based approach.\n\n    Args:\n        features: Tuple of (images, coords, cal_mask, target_mask)\n        labels: The target coordinates\n\n    Returns:\n        Dictionary of inputs for the model and the target labels\n    \"\"\"\n    images, coords, cal_mask, target_mask = features\n\n    # Use masking to create the model inputs\n    inputs = {\n        \"Input_All_Images\": images,                   # All images (both cal and target)\n        \"Input_All_Coords\": coords,                   # All coordinates\n        \"Input_Calibration_Mask\": cal_mask,           # Mask indicating calibration images\n    }\n\n    return inputs, labels, target_mask # target_mask is used as sample weights for loss function",
   "id": "ed1d493b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caGtgpGYHlAD"
   },
   "outputs": [],
   "source": "cal_points = np.array([\n    [5, 5],\n    [5, 27.5],\n    [5, 50],\n    [5, 72.5],\n    [5, 95],\n    [35, 5],\n    [35, 27.5],\n    [35, 50],\n    [35, 72.5],\n    [35, 95],\n    [65, 5],\n    [65, 27.5],\n    [65, 50],\n    [65, 72.5],\n    [65, 95],\n    [95, 5],\n    [95, 27.5],\n    [95, 50],\n    [95, 72.5],\n    [95, 95],\n], dtype=np.float32)\n\nscaled_cal_points = cal_points / 100.0",
   "id": "8d601346"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTBbOOV52Uqh"
   },
   "outputs": [],
   "source": [
    "masked_dataset = prepare_masked_dataset(train_data_rescaled)"
   ],
   "id": "a00cbde1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21AN7OpI4ee2"
   },
   "outputs": [],
   "source": [
    "train_ds_for_model = masked_dataset.map(\n",
    "    prepare_model_inputs,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").shuffle(200).prefetch(tf.data.AUTOTUNE)"
   ],
   "id": "dd7b12ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlsnVzhWl3Wq"
   },
   "outputs": [],
   "source": [
    "INDIVIDUALS = 0\n",
    "for e in train_ds_for_model.as_numpy_iterator():\n",
    "  INDIVIDUALS += 1"
   ],
   "id": "1828321a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3trgiBOS1A1"
   },
   "source": [
    "## Visualization"
   ],
   "id": "c298b80b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glQLYELqaOg9"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nimport tensorflow as tf\nfrom IPython.display import HTML\nimport base64\n\ndef visualize_eye_tracking_data(dataset, interval=100, figsize=(10, 6)):\n    \"\"\"\n    Visualize eye tracking data by animating through a subject's images and displaying\n    where they are looking with a red dot. Returns an HTML animation for direct display\n    in a Colab notebook.\n\n    Args:\n        dataset: A TensorFlow dataset containing a single batch of one subject's data\n                Expected format: ({\"Input_All_Images\": images, \"Input_All_Coords\": coords,\n                                 \"Input_Calibration_Mask\": cal_mask, \"Input_Target_Mask\": target_mask}, labels)\n                OR: Your model's dataset with one subject batch\n        interval: Time between frames in milliseconds\n        figsize: Size of the figure (width, height)\n\n    Returns:\n        IPython.display.HTML object with the animation\n\n    Example usage in a Colab notebook:\n    ```python\n    # For a dataset from a single subject\n    subject_dataset = train_ds_for_model.take(1)\n\n    # Or you can create a dataset for just one subject (example from your notebook):\n    # This line extracts data for one subject from the complete dataset\n    single_subject = train_ds_for_model.filter(\n        lambda inputs, labels: ops.equal(inputs[\"Input_Subject_ID\"][0], subject_id)\n    ).take(1)\n\n    # Display the animation directly in the notebook\n    from IPython.display import display\n    animation = visualize_eye_tracking_data(single_subject)\n    display(animation)\n    ```\n    \"\"\"\n    # Extract data from the dataset\n    for inputs, labels, _ in dataset.take(1):\n        images = inputs[\"Input_All_Images\"]\n        coords = inputs[\"Input_All_Coords\"]\n\n        # Convert to numpy arrays\n        valid_images = images.numpy()\n        valid_coords = coords.numpy()\n\n        # Sort by X coordinate, then by Y coordinate\n        sorted_indices = np.lexsort((valid_coords[:, 0], valid_coords[:, 1]))\n        valid_images = valid_images[sorted_indices]\n        valid_coords = valid_coords[sorted_indices]\n\n        num_images = valid_images.shape[0]\n\n    # Set up the figure\n    plt.ioff()  # Turn off interactive mode to avoid displaying during generation\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Create a function that draws each frame\n    def draw_frame(frame_num):\n        ax.clear()\n\n        # Set up the 16:9 coordinate space\n        ax.set_xlim(0, 100)\n        ax.set_ylim(100, 0)  # 16:9 aspect ratio\n\n        # Get the current image and coordinates\n        img = valid_images[frame_num].squeeze()\n        x, y = valid_coords[frame_num]\n\n        # Draw rectangle for the screen\n        rect = plt.Rectangle((0, 0), 100, 100, fill=False, color='black', linewidth=2)\n        ax.add_patch(rect)\n\n        # Add image display in the top right corner\n        img_display = ax.inset_axes([0.35, 0.35, 0.3, 0.3], transform=ax.transAxes)\n        img_display.imshow(np.fliplr(img), cmap='gray')\n        img_display.axis('off')\n\n        # Draw red dot at the coordinate - make it bigger for visibility\n        ax.scatter(x * 100, y * 100, color='red', s=150, zorder=5,\n                  edgecolor='white', linewidth=1.5)\n\n        # Draw crosshair\n        ax.axhline(y * 100, color='gray', linestyle='--', alpha=0.5, zorder=1)\n        ax.axvline(x * 100, color='gray', linestyle='--', alpha=0.5, zorder=1)\n\n        # Set title\n        ax.set_title('Eye Tracking Visualization')\n\n        # Set axis labels\n        ax.set_xlabel('X Coordinate')\n        ax.set_ylabel('Y Coordinate')\n\n    # Create the animation\n    anim = animation.FuncAnimation(fig, draw_frame, frames=num_images, interval=interval)\n\n  # Use animation's to_jshtml method which directly generates HTML\n    html_animation = anim.to_jshtml()\n\n    # Clean up\n    plt.close(fig)\n\n    # Create an HTML object that can be displayed in the notebook\n    return HTML(html_animation)",
   "id": "185f9eb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oB2m-VExaRSm"
   },
   "outputs": [],
   "source": [
    "visualize_eye_tracking_data(train_ds_for_model.take(1))"
   ],
   "id": "6dafa1ca"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMGmIObc0e8Z"
   },
   "source": [],
   "id": "2833fa7b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UfY_C8zMW8m"
   },
   "source": [
    "# Model building"
   ],
   "id": "69ddf0b1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIjimxPW0kMY"
   },
   "source": [
    "## Custom Layers\n"
   ],
   "id": "4754fcbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCx-whu4HlDB"
   },
   "outputs": [],
   "source": [
    "class SimpleTimeDistributed(keras.layers.Wrapper):\n",
    "    \"\"\"A simplified version of TimeDistributed that applies a layer to every temporal slice of an input.\n",
    "\n",
    "    This implementation avoids for loops by using reshape operations to apply the wrapped layer\n",
    "    to all time steps at once.\n",
    "\n",
    "    Args:\n",
    "        layer: a `keras.layers.Layer` instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        super().__init__(layer, **kwargs)\n",
    "        self.supports_masking = getattr(layer, 'supports_masking', False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Validate input shape has at least 3 dimensions (batch, time, ...)\n",
    "        if not isinstance(input_shape, (tuple, list)) or len(input_shape) < 3:\n",
    "            raise ValueError(\n",
    "                \"`SimpleTimeDistributed` requires input with at least 3 dimensions\"\n",
    "            )\n",
    "\n",
    "        # Build the wrapped layer with shape excluding the time dimension\n",
    "        super().build((input_shape[0], *input_shape[2:]))\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Get output shape by applying the layer to a single time slice\n",
    "        child_output_shape = self.layer.compute_output_shape((input_shape[0], *input_shape[2:]))\n",
    "        # Include time dimension in the result\n",
    "        return (child_output_shape[0], input_shape[1], *child_output_shape[1:])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        time_steps = input_shape[1]\n",
    "\n",
    "        # Reshape inputs to combine batch and time dimensions: (batch*time, ...)\n",
    "        reshaped_inputs = ops.reshape(inputs, (-1, *input_shape[2:]))\n",
    "\n",
    "        # Apply the layer to all time steps at once\n",
    "        outputs = self.layer.call(reshaped_inputs, training=training)\n",
    "\n",
    "        # Get output dimensions\n",
    "        output_shape = ops.shape(outputs)\n",
    "\n",
    "        # Reshape back to include the separate batch and time dimensions: (batch, time, ...)\n",
    "        return ops.reshape(outputs, (batch_size, time_steps, *output_shape[1:]))"
   ],
   "id": "3b5a1c66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbWzR7yfMZv9"
   },
   "outputs": [],
   "source": [
    "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A custom layer that performs weighted ridge regression with proper masking support.\n",
    "\n",
    "    This layer takes embeddings, coordinates, weights, and calibration mask as explicit inputs,\n",
    "    while using Keras' masking system to handle target masking. This separation allows more\n",
    "    precise control over calibration points while leveraging Keras' built-in mask propagation\n",
    "    for target predictions.\n",
    "\n",
    "    Args:\n",
    "        lambda_ridge (float): Regularization parameter for ridge regression\n",
    "        embedding_dim (int): Dimension of the embedding vectors\n",
    "        epsilon (float): Small constant for numerical stability inside sqrt\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_ridge, epsilon=1e-7, **kwargs): # Added epsilon argument\n",
    "        self.lambda_ridge = lambda_ridge\n",
    "        self.epsilon = epsilon # Store epsilon\n",
    "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        \"\"\"\n",
    "        The forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            inputs: A list containing:\n",
    "                - embeddings: Embeddings for all points (batch_size, n_points, embedding_dim)\n",
    "                - coords: Coordinates for all points (batch_size, n_points, 2)\n",
    "                - calibration_weights: Importance weights (batch_size, n_points, 1)\n",
    "                - cal_mask: Mask for calibration points (batch_size, n_points) [EXPLICIT]\n",
    "\n",
    "        Returns:\n",
    "            Predicted coordinates for the target points (batch_size, n_points, 2)\n",
    "        \"\"\"\n",
    "        # Unpack inputs\n",
    "        embeddings, coords, calibration_weights, cal_mask  = inputs\n",
    "\n",
    "        # Ensure correct dtype, especially important for JIT\n",
    "        embeddings = ops.cast(embeddings, \"float32\")\n",
    "        coords = ops.cast(coords, \"float32\")\n",
    "        calibration_weights = ops.cast(calibration_weights, \"float32\")\n",
    "        cal_mask = ops.cast(cal_mask, \"float32\")\n",
    "\n",
    "        # reshape weights to (batch, calibration)\n",
    "        w = ops.squeeze(calibration_weights, axis=-1)\n",
    "\n",
    "        # Pre-compute masked weights for calibration points\n",
    "        w_masked = w * cal_mask\n",
    "        # Add epsilon inside sqrt for numerical stability\n",
    "        w_sqrt = ops.sqrt(w_masked + self.epsilon)\n",
    "        w_sqrt = ops.expand_dims(w_sqrt, -1)  # More efficient than reshape\n",
    "\n",
    "        # Apply calibration mask to embeddings\n",
    "        cal_mask_expand = ops.expand_dims(cal_mask, -1)\n",
    "        X = embeddings * cal_mask_expand # Mask out non-calibration embeddings for regression calculation\n",
    "\n",
    "        # Weight calibration embeddings and coordinates using the masked weights\n",
    "        X_weighted = X * w_sqrt\n",
    "        y_weighted = coords * w_sqrt * cal_mask_expand # Also mask coords just to be safe\n",
    "\n",
    "        # Matrix operations\n",
    "        X_t = ops.transpose(X_weighted, axes=[0, 2, 1])\n",
    "        X_t_X = ops.matmul(X_t, X_weighted)\n",
    "\n",
    "        # Add regularization - CORRECTED: Removed the extra * 1e-3\n",
    "        # Ensure the identity matrix is also float32\n",
    "        identity_matrix = ops.cast(ops.eye(ops.shape(embeddings)[-1]), \"float32\")\n",
    "        lhs = X_t_X + self.lambda_ridge * identity_matrix\n",
    "\n",
    "        # Compute RHS\n",
    "        rhs = ops.matmul(X_t, y_weighted)\n",
    "\n",
    "        # Solve the system\n",
    "        # Consider adding checks or alternative solvers if instability persists,\n",
    "        # but correcting the regularization should be the primary fix.\n",
    "        kernel = ops.linalg.solve(lhs, rhs)\n",
    "\n",
    "        # Apply regression using the *original* full embeddings\n",
    "        output = ops.matmul(embeddings, kernel)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        \"\"\"\n",
    "        Computes the output shape of the layer.\n",
    "\n",
    "        Args:\n",
    "            input_shapes: List of input shapes\n",
    "\n",
    "        Returns:\n",
    "            Output shape tuple\n",
    "        \"\"\"\n",
    "        # Output shape matches coordinates: (batch_size, n_points, 2)\n",
    "        return (input_shapes[0][0], input_shapes[0][1], 2)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the configuration of the layer for serialization.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing the layer configuration\n",
    "        \"\"\"\n",
    "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"lambda_ridge\": self.lambda_ridge,\n",
    "            \"epsilon\": self.epsilon # Add epsilon to config\n",
    "        })\n",
    "        return config"
   ],
   "id": "f4a667e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt7x7bN6EzGn"
   },
   "outputs": [],
   "source": "class MaskInspectorLayer(keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(MaskInspectorLayer, self).__init__(**kwargs)\n        self.supports_masking = True\n    def call(self, inputs, mask=None):\n      # Print mask information\n      print(\"Layer mask:\", mask)\n      return inputs",
   "id": "283735f4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsb8-Q2T0pkB"
   },
   "source": [
    "## Custom loss"
   ],
   "id": "c95743d8"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3a-BpZNsvUE"
   },
   "source": [
    "## Embedding Model"
   ],
   "id": "6bb693c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JY_hBgtj1R9L"
   },
   "source": [
    "### Backbones"
   ],
   "id": "f45ea0fe"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vibW_5HzlOzj"
   },
   "source": [
    "#### DenseNet"
   ],
   "id": "05ca40c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9v9uaV4h1atr"
   },
   "outputs": [],
   "source": "def create_dense_net_backbone():\n    \"\"\"\n    Creates a DenseNet backbone for eye tracking images.\n    \n    Architecture:\n    - Input: 36x144x1 grayscale images\n    - Dense blocks: Configurable via DENSENET_STACKWISE_NUM_REPEATS\n    - Default [4,4,4]: 3 dense blocks with 4 layers each\n    \n    Returns:\n        DenseNet backbone model\n    \"\"\"\n    return keras_hub.models.DenseNetBackbone(\n        stackwise_num_repeats=DENSENET_STACKWISE_NUM_REPEATS,\n        image_shape=(36, 144, 1),\n    )",
   "id": "d8580a3a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn-kyypalQbD"
   },
   "source": [
    "#### Involution"
   ],
   "id": "5c795de8"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6A07vCFlU5k"
   },
   "source": [
    "#### EfficientNetB0"
   ],
   "id": "6edeadd2"
  },
  {
   "cell_type": "code",
   "source": "def create_vit_backbone():\n    \"\"\"\n    Creates a Vision Transformer backbone for processing eye images.\n    \n    Architecture:\n    - Input: 36x144x1 grayscale images\n    - Patches: 4x4 patches → 9×36 = 324 patches\n    - Transformer: 6 layers, 8 heads, 256 hidden dim, 1024 MLP dim\n    - Output: (batch, num_patches+1, hidden_dim) with CLS token\n    \n    Note: Uses custom Conv2D-based patching to handle non-square images\n    natively without wasting compute on padding.\n    \"\"\"\n    \n    image_shape = (36, 144, 1)\n    inputs = keras.layers.Input(shape=image_shape)\n    \n    # Patch embedding using Conv2D (handles non-square images)\n    patch_embeddings = keras.layers.Conv2D(\n        filters=VIT_HIDDEN_DIM,\n        kernel_size=VIT_PATCH_SIZE,\n        strides=VIT_PATCH_SIZE,\n        padding=\"valid\",\n        activation=None,\n        name=\"patch_embedding\"\n    )(inputs)\n    # Shape: (batch, 9, 36, 256)\n    \n    # Reshape patches to sequence\n    num_patches_h = 36 // VIT_PATCH_SIZE  # 9\n    num_patches_w = 144 // VIT_PATCH_SIZE  # 36\n    num_patches = num_patches_h * num_patches_w  # 324\n    \n    patches_reshaped = keras.layers.Reshape(\n        target_shape=(num_patches, VIT_HIDDEN_DIM),\n        name=\"flatten_patches\"\n    )(patch_embeddings)\n    # Shape: (batch, 324, 256)\n    \n    # Add CLS token\n    class_token = keras.layers.Embedding(\n        input_dim=1,\n        output_dim=VIT_HIDDEN_DIM,\n        embeddings_initializer=\"random_normal\",\n        name=\"cls_token\"\n    )(keras.ops.zeros((keras.ops.shape(patches_reshaped)[0], 1), dtype=\"int32\"))\n    # Shape: (batch, 1, 256)\n    \n    # Concatenate CLS token with patches\n    tokens = keras.layers.Concatenate(axis=1, name=\"concat_cls\")(\n        [class_token, patches_reshaped]\n    )\n    # Shape: (batch, 325, 256)\n    \n    # Add positional embeddings\n    num_positions = num_patches + 1  # 325\n    position_embedding = keras.layers.Embedding(\n        input_dim=num_positions,\n        output_dim=VIT_HIDDEN_DIM,\n        embeddings_initializer=keras.initializers.RandomNormal(stddev=0.02),\n        name=\"position_embedding\"\n    )\n    \n    position_ids = keras.ops.expand_dims(\n        keras.ops.arange(num_positions), axis=0\n    )\n    position_ids = keras.ops.tile(\n        position_ids, [keras.ops.shape(tokens)[0], 1]\n    )\n    pos_embeddings = position_embedding(position_ids)\n    \n    embeddings = keras.layers.Add(name=\"add_position_embedding\")(\n        [tokens, pos_embeddings]\n    )\n    # Shape: (batch, 325, 256)\n    \n    # Apply Transformer Encoder\n    x = embeddings\n    for i in range(VIT_NUM_LAYERS):\n        # Pre-norm architecture\n        x_norm = keras.layers.LayerNormalization(epsilon=1e-6, name=f\"layer_norm_{i}_1\")(x)\n        \n        # Multi-head self-attention\n        attn_output = keras.layers.MultiHeadAttention(\n            num_heads=VIT_NUM_HEADS,\n            key_dim=VIT_HIDDEN_DIM // VIT_NUM_HEADS,\n            dropout=VIT_ATTENTION_DROPOUT,\n            name=f\"attention_{i}\"\n        )(x_norm, x_norm)\n        \n        attn_output = keras.layers.Dropout(VIT_DROPOUT, name=f\"attn_dropout_{i}\")(attn_output)\n        x = keras.layers.Add(name=f\"attn_add_{i}\")([x, attn_output])\n        \n        # MLP block\n        x_norm = keras.layers.LayerNormalization(epsilon=1e-6, name=f\"layer_norm_{i}_2\")(x)\n        \n        mlp = keras.Sequential([\n            keras.layers.Dense(VIT_MLP_DIM, activation=\"gelu\", name=f\"mlp_{i}_dense_1\"),\n            keras.layers.Dropout(VIT_DROPOUT, name=f\"mlp_{i}_dropout_1\"),\n            keras.layers.Dense(VIT_HIDDEN_DIM, name=f\"mlp_{i}_dense_2\"),\n            keras.layers.Dropout(VIT_DROPOUT, name=f\"mlp_{i}_dropout_2\"),\n        ], name=f\"mlp_{i}\")\n        \n        mlp_output = mlp(x_norm)\n        x = keras.layers.Add(name=f\"mlp_add_{i}\")([x, mlp_output])\n    \n    # Final layer norm\n    encoder_output = keras.layers.LayerNormalization(epsilon=1e-6, name=\"final_layer_norm\")(x)\n    # Shape: (batch, 325, 256)\n    \n    model = keras.Model(inputs=inputs, outputs=encoder_output, name=\"ViT_Backbone\")\n    \n    return model",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "ba5a7086"
  },
  {
   "cell_type": "markdown",
   "source": "#### Vision Transformer",
   "metadata": {},
   "id": "716a9c02"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_1qYhG1y8eN"
   },
   "source": [
    "### Create Embedding Model"
   ],
   "id": "67eb3614"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnKs983UsyDj"
   },
   "outputs": [],
   "source": [
    "def create_embedding_model(BACKBONE):\n",
    "  image_shape = (36, 144, 1)\n",
    "  input_eyes = keras.layers.Input(shape=image_shape)\n",
    "\n",
    "  eyes_rescaled = keras.layers.Rescaling(scale=1./255)(input_eyes)\n",
    "\n",
    "  # Continue with the backbone\n",
    "  # backbone = create_rednet_backbone()\n",
    "\n",
    "\n",
    "  # backbone = keras.Sequential([\n",
    "  #     keras.layers.Flatten(),\n",
    "  #     keras.layers.Dense(10, activation=\"relu\")\n",
    "  # ])\n",
    "  if BACKBONE == \"densenet\":\n",
    "    backbone = create_dense_net_backbone()\n",
    "\n",
    "  # backbone = create_efficientnet_backbone()\n",
    "\n",
    "  # backbone = keras_hub.models.MiTBackbone(\n",
    "  #   image_shape=(36,144,1),\n",
    "  #   layerwise_depths=[2,2,2,2],\n",
    "  #   num_layers=4,\n",
    "  #   layerwise_num_heads=[1,2,5,8],\n",
    "  #   layerwise_sr_ratios=[8,4,2,1],\n",
    "  #   max_drop_path_rate=0.1,\n",
    "  #   layerwise_patch_sizes=[7,3,3,3],\n",
    "  #   layerwise_strides=[4,2,2,2],\n",
    "  #   hidden_dims=[32,64,160,256]\n",
    "  # )\n",
    "\n",
    "  backbone_encoder = backbone(eyes_rescaled)\n",
    "  flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
    "  eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
    "\n",
    "  embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
    "\n",
    "  return embedding_model"
   ],
   "id": "f84812a6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoGfwwgI03Ry"
   },
   "source": [
    "## Full Model"
   ],
   "id": "e6dc6610"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdQbBaKR6vLN"
   },
   "source": [
    "# Model Training\n"
   ],
   "id": "4746b450"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xes53rsAcBOp"
   },
   "source": [
    "## Augmentation"
   ],
   "id": "9c15d114"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsXuwWINgwb7"
   },
   "outputs": [],
   "source": "def get_translation_matrix(tx, ty):\n    \"\"\"Creates a 3x3 translation matrix.\"\"\"\n    return ops.convert_to_tensor([\n        [1., 0., tx],\n        [0., 1., ty],\n        [0., 0., 1.]\n    ], dtype=\"float32\")\n\ndef get_rotation_matrix(angle, center_x, center_y):\n    \"\"\"Creates a 3x3 rotation matrix around a center point.\"\"\"\n    center_x = ops.cast(center_x, \"float32\")\n    center_y = ops.cast(center_y, \"float32\")\n    cos_a = ops.cos(angle)\n    sin_a = ops.sin(angle)\n    m_rot = ops.convert_to_tensor([\n        [cos_a, -sin_a, 0.],\n        [sin_a,  cos_a, 0.],\n        [0.,     0.,    1.]\n    ], dtype=\"float32\")\n    m_trans1 = get_translation_matrix(-center_x, -center_y)\n    m_trans2 = get_translation_matrix(center_x, center_y)\n    return m_trans2 @ m_rot @ m_trans1\n\ndef get_zoom_matrix(zx, zy, center_x, center_y):\n    \"\"\"Creates a 3x3 zoom matrix around a center point.\"\"\"\n    center_x = ops.cast(center_x, \"float32\")\n    center_y = ops.cast(center_y, \"float32\")\n    m_scale = ops.convert_to_tensor([\n        [zx, 0., 0.],\n        [0., zy, 0.],\n        [0., 0., 1.]\n    ], dtype=\"float32\")\n    m_trans1 = get_translation_matrix(-center_x, -center_y)\n    m_trans2 = get_translation_matrix(center_x, center_y)\n    return m_trans2 @ m_scale @ m_trans1\n\ndef get_flip_matrix(horizontal, center_x, center_y):\n    \"\"\"Creates a 3x3 horizontal flip matrix around a center point.\"\"\"\n    if not horizontal:\n        return ops.eye(3, dtype=\"float32\")\n    center_x = ops.cast(center_x, \"float32\")\n    center_y = ops.cast(center_y, \"float32\")\n    m_flip = ops.convert_to_tensor([\n        [-1., 0., 0.],\n        [ 0., 1., 0.],\n        [ 0., 0., 1.]\n    ], dtype=\"float32\")\n    m_trans1 = get_translation_matrix(-center_x, -center_y)\n    m_trans2 = get_translation_matrix(center_x, center_y)\n    return m_trans2 @ m_flip @ m_trans1\n\ndef matrix_to_affine_params(matrix):\n    \"\"\"Extracts the 8 parameters from a 3x3 affine matrix.\"\"\"\n    params = ops.stack([\n        matrix[0, 0], matrix[0, 1], matrix[0, 2],\n        matrix[1, 0], matrix[1, 1], matrix[1, 2],\n        matrix[2, 0], matrix[2, 1]\n    ])\n    return params",
   "id": "c8e718ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roV0qSaLcT4T"
   },
   "outputs": [],
   "source": "def apply_affine_augmentation(all_inputs, targets, mask):\n    \"\"\"\n    Applies the same random AFFINE augmentations to all frames in a sequence\n    using keras.ops.image.affine_transform and keras.random for seeded randomness.\n\n    Args:\n        sequence: A tensor of shape (timesteps, H, W, C).\n\n    Returns:\n        The augmented sequence tensor with the same shape.\n    \"\"\"\n\n    sequence = all_inputs\n\n    sequence_shape = keras.ops.shape(sequence)\n    img_height = sequence_shape[1]\n    img_width = sequence_shape[2]\n    img_height_f = ops.cast(img_height, \"float32\")\n    img_width_f = ops.cast(img_width, \"float32\")\n    center_x = img_width_f / 2.0\n    center_y = img_height_f / 2.0\n\n    # --- Generate ONE base seed pair for this sequence ---\n    # Use tf.random.uniform (stateful) to get a UNIQUE starting point\n    # for EACH sequence processed by the map function. This works reliably.\n    base_seed = tf.random.uniform([2], minval=0, maxval=2147483647, dtype=\"int32\")\n\n    # --- Generate Random Parameters using tf.random.stateless_uniform ---\n    # Note: tf.random.stateless_uniform requires shape as the first arg.\n    # 1. Flip Decision\n    flip_seed = base_seed + [0, 1] # Derive seed\n    # Use tf.random.stateless_uniform directly\n    random_flip_val = tf.random.stateless_uniform(shape=(), seed=flip_seed, minval=0.0, maxval=1.0)\n    apply_flip = random_flip_val < 0.5\n\n    # 2. Rotation Angle\n    rotation_factor = 0.05\n    max_angle = rotation_factor * math.pi\n    rotation_seed = base_seed + [0, 2] # Derive seed\n    # Use tf.random.stateless_uniform directly\n    rotation_angle = tf.random.stateless_uniform(shape=(), seed=rotation_seed, minval=-max_angle, maxval=max_angle)\n\n    # 3. Zoom Factors\n    zoom_factor_range = (0.9, 1.1)\n    zoom_h_seed = base_seed + [0, 3] # Derive seed H\n    zoom_w_seed = base_seed + [0, 4] # Derive seed W\n    # Use tf.random.stateless_uniform directly\n    zoom_height_factor = tf.random.stateless_uniform(shape=(), seed=zoom_h_seed, minval=zoom_factor_range[0], maxval=zoom_factor_range[1])\n    zoom_width_factor = tf.random.stateless_uniform(shape=(), seed=zoom_w_seed, minval=zoom_factor_range[0], maxval=zoom_factor_range[1])\n\n    # 4. Translation Shifts\n    translation_height_factor = 0.1\n    translation_width_factor = 0.1\n    shift_h_seed = base_seed + [0, 5] # Derive seed H\n    shift_w_seed = base_seed + [0, 6] # Derive seed W\n    # Use tf.random.stateless_uniform directly\n    random_shift_h = tf.random.stateless_uniform(shape=(), seed=shift_h_seed, minval=-translation_height_factor, maxval=translation_height_factor)\n    random_shift_w = tf.random.stateless_uniform(shape=(), seed=shift_w_seed, minval=-translation_width_factor, maxval=translation_width_factor)\n    shift_height = random_shift_h * ops.cast(img_height, \"float32\")\n    shift_width = random_shift_w * ops.cast(img_width, \"float32\")\n\n    # --- Build Combined Affine Matrix (Same as before) ---\n    combined_matrix = ops.eye(3, dtype=\"float32\")\n    flip_matrix = get_flip_matrix(apply_flip, center_x, center_y)\n    zoom_matrix = get_zoom_matrix(zoom_width_factor, zoom_height_factor, center_x, center_y)\n    rotation_matrix = get_rotation_matrix(rotation_angle, center_x, center_y)\n    translation_matrix = get_translation_matrix(shift_width, shift_height)\n    combined_matrix = translation_matrix @ rotation_matrix @ zoom_matrix @ flip_matrix\n    combined_matrix = flip_matrix\n\n    # --- Extract 8 parameters for Keras Op (Same as before) ---\n    affine_params = matrix_to_affine_params(combined_matrix)\n\n    # --- Apply the transformation using Keras Ops (Same as before) ---\n    # keras.ops.image.affine_transform uses the backend implementation which\n    # should handle the transformation correctly based on the calculated params.\n    augmented_sequence = keras.ops.image.affine_transform(\n        sequence,\n        affine_params,\n        interpolation=\"bilinear\",\n        fill_mode=\"constant\"\n    )\n\n     # === PERSPECTIVE AUGMENTATION ===\n\n    # Define the magnitude of perspective shift\n    perspective_factor = 0.1 # Max shift of 10% of image width/height\n\n    # --- Define Start Points (corners of the original image) ---\n    # Order: Top-left, Bottom-left, Top-right, Bottom-right (consistent with docs example)\n    start_points = ops.convert_to_tensor([\n        [0.0, 0.0],\n        [0.0, img_height_f],\n        [img_width_f, 0.0],\n        [img_width_f, img_height_f]\n    ], dtype=\"float32\") # Shape: (4, 2)\n\n    # --- Generate Random Shifts for End Points ---\n    perspective_shift_seed = base_seed + [0, 7] # Next available seed offset\n    max_shift_x = perspective_factor * img_width_f\n    max_shift_y = perspective_factor * img_height_f\n\n    # Generate random shifts for each of the 4 points (x, y coordinates)\n    # Shape (4, 2) -> [[dx_tl, dy_tl], [dx_bl, dy_bl], [dx_tr, dy_tr], [dx_br, dy_br]]\n    random_shifts = tf.random.stateless_uniform(\n        shape=(4, 2),\n        seed=perspective_shift_seed,\n        minval=-1.0, # Generate normalized shifts first\n        maxval=1.0\n    )\n\n    # Scale normalized shifts by max possible shift\n    scaled_shifts = random_shifts * ops.convert_to_tensor([[max_shift_x, max_shift_y]], dtype=\"float32\") # Shape (1, 2) broadcasts\n\n    # --- Calculate End Points ---\n    end_points = start_points + scaled_shifts # Shape (4, 2)\n\n    start_points_tiled = ops.tile(ops.expand_dims(start_points, axis=0), [sequence_shape[0], 1, 1])\n    end_points_tiled = ops.tile(ops.expand_dims(end_points, axis=0), [sequence_shape[0], 1, 1])\n\n    # --- Apply the Perspective transformation using Keras Ops ---\n    # Apply perspective AFTER affine transformation\n    augmented_sequence = keras.ops.image.perspective_transform(\n        augmented_sequence, # Input is the result from affine transform\n        start_points_tiled,       # Shape (4, 2)\n        end_points_tiled,         # Shape (4, 2)\n        interpolation=\"bilinear\",\n        fill_value=0.0      # Value for pixels outside the warped image\n    )\n\n    # --- Optional Color Augmentations (using tf.random.stateless_*) ---\n    # Example:\n    # brightness_seed = base_seed + [0, 7]\n    # brightness_delta = tf.random.stateless_uniform((), seed=brightness_seed, minval=-0.2, maxval=0.2)\n    # augmented_sequence = augmented_sequence + brightness_delta\n    # augmented_sequence = keras.ops.clip(augmented_sequence, 0.0, 1.0)\n\n    # contrast_seed = base_seed + [0, 8]\n    # contrast_factor = tf.random.stateless_uniform((), seed=contrast_seed, minval=0.7, maxval=1.3)\n    # pixel_mean = keras.ops.mean(augmented_sequence, axis=[1, 2], keepdims=True)\n    # augmented_sequence = (augmented_sequence - pixel_mean) * contrast_factor + pixel_mean\n    # augmented_sequence = keras.ops.clip(augmented_sequence, 0.0, 1.0)\n\n\n\n    return augmented_sequence",
   "id": "5cbe4c9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23NmRY6ckdnl"
   },
   "outputs": [],
   "source": "def apply_non_affine_augmentations(sequence, per_image=False):\n    \"\"\"\n    Apply non-affine augmentations (blur, brightness, contrast, gamma, noise) to a sequence.\n\n    Args:\n        sequence: Input image sequence tensor of shape (timesteps, H, W, C) - uint8 [0,255]\n        per_image: If True, each image gets different augmentation; if False, same for all\n\n    Returns:\n        Augmented sequence tensor - uint8 [0,255]\n    \"\"\"\n    sequence_shape = keras.ops.shape(sequence)\n    timesteps = sequence_shape[0]\n\n    # Generate the base_seed internally\n    base_seed = tf.random.uniform([2], minval=0, maxval=2147483647, dtype=\"int32\")\n\n    # Work with uint8 input directly - convert to float32 only for processing\n    augmented_sequence = keras.ops.cast(sequence, \"float32\")\n\n    if per_image:\n        # Apply different augmentation to each image in the sequence\n\n        # Generate random parameters for each timestep\n        # Blur\n        blur_seed = base_seed + [1, 0]\n        blur_probs = tf.random.stateless_uniform(\n            shape=(timesteps,),\n            seed=blur_seed,\n            minval=0.0,\n            maxval=1.0\n        )\n        blur_sigmas = tf.random.stateless_uniform(\n            shape=(timesteps,),\n            seed=blur_seed + [0, 1],\n            minval=BLUR_SIGMA_RANGE[0],\n            maxval=BLUR_SIGMA_RANGE[1]\n        )\n\n        # Brightness - scale to uint8 range\n        brightness_seed = base_seed + [2, 0]\n        brightness_deltas = tf.random.stateless_uniform(\n            shape=(timesteps,),\n            seed=brightness_seed,\n            minval=-25.0,  # Equivalent to -0.1 * 255\n            maxval=25.0    # Equivalent to 0.1 * 255\n        )\n\n        # Contrast - keep same range as before\n        contrast_seed = base_seed + [3, 0]\n        contrast_factors = tf.random.stateless_uniform(\n            shape=(timesteps,),\n            seed=contrast_seed,\n            minval=0.9,\n            maxval=1.1\n        )\n\n        # Gamma - keep same range\n        gamma_seed = base_seed + [4, 0]\n        gamma_values = tf.random.stateless_uniform(\n            shape=(timesteps,),\n            seed=gamma_seed,\n            minval=0.9,\n            maxval=1.1\n        )\n\n        # Noise - scale to uint8 range\n        noise_seed = base_seed + [5, 0]\n        noise_probs = tf.random.stateless_uniform(\n            shape=(timesteps,),\n            seed=noise_seed,\n            minval=0.0,\n            maxval=1.0\n        )\n        noise_stds = tf.random.stateless_uniform(\n            shape=(timesteps,),\n            seed=noise_seed + [0, 1],\n            minval=1.0,    # Equivalent to 0.005 * 255\n            maxval=5.0     # Equivalent to 0.02 * 255\n        )\n\n        # Apply augmentations per image\n        augmented_images = []\n        for t in range(timesteps):\n            img = augmented_sequence[t]\n\n            # Blur\n            if blur_probs[t] < BLUR_PROB:\n                # Apply Gaussian blur using tf.nn.depthwise_conv2d with Gaussian kernel\n                sigma = blur_sigmas[t]\n                kernel_size = ops.cast(ops.ceil(sigma * 3) * 2 + 1, \"int32\")\n                kernel_size = ops.minimum(kernel_size, 15)  # Limit kernel size\n\n                # Create Gaussian kernel\n                x = ops.arange(ops.cast(-kernel_size // 2 + 1, \"float32\"),\n                           ops.cast(kernel_size // 2 + 1, \"float32\"))\n                gauss_kernel_1d = ops.exp(-0.5 * ops.square(x / sigma))\n                gauss_kernel_1d = gauss_kernel_1d / ops.sum(gauss_kernel_1d)\n\n                # Create 2D kernel\n                gauss_kernel_2d = ops.tensordot(gauss_kernel_1d, gauss_kernel_1d, axes=0)\n                gauss_kernel_2d = ops.expand_dims(ops.expand_dims(gauss_kernel_2d, -1), -1)\n\n                # Apply convolution\n                img_expanded = ops.expand_dims(img, 0)  # Add batch dimension\n                img_blurred = tf.nn.depthwise_conv2d(\n                    img_expanded, gauss_kernel_2d, strides=[1, 1, 1, 1], padding='SAME'\n                )\n                img = ops.squeeze(img_blurred, 0)  # Remove batch dimension\n\n            # Brightness - add directly to uint8 values\n            img = img + brightness_deltas[t]\n\n            # Contrast - apply around 127.5 (middle of uint8 range)\n            img = (img - 127.5) * contrast_factors[t] + 127.5\n\n            # Gamma correction - normalize to [0,1], apply gamma, then scale back\n            img_normalized = img / 255.0\n            img_normalized = ops.power(ops.maximum(img_normalized, 1e-8), gamma_values[t])\n            img = img_normalized * 255.0\n\n            # Noise\n            if noise_probs[t] < NOISE_PROB:\n                noise = tf.random.stateless_normal(\n                    shape=ops.shape(img),\n                    seed=noise_seed + [t, 0],\n                    stddev=noise_stds[t]\n                )\n                img = img + noise\n\n            # Clip to valid uint8 range\n            img = ops.clip(img, 0.0, 255.0)\n            augmented_images.append(img)\n\n        augmented_sequence = ops.stack(augmented_images, axis=0)\n\n    else:\n        # Apply same augmentation to all images in the sequence\n\n        # Generate single random parameters for the entire sequence\n        # Blur\n        blur_seed = base_seed + [1, 0]\n        blur_prob = tf.random.stateless_uniform(shape=(), seed=blur_seed, minval=0.0, maxval=1.0)\n        blur_sigma = tf.random.stateless_uniform(\n            shape=(),\n            seed=blur_seed + [0, 1],\n            minval=BLUR_SIGMA_RANGE[0],\n            maxval=BLUR_SIGMA_RANGE[1]\n        )\n\n        # Brightness - scale to uint8 range\n        brightness_seed = base_seed + [2, 0]\n        brightness_delta = tf.random.stateless_uniform(\n            shape=(),\n            seed=brightness_seed,\n            minval=-25.0,  # Equivalent to -0.1 * 255\n            maxval=25.0    # Equivalent to 0.1 * 255\n        )\n\n        # Contrast\n        contrast_seed = base_seed + [3, 0]\n        contrast_factor = tf.random.stateless_uniform(\n            shape=(),\n            seed=contrast_seed,\n            minval=0.9,\n            maxval=1.1\n        )\n\n        # Gamma\n        gamma_seed = base_seed + [4, 0]\n        gamma_value = tf.random.stateless_uniform(\n            shape=(),\n            seed=gamma_seed,\n            minval=0.9,\n            maxval=1.1\n        )\n\n        # Noise - scale to uint8 range\n        noise_seed = base_seed + [5, 0]\n        noise_prob = tf.random.stateless_uniform(shape=(), seed=noise_seed, minval=0.0, maxval=1.0)\n        noise_std = tf.random.stateless_uniform(\n            shape=(),\n            seed=noise_seed + [0, 1],\n            minval=1.0,    # Equivalent to 0.005 * 255\n            maxval=5.0     # Equivalent to 0.02 * 255\n        )\n\n        # Apply blur to entire sequence\n        if blur_prob < BLUR_PROB:\n            # Create Gaussian kernel\n            kernel_size = ops.cast(ops.ceil(blur_sigma * 3) * 2 + 1, \"int32\")\n            kernel_size = ops.minimum(kernel_size, 15)  # Limit kernel size\n\n            x = ops.arange(ops.cast(-kernel_size // 2 + 1, \"float32\"),\n                       ops.cast(kernel_size // 2 + 1, \"float32\"))\n            gauss_kernel_1d = ops.exp(-0.5 * ops.square(x / blur_sigma))\n            gauss_kernel_1d = gauss_kernel_1d / ops.sum(gauss_kernel_1d)\n\n            # Create 2D kernel\n            gauss_kernel_2d = ops.tensordot(gauss_kernel_1d, gauss_kernel_1d, axes=0)\n            gauss_kernel_2d = ops.expand_dims(ops.expand_dims(gauss_kernel_2d, -1), -1)\n\n            # Apply to all frames\n            augmented_sequence = tf.nn.depthwise_conv2d(\n                augmented_sequence, gauss_kernel_2d, strides=[1, 1, 1, 1], padding='SAME'\n            )\n\n        # Apply brightness\n        augmented_sequence = augmented_sequence + brightness_delta\n\n        # Apply contrast around 127.5 (middle of uint8 range)\n        augmented_sequence = (augmented_sequence - 127.5) * contrast_factor + 127.5\n\n        # Apply gamma correction\n        sequence_normalized = augmented_sequence / 255.0\n        sequence_normalized = ops.power(ops.maximum(sequence_normalized, 1e-8), gamma_value)\n        augmented_sequence = sequence_normalized * 255.0\n\n        # Apply noise\n        if noise_prob < NOISE_PROB:\n            noise = tf.random.stateless_normal(\n                shape=ops.shape(augmented_sequence),\n                seed=noise_seed + [0, 2],\n                stddev=noise_std\n            )\n            augmented_sequence = augmented_sequence + noise\n\n        # Clip to valid uint8 range\n        augmented_sequence = ops.clip(augmented_sequence, 0.0, 255.0)\n\n    # Convert back to uint8 to save memory\n    augmented_sequence = keras.ops.cast(augmented_sequence, \"uint8\")\n\n    return augmented_sequence",
   "id": "099c52f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQFeFhHApQb3"
   },
   "outputs": [],
   "source": [
    "def augment_sequence_complete(all_inputs, targets, mask):\n",
    "    \"\"\"\n",
    "    Complete augmentation function that applies both affine and non-affine augmentations.\n",
    "\n",
    "    Affine transformations (flip, rotation, zoom, translation, perspective) are always\n",
    "    applied consistently across the sequence. Non-affine transformations (blur, brightness,\n",
    "    contrast, gamma, noise) can be applied either consistently or per-image based on\n",
    "    PER_IMAGE_AUGMENTATION setting.\n",
    "    \"\"\"\n",
    "\n",
    "    input_all_images = all_inputs[\"Input_All_Images\"]\n",
    "    input_all_coords = all_inputs[\"Input_All_Coords\"]\n",
    "    input_cal_mask = all_inputs[\"Input_Calibration_Mask\"]\n",
    "\n",
    "    augmented_sequence = input_all_images\n",
    "\n",
    "    # Apply non-affine augmentations (blur, brightness, contrast, gamma, noise)\n",
    "    augmented_sequence = apply_non_affine_augmentations(\n",
    "        augmented_sequence,\n",
    "        per_image=PER_IMAGE_AUGMENTATION\n",
    "    )\n",
    "\n",
    "    # Apply affine augmentations (rotation, perspective shift, etc.)\n",
    "    augmented_sequence = apply_affine_augmentation(\n",
    "        augmented_sequence, targets, mask\n",
    "    )\n",
    "\n",
    "    inputs = {\n",
    "        \"Input_All_Images\": augmented_sequence,\n",
    "        \"Input_All_Coords\": input_all_coords,\n",
    "        \"Input_Calibration_Mask\": input_cal_mask,\n",
    "    }\n",
    "\n",
    "    return inputs, targets, mask"
   ],
   "id": "4a77564e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aif6VhRcx3G"
   },
   "outputs": [],
   "source": [
    "if AUGMENTATION:\n",
    "  train_ds_for_model_augmented = train_ds_for_model.map(\n",
    "    augment_sequence_complete,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    "  ).prefetch(tf.data.AUTOTUNE)\n",
    "else:\n",
    "  train_ds_for_model_augmented = train_ds_for_model"
   ],
   "id": "32b1bb47"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQqSdhHwdQe7"
   },
   "source": [
    "### Visualizing augmentation"
   ],
   "id": "4da04358"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB0BDowFdTBq"
   },
   "outputs": [],
   "source": [
    "def plot_sequence_grid(sequence, title=\"Image Sequence Timesteps\", figsize=None, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Plots all frames (timesteps) of an image sequence in a grid layout.\n",
    "\n",
    "    This helps visualize if the same augmentation was applied consistently\n",
    "    across the time dimension.\n",
    "\n",
    "    Args:\n",
    "        sequence: A NumPy array or TensorFlow tensor representing the image\n",
    "                  sequence. Expected shape: (timesteps, H, W, C).\n",
    "        title (str): The overall title for the plot.\n",
    "        figsize (tuple, optional): Desired figure size (width, height) in inches.\n",
    "                                   If None, a default size is calculated based\n",
    "                                   on the number of timesteps.\n",
    "        cmap (str): The colormap to use for grayscale images (when C=1).\n",
    "                    Defaults to 'viridis'. Ignored for RGB images (C=3).\n",
    "    \"\"\"\n",
    "    # --- Input Validation and Conversion ---\n",
    "    if hasattr(sequence, 'numpy'): # Check if it's a TF tensor\n",
    "        sequence = sequence.numpy()\n",
    "\n",
    "    if not isinstance(sequence, np.ndarray):\n",
    "        raise TypeError(\"Input 'sequence' must be a NumPy array or TensorFlow tensor.\")\n",
    "\n",
    "    if sequence.ndim != 4:\n",
    "        raise ValueError(f\"Input 'sequence' must be 4-dimensional (timesteps, H, W, C), got shape {sequence.shape}\")\n",
    "\n",
    "    num_timesteps, height, width, channels = sequence.shape\n",
    "\n",
    "    if num_timesteps == 0:\n",
    "        print(\"Sequence has 0 timesteps, nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    # --- Determine Grid Size ---\n",
    "    # Calculate a reasonable grid size (aiming for roughly square or slightly wider)\n",
    "    # Ignore the 12x12 request as it's usually too large for typical timesteps;\n",
    "    # a dynamic grid is more practical.\n",
    "    ncols = int(math.ceil(math.sqrt(num_timesteps)))\n",
    "    # Try to make it slightly wider if not square - adjust ncols calculation if needed\n",
    "    # A simpler approach: use a max number of columns\n",
    "    # ncols = min(num_timesteps, 6) # Example: Limit to 6 columns max\n",
    "    nrows = int(math.ceil(num_timesteps / float(ncols)))\n",
    "\n",
    "    # --- Determine Figure Size ---\n",
    "    if figsize is None:\n",
    "        # Default figsize: scale based on grid size\n",
    "        # Adjust the multipliers (2.5, 2) as needed for your image aspect ratio/preference\n",
    "        figsize = (ncols * 2.5, nrows * 2)\n",
    "\n",
    "    # --- Create Subplots ---\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "\n",
    "    # Flatten axes array for easy iteration, handle cases with single row/col\n",
    "    if isinstance(axes, plt.Axes): # Single subplot case (nrows=1, ncols=1)\n",
    "        axes_flat = [axes]\n",
    "    else:\n",
    "        axes_flat = axes.flat # Flatten the 2D array of axes\n",
    "\n",
    "    # --- Prepare Data for Plotting ---\n",
    "    plot_cmap = None\n",
    "    if channels == 1:\n",
    "        plot_cmap = cmap # Use provided cmap for grayscale\n",
    "        # Remove channel dim for imshow if it exists and is 1\n",
    "        sequence_to_plot = np.squeeze(sequence, axis=-1)\n",
    "    elif channels == 3:\n",
    "         sequence_to_plot = sequence # imshow handles RGB (expects float 0-1 or int 0-255)\n",
    "         # Ensure data range is suitable for imshow if it's float\n",
    "         if sequence_to_plot.dtype == np.float32 or sequence_to_plot.dtype == np.float64:\n",
    "              sequence_to_plot = np.clip(sequence_to_plot, 0, 1)\n",
    "    else:\n",
    "        # Handle other channel numbers? Plot the first channel as grayscale.\n",
    "        print(f\"Warning: Sequence has {channels} channels. Plotting the first channel as grayscale.\")\n",
    "        plot_cmap = cmap\n",
    "        sequence_to_plot = sequence[..., 0] # Take the first channel\n",
    "\n",
    "    # Determine global vmin/vmax for consistent coloring across grayscale timesteps\n",
    "    vmin = np.min(sequence_to_plot)\n",
    "    vmax = np.max(sequence_to_plot)\n",
    "\n",
    "    # --- Plot Each Timestep ---\n",
    "    for t in range(num_timesteps):\n",
    "        ax = axes_flat[t] # Get the current subplot axis\n",
    "        ax.imshow(sequence_to_plot[t], cmap=plot_cmap, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"t={t}\")\n",
    "        ax.axis('off') # Hide axes ticks and labels\n",
    "\n",
    "    # --- Hide Unused Subplots ---\n",
    "    # If nrows * ncols > num_timesteps, hide the remaining axes\n",
    "    for t in range(num_timesteps, len(axes_flat)):\n",
    "         axes_flat[t].axis('off')\n",
    "\n",
    "    # --- Final Touches ---\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    # Adjust layout to prevent titles/labels overlapping and make space for suptitle\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ],
   "id": "0d1d603a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMKcBXCThvCu"
   },
   "outputs": [],
   "source": [
    "for element in train_ds_for_model_augmented.take(1):\n",
    "  plot_sequence_grid(element[0][\"Input_All_Images\"])"
   ],
   "id": "dc1a026e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lptb4zfQsywN"
   },
   "source": [
    "## Run the model"
   ],
   "id": "8d1ed1d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9e6VPgoi6pb"
   },
   "outputs": [],
   "source": [
    "learning_rate_scheduler = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = INITIAL_LEARNING_RATE,\n",
    "    decay_steps = INDIVIDUALS // BATCH_SIZE * DECAY_EPOCHS,\n",
    "    alpha = DECAY_ALPHA,\n",
    "    warmup_target = LEARNING_RATE,\n",
    "    warmup_steps = INDIVIDUALS // BATCH_SIZE * WARMUP_EPOCHS\n",
    ")"
   ],
   "id": "36eb8f7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPT1PjTv4tNv"
   },
   "outputs": [],
   "source": [
    "mask_model = create_masked_model()\n",
    "mask_model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=learning_rate_scheduler),\n",
    "    loss=normalized_weighted_euc_dist,\n",
    "    jit_compile=True,\n",
    ")\n",
    "mask_model.summary()"
   ],
   "id": "30bf0618"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QWxiUiBMmue"
   },
   "outputs": [],
   "source": [
    "mask_model.fit(train_ds_for_model_augmented.batch(BATCH_SIZE), epochs=TRAIN_EPOCHS, callbacks=[WandbMetricsLogger()])"
   ],
   "id": "d424421e"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzxl5IX0ycoR"
   },
   "source": [
    "# Save and export"
   ],
   "id": "85fbd911"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVk3s8vQRFpl"
   },
   "outputs": [],
   "source": [
    "mask_model.save('full_model.keras')"
   ],
   "id": "50ac1fcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EI9SocMbzC5p"
   },
   "outputs": [],
   "source": [
    "mask_model.save_weights('full_model.weights.h5')"
   ],
   "id": "2bcef33a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZy4iASiRJvi"
   },
   "outputs": [],
   "source": [
    "wandb.save('full_model.keras')\n",
    "wandb.save('full_model.weights.h5')"
   ],
   "id": "d2f49487"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nx0NflgbDi1A"
   },
   "outputs": [],
   "source": [
    "masked_dataset = prepare_masked_dataset(train_data_rescaled, calibration_points=scaled_cal_points)"
   ],
   "id": "30e8ff13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCx68NkMCzql"
   },
   "outputs": [],
   "source": [
    "test_ds = masked_dataset.map(\n",
    "    prepare_model_inputs,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)"
   ],
   "id": "8804c12b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGFrhP80C0XV"
   },
   "outputs": [],
   "source": [
    "test_ds.element_spec"
   ],
   "id": "9042bce2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_OUDhetR5LdO"
   },
   "outputs": [],
   "source": [
    "predictions = mask_model.predict(test_ds.batch(1))\n",
    "\n",
    "y_true = np.zeros(predictions.shape, dtype=np.float32)\n",
    "\n",
    "i = 0\n",
    "for e in test_ds.as_numpy_iterator():\n",
    "  y_true[i,:,:] = e[1]\n",
    "  i = i+1\n",
    "# y_true = np.array(y_true).reshape(-1, 144, 2)\n",
    "\n",
    "# # this step is slower than expected. not sure what's going on.\n",
    "# predictions = full_model.predict(test_ds.)\n",
    "\n",
    "batch_losses = np.zeros((predictions.shape[0], predictions.shape[1]))\n",
    "for i in range(len(predictions)):\n",
    "  loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
    "  batch_losses[i,:] = loss\n",
    "\n",
    "# Get mean per subject\n",
    "batch_losses = np.mean(batch_losses, axis=1)"
   ],
   "id": "b57b5508"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvKzpmrI9PBO"
   },
   "outputs": [],
   "source": [
    "table = wandb.Table(data=[[s] for s in batch_losses], columns=[\"Loss\"])\n",
    "final_loss_hist = wandb.plot.histogram(table, value=\"Loss\", title=\"Normalized Euclidean Distance\")"
   ],
   "id": "00226ff1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9iK_69wdrZ0"
   },
   "outputs": [],
   "source": [
    "final_loss_mean = np.mean(batch_losses)"
   ],
   "id": "a5453bf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk8z0iV4DV2d"
   },
   "outputs": [],
   "source": [
    "wandb.log({\"final_val_loss_hist\": final_loss_hist, \"final_loss_mean\": final_loss_mean})"
   ],
   "id": "6d463446"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blgWDH3E5Swd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(batch_losses, bins=20, edgecolor='black')\n",
    "plt.title('Histogram of Batch Losses')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "40ed54d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXAdVnFKMjg7"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ],
   "id": "0b6cade4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuePQAbwTyeS"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ],
   "id": "ebb7380b"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}