{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LAJHmSmF84"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW4fMCshWBrD"
      },
      "outputs": [],
      "source": [
        "!pip install osfclient --quiet\n",
        "!pip install git+https://github.com/jspsych/eyetracking-utils.git --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install --upgrade keras --quiet\n",
        "!pip install keras-hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQRQZ3D5Vwqc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import keras\n",
        "import keras_hub\n",
        "from keras import ops\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "import et_util.dataset_utils as dataset_utils\n",
        "import et_util.embedding_preprocessing as embed_pre\n",
        "import et_util.model_layers as model_layers\n",
        "from et_util import experiment_utils\n",
        "from et_util.custom_loss import normalized_weighted_euc_dist\n",
        "from et_util.model_analysis import plot_model_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BUzHdPEc-ef"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
        "os.environ['OSF_USERNAME'] = userdata.get('osfusername')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.version()"
      ],
      "metadata": {
        "id": "FKOfeTxzYAKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3M9X95jV8du"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9v9bxBcHc0"
      },
      "source": [
        "# Configure W&B experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SPGBdleCfbe"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWWKsFf6p1HJ"
      },
      "outputs": [],
      "source": [
        "# Fixed constants\n",
        "MAX_TARGETS = 144\n",
        "\n",
        "# Config constants\n",
        "EMBEDDING_DIM = 200\n",
        "RIDGE_REGULARIZATION = 0.1\n",
        "MIN_CAL_POINTS = 8\n",
        "MAX_CAL_POINTS = 40\n",
        "\n",
        "BACKBONE = \"densenet\"\n",
        "\n",
        "AUGMENTATION = True\n",
        "\n",
        "INITIAL_LEARNING_RATE = 0.00001\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 5\n",
        "TRAIN_EPOCHS = 50\n",
        "WARMUP_EPOCHS = 2\n",
        "DECAY_EPOCHS = TRAIN_EPOCHS - WARMUP_EPOCHS\n",
        "DECAY_ALPHA = 0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkHFv5kHfaWR"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"ridge_regularization\": RIDGE_REGULARIZATION,\n",
        "    \"train_epochs\": TRAIN_EPOCHS,\n",
        "    \"min_cal_points\": MIN_CAL_POINTS,\n",
        "    \"max_cal_points\": MAX_CAL_POINTS,\n",
        "    \"backbone\": BACKBONE,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"initial_learning_rate\": INITIAL_LEARNING_RATE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"augmentation\": AUGMENTATION,\n",
        "    \"warmup_epochs\": WARMUP_EPOCHS,\n",
        "    \"decay_epochs\": DECAY_EPOCHS,\n",
        "    \"decay_alpha\": DECAY_ALPHA,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjDFWXNEddE9"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project='eye-tracking-dense-full-data-set-single-eye',\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preparation"
      ],
      "metadata": {
        "id": "ERTCx1sP0XGv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeHGiFxtvo0"
      },
      "source": [
        "## Download dataset from OSF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tswV6s38WbtO"
      },
      "outputs": [],
      "source": [
        "!osf -p 6b5cd fetch single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgWOhWTt4iD"
      },
      "source": [
        "## Process raw data records into TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4h8QhGvWhZf"
      },
      "outputs": [],
      "source": [
        "!mkdir single_eye_tfrecords\n",
        "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZRcQgPadqtI"
      },
      "outputs": [],
      "source": [
        "def parse(element):\n",
        "    \"\"\"Process function that parses a tfr element in a raw dataset for process_tfr_to_tfds function.\n",
        "    Gets mediapipe landmarks, raw image, image width, image height, subject id, and xy labels.\n",
        "    Use for data generated with make_single_example_landmarks_and_jpg (i.e. data in\n",
        "    jpg_landmarks_tfrecords.tar.gz)\n",
        "\n",
        "    :param element: tfr element in raw dataset\n",
        "    :return: image, label(x,y), landmarks, subject_id\n",
        "    \"\"\"\n",
        "\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    landmarks = content['landmarks']\n",
        "    raw_image = content['eye_img']\n",
        "    width = content['img_width']\n",
        "    height = content['img_height']\n",
        "    depth = 3\n",
        "    label = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n",
        "    landmarks = tf.reshape(landmarks, shape=(478, 3))\n",
        "\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, landmarks, label, subject_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWv57eb5XHVP"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data, test_data = dataset_utils.process_tfr_to_tfds(\n",
        "    'single_eye_tfrecords/',\n",
        "    parse,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, landmarks, coords, z: z\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apcLK9klHLQV"
      },
      "outputs": [],
      "source": [
        "def rescale_coords_map(eyes, mesh, coords, id):\n",
        "  return eyes, mesh, tf.divide(coords, tf.constant([100.])), id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b47CtWKqHg0B"
      },
      "outputs": [],
      "source": [
        "train_data_rescaled = train_data.map(rescale_coords_map)\n",
        "validation_data_rescaled = validation_data.map(rescale_coords_map)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_masked_dataset(dataset, calibration_points=None):\n",
        "\n",
        "    # Step 1: Group dataset by subject_id and batch all images\n",
        "    def group_by_subject(subject_id, ds):\n",
        "        return ds.batch(batch_size=MAX_TARGETS)\n",
        "\n",
        "    grouped_dataset = dataset.group_by_window(\n",
        "        key_func=lambda img, mesh, coords, subject_id: subject_id,\n",
        "        reduce_func=group_by_subject,\n",
        "        window_size=MAX_TARGETS\n",
        "    )\n",
        "\n",
        "    # Step 2: Filter out subjects with fewer than 72 images\n",
        "    def filter_by_image_count(images, meshes, coords, subject_ids):\n",
        "        return tf.shape(images)[0] >= 144\n",
        "\n",
        "    grouped_dataset = grouped_dataset.filter(filter_by_image_count)\n",
        "\n",
        "    # Step 3: Transform each batch to include masks\n",
        "    def add_masks_to_batch(images, meshes, coords, subject_ids):\n",
        "\n",
        "        actual_batch_size = tf.shape(images)[0]\n",
        "\n",
        "        cal_mask = tf.zeros(MAX_TARGETS, dtype=tf.int8)\n",
        "        target_mask = tf.zeros(MAX_TARGETS, dtype=tf.int8)\n",
        "\n",
        "        # Determine how many calibration images to use (random between min and max)\n",
        "        if calibration_points is None:\n",
        "          n_cal_images = tf.random.uniform(\n",
        "              shape=[],\n",
        "              minval=MIN_CAL_POINTS,\n",
        "              maxval=MAX_CAL_POINTS,\n",
        "              dtype=tf.int32\n",
        "          )\n",
        "          # Create random indices for calibration images\n",
        "          # NOTE: These will be different each time through the dataset\n",
        "          random_indices = tf.random.shuffle(tf.range(actual_batch_size))\n",
        "          cal_indices = random_indices[:n_cal_images]\n",
        "\n",
        "          # Create masks (1 = included, 0 = excluded)\n",
        "          cal_mask = tf.scatter_nd(\n",
        "              tf.expand_dims(cal_indices, 1),\n",
        "              tf.ones(n_cal_images, dtype=tf.int8),\n",
        "              [MAX_TARGETS]\n",
        "          )\n",
        "        else:\n",
        "          coords_xpand = tf.expand_dims(coords, axis=1)\n",
        "          cal_xpand = tf.expand_dims(calibration_points, axis=0)\n",
        "          equality = tf.equal(coords_xpand, cal_xpand)\n",
        "          matches = tf.reduce_all(equality, axis=-1)\n",
        "          point_matches = tf.reduce_any(matches, axis=1)\n",
        "          cal_mask = tf.cast(point_matches, dtype=tf.int8)\n",
        "\n",
        "\n",
        "\n",
        "        target_mask = 1 - cal_mask\n",
        "\n",
        "        # Pad everything to fixed size\n",
        "\n",
        "        padded_images = tf.pad(\n",
        "            tf.reshape(images, (-1, 36, 144, 1)),\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0], [0, 0], [0, 0]]\n",
        "        )\n",
        "        padded_coords = tf.pad(\n",
        "            coords,\n",
        "            [[0, MAX_TARGETS - actual_batch_size], [0, 0]]\n",
        "        )\n",
        "\n",
        "        # Ensure all shapes are fixed\n",
        "        padded_images = tf.ensure_shape(padded_images, [MAX_TARGETS, 36, 144, 1])\n",
        "        padded_coords = tf.ensure_shape(padded_coords, [MAX_TARGETS, 2])\n",
        "        padded_cal_mask = tf.ensure_shape(cal_mask, [MAX_TARGETS])\n",
        "        padded_target_mask = tf.ensure_shape(target_mask, [MAX_TARGETS])\n",
        "        return (padded_images, padded_coords, padded_cal_mask, padded_target_mask), padded_coords, subject_ids\n",
        "\n",
        "    # Apply the transformation\n",
        "    masked_dataset = grouped_dataset.map(\n",
        "        lambda imgs, meshes, coords, subj_ids: add_masks_to_batch(imgs, meshes, coords, subj_ids),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return masked_dataset\n",
        "\n",
        "# Modified model input preparation function\n",
        "def prepare_model_inputs(features, labels, subject_ids):\n",
        "    \"\"\"\n",
        "    Restructure the inputs for the model, using the mask-based approach.\n",
        "\n",
        "    Args:\n",
        "        features: Tuple of (images, coords, cal_mask, target_mask)\n",
        "        labels: The target coordinates\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of inputs for the model and the target labels\n",
        "    \"\"\"\n",
        "    images, coords, cal_mask, target_mask = features\n",
        "\n",
        "    # Use masking to create the model inputs\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": images,                   # All images (both cal and target)\n",
        "        \"Input_All_Coords\": coords,                   # All coordinates\n",
        "        \"Input_Calibration_Mask\": cal_mask,           # Mask indicating calibration images\n",
        "    }\n",
        "\n",
        "    return inputs, labels, target_mask # target_mask is used as sample weights for loss function"
      ],
      "metadata": {
        "id": "2-AX95Hj7_mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))"
      ],
      "metadata": {
        "id": "caGtgpGYHlAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_dataset = prepare_masked_dataset(train_data_rescaled)"
      ],
      "metadata": {
        "id": "eTBbOOV52Uqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_for_model = masked_dataset.map(\n",
        "    prepare_model_inputs,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").shuffle(200).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "21AN7OpI4ee2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INDIVIDUALS = 0\n",
        "for e in train_ds_for_model.as_numpy_iterator():\n",
        "  INDIVIDUALS += 1"
      ],
      "metadata": {
        "id": "hlsnVzhWl3Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "V3trgiBOS1A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from IPython.display import HTML\n",
        "import base64\n",
        "\n",
        "def visualize_eye_tracking_data(dataset, interval=100, figsize=(10, 6)):\n",
        "    \"\"\"\n",
        "    Visualize eye tracking data by animating through a subject's images and displaying\n",
        "    where they are looking with a red dot. Returns an HTML animation for direct display\n",
        "    in a Colab notebook.\n",
        "\n",
        "    Args:\n",
        "        dataset: A TensorFlow dataset containing a single batch of one subject's data\n",
        "                Expected format: ({\"Input_All_Images\": images, \"Input_All_Coords\": coords,\n",
        "                                 \"Input_Calibration_Mask\": cal_mask, \"Input_Target_Mask\": target_mask}, labels)\n",
        "                OR: Your model's dataset with one subject batch\n",
        "        interval: Time between frames in milliseconds\n",
        "        figsize: Size of the figure (width, height)\n",
        "\n",
        "    Returns:\n",
        "        IPython.display.HTML object with the animation\n",
        "\n",
        "    Example usage in a Colab notebook:\n",
        "    ```python\n",
        "    # For a dataset from a single subject\n",
        "    subject_dataset = train_ds_for_model.take(1)\n",
        "\n",
        "    # Or you can create a dataset for just one subject (example from your notebook):\n",
        "    # This line extracts data for one subject from the complete dataset\n",
        "    single_subject = train_ds_for_model.filter(\n",
        "        lambda inputs, labels: tf.equal(inputs[\"Input_Subject_ID\"][0], subject_id)\n",
        "    ).take(1)\n",
        "\n",
        "    # Display the animation directly in the notebook\n",
        "    from IPython.display import display\n",
        "    animation = visualize_eye_tracking_data(single_subject)\n",
        "    display(animation)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    # Extract data from the dataset\n",
        "    for inputs, labels, _ in dataset.take(1):\n",
        "        images = inputs[\"Input_All_Images\"]\n",
        "        coords = inputs[\"Input_All_Coords\"]\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        valid_images = images.numpy()\n",
        "        valid_coords = coords.numpy()\n",
        "\n",
        "        # Sort by X coordinate, then by Y coordinate\n",
        "        sorted_indices = np.lexsort((valid_coords[:, 0], valid_coords[:, 1]))\n",
        "        valid_images = valid_images[sorted_indices]\n",
        "        valid_coords = valid_coords[sorted_indices]\n",
        "\n",
        "        num_images = valid_images.shape[0]\n",
        "\n",
        "    # Set up the figure\n",
        "    plt.ioff()  # Turn off interactive mode to avoid displaying during generation\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Create a function that draws each frame\n",
        "    def draw_frame(frame_num):\n",
        "        ax.clear()\n",
        "\n",
        "        # Set up the 16:9 coordinate space\n",
        "        ax.set_xlim(0, 100)\n",
        "        ax.set_ylim(100, 0)  # 16:9 aspect ratio\n",
        "\n",
        "        # Get the current image and coordinates\n",
        "        img = valid_images[frame_num].squeeze()\n",
        "        x, y = valid_coords[frame_num]\n",
        "\n",
        "        # Draw rectangle for the screen\n",
        "        rect = plt.Rectangle((0, 0), 100, 100, fill=False, color='black', linewidth=2)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Add image display in the top right corner\n",
        "        img_display = ax.inset_axes([0.35, 0.35, 0.3, 0.3], transform=ax.transAxes)\n",
        "        img_display.imshow(np.fliplr(img), cmap='gray')\n",
        "        img_display.axis('off')\n",
        "\n",
        "        # Draw red dot at the coordinate - make it bigger for visibility\n",
        "        ax.scatter(x * 100, y * 100, color='red', s=150, zorder=5,\n",
        "                  edgecolor='white', linewidth=1.5)\n",
        "\n",
        "        # Draw crosshair\n",
        "        ax.axhline(y * 100, color='gray', linestyle='--', alpha=0.5, zorder=1)\n",
        "        ax.axvline(x * 100, color='gray', linestyle='--', alpha=0.5, zorder=1)\n",
        "\n",
        "        # Set title\n",
        "        ax.set_title('Eye Tracking Visualization')\n",
        "\n",
        "        # Set axis labels\n",
        "        ax.set_xlabel('X Coordinate')\n",
        "        ax.set_ylabel('Y Coordinate')\n",
        "\n",
        "    # Create the animation\n",
        "    anim = animation.FuncAnimation(fig, draw_frame, frames=num_images, interval=interval)\n",
        "\n",
        "  # Use animation's to_jshtml method which directly generates HTML\n",
        "    html_animation = anim.to_jshtml()\n",
        "\n",
        "    # Clean up\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Create an HTML object that can be displayed in the notebook\n",
        "    return HTML(html_animation)\n"
      ],
      "metadata": {
        "id": "glQLYELqaOg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_eye_tracking_data(train_ds_for_model.take(1))"
      ],
      "metadata": {
        "id": "oB2m-VExaRSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gMGmIObc0e8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model building"
      ],
      "metadata": {
        "id": "0UfY_C8zMW8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Layers\n"
      ],
      "metadata": {
        "id": "fIjimxPW0kMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTimeDistributed(keras.layers.Wrapper):\n",
        "    \"\"\"A simplified version of TimeDistributed that applies a layer to every temporal slice of an input.\n",
        "\n",
        "    This implementation avoids for loops by using reshape operations to apply the wrapped layer\n",
        "    to all time steps at once.\n",
        "\n",
        "    Args:\n",
        "        layer: a `keras.layers.Layer` instance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.supports_masking = getattr(layer, 'supports_masking', False)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Validate input shape has at least 3 dimensions (batch, time, ...)\n",
        "        if not isinstance(input_shape, (tuple, list)) or len(input_shape) < 3:\n",
        "            raise ValueError(\n",
        "                \"`SimpleTimeDistributed` requires input with at least 3 dimensions\"\n",
        "            )\n",
        "\n",
        "        # Build the wrapped layer with shape excluding the time dimension\n",
        "        super().build((input_shape[0], *input_shape[2:]))\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # Get output shape by applying the layer to a single time slice\n",
        "        child_output_shape = self.layer.compute_output_shape((input_shape[0], *input_shape[2:]))\n",
        "        # Include time dimension in the result\n",
        "        return (child_output_shape[0], input_shape[1], *child_output_shape[1:])\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        time_steps = input_shape[1]\n",
        "\n",
        "        # Reshape inputs to combine batch and time dimensions: (batch*time, ...)\n",
        "        reshaped_inputs = ops.reshape(inputs, (-1, *input_shape[2:]))\n",
        "\n",
        "        # Apply the layer to all time steps at once\n",
        "        outputs = self.layer.call(reshaped_inputs, training=training)\n",
        "\n",
        "        # Get output dimensions\n",
        "        output_shape = ops.shape(outputs)\n",
        "\n",
        "        # Reshape back to include the separate batch and time dimensions: (batch, time, ...)\n",
        "        return ops.reshape(outputs, (batch_size, time_steps, *output_shape[1:]))"
      ],
      "metadata": {
        "id": "JCx-whu4HlDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuCld98e7vAZ"
      },
      "outputs": [],
      "source": [
        "# cal_points = tf.constant([\n",
        "#     [5, 5],\n",
        "#     [5, 27.5],\n",
        "#     [5, 50],\n",
        "#     [5, 72.5],\n",
        "#     [5, 95],\n",
        "#     [35, 5],\n",
        "#     [35, 27.5],\n",
        "#     [35, 50],\n",
        "#     [35, 72.5],\n",
        "#     [35, 95],\n",
        "#     [65, 5],\n",
        "#     [65, 27.5],\n",
        "#     [65, 50],\n",
        "#     [65, 72.5],\n",
        "#     [65, 95],\n",
        "#     [95, 5],\n",
        "#     [95, 27.5],\n",
        "#     [95, 50],\n",
        "#     [95, 72.5],\n",
        "#     [95, 95],\n",
        "# ], dtype=tf.float32)\n",
        "\n",
        "# scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedWeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom layer that performs weighted ridge regression with proper masking support.\n",
        "\n",
        "    This layer takes embeddings, coordinates, weights, and calibration mask as explicit inputs,\n",
        "    while using Keras' masking system to handle target masking. This separation allows more\n",
        "    precise control over calibration points while leveraging Keras' built-in mask propagation\n",
        "    for target predictions.\n",
        "\n",
        "    Args:\n",
        "        lambda_ridge (float): Regularization parameter for ridge regression\n",
        "        embedding_dim (int): Dimension of the embedding vectors\n",
        "        epsilon (float): Small constant for numerical stability inside sqrt\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_ridge, epsilon=1e-7, **kwargs): # Added epsilon argument\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon # Store epsilon\n",
        "        super(MaskedWeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        \"\"\"\n",
        "        The forward pass of the layer.\n",
        "\n",
        "        Args:\n",
        "            inputs: A list containing:\n",
        "                - embeddings: Embeddings for all points (batch_size, n_points, embedding_dim)\n",
        "                - coords: Coordinates for all points (batch_size, n_points, 2)\n",
        "                - calibration_weights: Importance weights (batch_size, n_points, 1)\n",
        "                - cal_mask: Mask for calibration points (batch_size, n_points) [EXPLICIT]\n",
        "\n",
        "        Returns:\n",
        "            Predicted coordinates for the target points (batch_size, n_points, 2)\n",
        "        \"\"\"\n",
        "        # Unpack inputs\n",
        "        embeddings, coords, calibration_weights, cal_mask  = inputs\n",
        "\n",
        "        # Ensure correct dtype, especially important for JIT\n",
        "        embeddings = ops.cast(embeddings, \"float32\")\n",
        "        coords = ops.cast(coords, \"float32\")\n",
        "        calibration_weights = ops.cast(calibration_weights, \"float32\")\n",
        "        cal_mask = ops.cast(cal_mask, \"float32\")\n",
        "\n",
        "        # reshape weights to (batch, calibration)\n",
        "        w = ops.squeeze(calibration_weights, axis=-1)\n",
        "\n",
        "        # Pre-compute masked weights for calibration points\n",
        "        w_masked = w * cal_mask\n",
        "        # Add epsilon inside sqrt for numerical stability\n",
        "        w_sqrt = ops.sqrt(w_masked + self.epsilon)\n",
        "        w_sqrt = ops.expand_dims(w_sqrt, -1)  # More efficient than reshape\n",
        "\n",
        "        # Apply calibration mask to embeddings\n",
        "        cal_mask_expand = ops.expand_dims(cal_mask, -1)\n",
        "        X = embeddings * cal_mask_expand # Mask out non-calibration embeddings for regression calculation\n",
        "\n",
        "        # Weight calibration embeddings and coordinates using the masked weights\n",
        "        X_weighted = X * w_sqrt\n",
        "        y_weighted = coords * w_sqrt * cal_mask_expand # Also mask coords just to be safe\n",
        "\n",
        "        # Matrix operations\n",
        "        X_t = ops.transpose(X_weighted, axes=[0, 2, 1])\n",
        "        X_t_X = ops.matmul(X_t, X_weighted)\n",
        "\n",
        "        # Add regularization - CORRECTED: Removed the extra * 1e-3\n",
        "        # Ensure the identity matrix is also float32\n",
        "        identity_matrix = ops.cast(ops.eye(ops.shape(embeddings)[-1]), \"float32\")\n",
        "        lhs = X_t_X + self.lambda_ridge * identity_matrix\n",
        "\n",
        "        # Compute RHS\n",
        "        rhs = ops.matmul(X_t, y_weighted)\n",
        "\n",
        "        # Solve the system\n",
        "        # Consider adding checks or alternative solvers if instability persists,\n",
        "        # but correcting the regularization should be the primary fix.\n",
        "        kernel = ops.linalg.solve(lhs, rhs)\n",
        "\n",
        "        # Apply regression using the *original* full embeddings\n",
        "        output = ops.matmul(embeddings, kernel)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        \"\"\"\n",
        "        Computes the output shape of the layer.\n",
        "\n",
        "        Args:\n",
        "            input_shapes: List of input shapes\n",
        "\n",
        "        Returns:\n",
        "            Output shape tuple\n",
        "        \"\"\"\n",
        "        # Output shape matches coordinates: (batch_size, n_points, 2)\n",
        "        return (input_shapes[0][0], input_shapes[0][1], 2)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration of the layer for serialization.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing the layer configuration\n",
        "        \"\"\"\n",
        "        config = super(MaskedWeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"lambda_ridge\": self.lambda_ridge,\n",
        "            \"epsilon\": self.epsilon # Add epsilon to config\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "wbWzR7yfMZv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskInspectorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MaskInspectorLayer, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "    def call(self, inputs, mask=None):\n",
        "      # Print mask information\n",
        "      tf.print(\"Layer mask:\", mask)\n",
        "      return inputs"
      ],
      "metadata": {
        "id": "vt7x7bN6EzGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom loss"
      ],
      "metadata": {
        "id": "fsb8-Q2T0pkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalized_weighted_euc_dist(y_true, y_pred):\n",
        "    \"\"\"Custom loss function that calculates a weighted Euclidean distance between two sets of points,\n",
        "    respecting masks for padded sequences.\n",
        "\n",
        "    Weighting multiplies the x-coordinate of each input by 1.778 (derived from the 16:9 aspect ratio of most laptops)\n",
        "    to match the scale of the y-axis. For interpretability, the distances are normalized to the diagonal\n",
        "    such that the maximum distance between two points (corner to corner) is 100.\n",
        "\n",
        "    :param y_true (tensor): A tensor of shape (seq_len, 2) containing ground-truth x- and y- coordinates\n",
        "    :param y_pred (tensor): A tensor of shape (seq_len, 2) containing predicted x- and y- coordinates\n",
        "\n",
        "    :returns: A tensor of shape (1,) with the weighted and normalized euclidean distances between valid points.\n",
        "    \"\"\"\n",
        "    # Weighting treats y-axis as unit-scale and creates a rectangle that's 177.8x100 units.\n",
        "    x_weight = ops.convert_to_tensor([1.778, 1.0], dtype=\"float32\")\n",
        "\n",
        "    # Multiply x-coordinate by 16/9 = 1.778\n",
        "    y_true_weighted = ops.multiply(x_weight, y_true)\n",
        "    y_pred_weighted = ops.multiply(x_weight, y_pred)\n",
        "\n",
        "    # Calculate Euclidean distance with weighted coordinates\n",
        "    squared_diff = ops.square(y_pred_weighted - y_true_weighted)\n",
        "    squared_dist = ops.sum(squared_diff, axis=-1)\n",
        "    dist = ops.sqrt(squared_dist)\n",
        "\n",
        "    # Euclidean Distance from [0,0] to [1.778, 1.00] = 2.03992\n",
        "    # Divide by 2.03992, mult by 100 is same as / .0203992\n",
        "    # Normalizes loss values to the diagonal-- makes loss easier to interpret\n",
        "    normalized_dist = ops.divide(dist, .0203992)\n",
        "\n",
        "    return normalized_dist"
      ],
      "metadata": {
        "id": "y5Kghim-rVhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Model"
      ],
      "metadata": {
        "id": "Y3a-BpZNsvUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbones"
      ],
      "metadata": {
        "id": "JY_hBgtj1R9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DenseNet"
      ],
      "metadata": {
        "id": "vibW_5HzlOzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dense_net_backbone():\n",
        "  DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "  return keras_hub.models.DenseNetBackbone(\n",
        "      stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "      image_shape=(36, 144, 1),\n",
        "  )"
      ],
      "metadata": {
        "id": "9v9uaV4h1atr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Involution"
      ],
      "metadata": {
        "id": "Zn-kyypalQbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Involution(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self, channel, group_number, kernel_size, stride, reduction_ratio\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the parameters.\n",
        "        self.channel = channel\n",
        "        self.group_number = group_number\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Get the shape of the input.\n",
        "        (_, height, width, num_channels) = input_shape\n",
        "\n",
        "        # Scale the height and width with respect to the strides.\n",
        "        height = height // self.stride\n",
        "        width = width // self.stride\n",
        "\n",
        "        # Define a layer that average pools the input tensor\n",
        "        # if stride is more than 1.\n",
        "        self.stride_layer = (\n",
        "            keras.layers.AveragePooling2D(\n",
        "                pool_size=self.stride, strides=self.stride, padding=\"same\"\n",
        "            )\n",
        "            if self.stride > 1\n",
        "            else tf.identity\n",
        "        )\n",
        "        # Define the kernel generation layer.\n",
        "        self.kernel_gen = keras.Sequential(\n",
        "            [\n",
        "                keras.layers.Conv2D(\n",
        "                    filters=self.channel // self.reduction_ratio, kernel_size=1\n",
        "                ),\n",
        "                keras.layers.BatchNormalization(),\n",
        "                keras.layers.ReLU(),\n",
        "                keras.layers.Conv2D(\n",
        "                    filters=self.kernel_size * self.kernel_size * self.group_number,\n",
        "                    kernel_size=1,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        # Define reshape layers\n",
        "        self.kernel_reshape = keras.layers.Reshape(\n",
        "            target_shape=(\n",
        "                height,\n",
        "                width,\n",
        "                self.kernel_size * self.kernel_size,\n",
        "                1,\n",
        "                self.group_number,\n",
        "            )\n",
        "        )\n",
        "        self.input_patches_reshape = keras.layers.Reshape(\n",
        "            target_shape=(\n",
        "                height,\n",
        "                width,\n",
        "                self.kernel_size * self.kernel_size,\n",
        "                num_channels // self.group_number,\n",
        "                self.group_number,\n",
        "            )\n",
        "        )\n",
        "        self.output_reshape = keras.layers.Reshape(\n",
        "            target_shape=(height, width, num_channels)\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        # Generate the kernel with respect to the input tensor.\n",
        "        # B, H, W, K*K*G\n",
        "        kernel_input = self.stride_layer(x)\n",
        "        kernel = self.kernel_gen(kernel_input)\n",
        "\n",
        "        # reshape the kerenl\n",
        "        # B, H, W, K*K, 1, G\n",
        "        kernel = self.kernel_reshape(kernel)\n",
        "\n",
        "        # Extract input patches.\n",
        "        # B, H, W, K*K*C\n",
        "        input_patches = keras.ops.image.extract_patches(\n",
        "            images=x,\n",
        "            size=self.kernel_size,\n",
        "            strides=self.stride,\n",
        "            dilation_rate=1,\n",
        "            padding=\"same\",\n",
        "        )\n",
        "\n",
        "        # Reshape the input patches to align with later operations.\n",
        "        # B, H, W, K*K, C//G, G\n",
        "        input_patches = self.input_patches_reshape(input_patches)\n",
        "\n",
        "        # Compute the multiply-add operation of kernels and patches.\n",
        "        # B, H, W, K*K, C//G, G\n",
        "        output = keras.ops.multiply(kernel, input_patches)\n",
        "        # B, H, W, C//G, G\n",
        "        output = keras.ops.sum(output, axis=3)\n",
        "\n",
        "        # Reshape the output kernel.\n",
        "        # B, H, W, C\n",
        "        output = self.output_reshape(output)\n",
        "\n",
        "        # Return the output tensor and the kernel.\n",
        "        return output #, kernel"
      ],
      "metadata": {
        "id": "opzvkgz-1zlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rednet_block(x, channels, kernel_size=3, stride=1, groups=8, reduction_ratio=4):\n",
        "    # Save input for residual connection\n",
        "    residual = keras.layers.Conv2D(channels, kernel_size=1, strides=stride, padding='same')(x)\n",
        "\n",
        "    # Apply involution\n",
        "    x = Involution(\n",
        "        channel=channels,\n",
        "        group_number=groups,\n",
        "        kernel_size=kernel_size,\n",
        "        stride=stride,\n",
        "        reduction_ratio=reduction_ratio\n",
        "    )(x)\n",
        "\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "    # Add residual connection\n",
        "    x = keras.layers.Add()([x, residual])\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "cobHzKP1Gu9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rednet_backbone():\n",
        "    \"\"\"\n",
        "    Create a RedNet-based model for gaze prediction using the custom Involution layer\n",
        "    \"\"\"\n",
        "    inputs = keras.layers.Input(shape=(36,144,1))\n",
        "\n",
        "    # Initial convolution\n",
        "    x = keras.layers.Conv2D(128, kernel_size=7, strides=1, padding='same')(inputs)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.Activation('relu')(x)\n",
        "\n",
        "    # RedNet stages\n",
        "    x = create_rednet_block(x, 128, stride=1)\n",
        "    x = create_rednet_block(x, 128, stride=1)\n",
        "    x = create_rednet_block(x, 128, stride=2)\n",
        "\n",
        "    x = keras.layers.Conv2D(64, kernel_size=1, strides=1, padding='same')(x)\n",
        "\n",
        "    x = create_rednet_block(x, 64, stride=1)\n",
        "    x = create_rednet_block(x, 64, stride=1)\n",
        "    x = create_rednet_block(x, 64, stride=2)\n",
        "\n",
        "    x = keras.layers.Conv2D(32, kernel_size=1, strides=1, padding='same')(x)\n",
        "\n",
        "    # weights = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    # x = keras.layers.Multiply()([x, weights])\n",
        "    # x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "\n",
        "\n",
        "    # Model with dual outputs\n",
        "    model = keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sTzWObqwG5xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = create_rednet_backbone()"
      ],
      "metadata": {
        "id": "K2bhtd_r35nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone.summary()"
      ],
      "metadata": {
        "id": "QgNa9Gm44eJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_involution_backbone():\n",
        "  inputs = keras.layers.Input(shape=(36, 144, 1))\n",
        "  x = keras.layers.Conv2D(filters=64, kernel_size=4, padding=\"same\")(inputs)\n",
        "  x = keras.layers.ReLU()(x)\n",
        "  x = Involution(channel=64, group_number=1, kernel_size=3, stride=1, reduction_ratio=2, name=\"involution_1\")(x)\n",
        "  x = keras.layers.ReLU()(x)\n",
        "  x = Involution(channel=64, group_number=1, kernel_size=3, stride=1, reduction_ratio=2, name=\"involution_2\")(x)\n",
        "  x = keras.layers.ReLU()(x)\n",
        "  weights = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
        "  x = keras.layers.Multiply()([x, weights])\n",
        "  x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "  return keras.Model(inputs=inputs, outputs=x)"
      ],
      "metadata": {
        "id": "nLMSHz8E1jhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EfficientNetB0"
      ],
      "metadata": {
        "id": "Y6A07vCFlU5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_efficientnet_backbone():\n",
        "    \"\"\"\n",
        "    Creates an embedding model using EfficientNetB0 as the backbone.\n",
        "\n",
        "    Handles grayscale input by expanding channels before feeding into EfficientNet.\n",
        "    \"\"\"\n",
        "    image_shape = (36, 144, 1)  # Original grayscale input shape\n",
        "    input_eyes = keras.layers.Input(shape=image_shape, name=\"Input_Eye_Image\")\n",
        "\n",
        "    # --- Preprocessing ---\n",
        "    # 1. Rescale pixel values to [0, 1] (standard practice)\n",
        "\n",
        "    # 2. Expand channels from 1 (grayscale) to 3\n",
        "    # EfficientNetB0 expects 3 input channels. We can simply repeat the\n",
        "    # grayscale channel three times.\n",
        "    # A Conv2D layer could also learn this mapping, but repeating is simpler.\n",
        "    eyes_3channel = keras.layers.Concatenate(axis=-1, name=\"Expand_Channels\")(\n",
        "        [input_eyes, input_eyes, input_eyes]\n",
        "    )\n",
        "    # The shape is now (batch, 36, 144, 3)\n",
        "\n",
        "    # --- Backbone ---\n",
        "    # Load EfficientNetB0 from keras.applications\n",
        "    # - include_top=False: Remove the final classification layer.\n",
        "    # - weights=None: Train from scratch. ImageNet weights are unlikely\n",
        "    #                 to be optimal for eye images and might hinder learning.\n",
        "    # - input_shape=(36, 144, 3): Match the 3-channel input we created.\n",
        "    # - pooling=None: Get the raw feature maps from the last conv layer.\n",
        "    try:\n",
        "        backbone = keras.applications.EfficientNetB0(\n",
        "            include_top=False,\n",
        "            weights=None, # Start fresh for this specific task\n",
        "            input_shape=(36, 144, 3),\n",
        "            pooling=None, # We'll add our own pooling\n",
        "            name=\"EfficientNetB0_Backbone\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading EfficientNetB0: {e}\")\n",
        "        print(\"Ensure you have tensorflow installed correctly.\")\n",
        "        print(\"You might need to run: pip install --upgrade tensorflow keras\")\n",
        "        # As a fallback, you might try loading from TF Hub if available,\n",
        "        # but keras.applications should generally work.\n",
        "        # import tensorflow_hub as hub\n",
        "        # backbone = hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\", trainable=True)\n",
        "        # Need input preprocessing specific to the Hub model if using this.\n",
        "        raise # Re-raise the exception to stop execution if loading fails.\n",
        "\n",
        "\n",
        "    # Apply the backbone to the 3-channel input\n",
        "    backbone_model = keras.Model(inputs=input_eyes, outputs=backbone(eyes_3channel))\n",
        "    # Shape will depend on EfficientNetB0's final conv layer output size\n",
        "\n",
        "    return backbone_model"
      ],
      "metadata": {
        "id": "B3C5lYhXlW6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCSmeKFBH5YQ"
      },
      "source": [
        "Cols = 5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95\n",
        "\n",
        "Rows = 5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Embedding Model"
      ],
      "metadata": {
        "id": "j_1qYhG1y8eN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnKs983UsyDj"
      },
      "outputs": [],
      "source": [
        "def create_embedding_model(BACKBONE):\n",
        "  image_shape = (36, 144, 1)\n",
        "  input_eyes = keras.layers.Input(shape=image_shape)\n",
        "\n",
        "  eyes_rescaled = keras.layers.Rescaling(scale=1./255)(input_eyes)\n",
        "\n",
        "  # Continue with the backbone\n",
        "  # backbone = create_rednet_backbone()\n",
        "\n",
        "\n",
        "  # backbone = keras.Sequential([\n",
        "  #     keras.layers.Flatten(),\n",
        "  #     keras.layers.Dense(10, activation=\"relu\")\n",
        "  # ])\n",
        "  if BACKBONE == \"densenet\":\n",
        "    backbone = create_dense_net_backbone()\n",
        "\n",
        "  # backbone = create_efficientnet_backbone()\n",
        "\n",
        "  # backbone = keras_hub.models.MiTBackbone(\n",
        "  #   image_shape=(36,144,1),\n",
        "  #   layerwise_depths=[2,2,2,2],\n",
        "  #   num_layers=4,\n",
        "  #   layerwise_num_heads=[1,2,5,8],\n",
        "  #   layerwise_sr_ratios=[8,4,2,1],\n",
        "  #   max_drop_path_rate=0.1,\n",
        "  #   layerwise_patch_sizes=[7,3,3,3],\n",
        "  #   layerwise_strides=[4,2,2,2],\n",
        "  #   hidden_dims=[32,64,160,256]\n",
        "  # )\n",
        "\n",
        "  backbone_encoder = backbone(eyes_rescaled)\n",
        "  flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "  eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "  embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "\n",
        "  return embedding_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Model"
      ],
      "metadata": {
        "id": "OoGfwwgI03Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified model to work with the new masked input approach\n",
        "def create_masked_model():\n",
        "    \"\"\"\n",
        "    Create a model that uses masks to distinguish calibration and target images.\n",
        "\n",
        "    This implementation:\n",
        "    - Uses explicit calibration mask as a regular input\n",
        "    - Integrates target masking with Keras' masking system\n",
        "    - Properly propagates the target mask through the network\n",
        "    \"\"\"\n",
        "\n",
        "    # Define inputs\n",
        "    input_all_images = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 36, 144, 1),\n",
        "        name=\"Input_All_Images\"\n",
        "    )\n",
        "\n",
        "    input_all_coords = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS, 2),\n",
        "        name=\"Input_All_Coords\"\n",
        "    )\n",
        "\n",
        "    # Calibration mask is a regular input (not using Keras masking)\n",
        "    input_cal_mask = keras.layers.Input(\n",
        "        shape=(MAX_TARGETS,),\n",
        "        name=\"Input_Calibration_Mask\",\n",
        "    )\n",
        "\n",
        "    # Create the embedding model\n",
        "    embedding_model = create_embedding_model(BACKBONE)\n",
        "\n",
        "    # Apply the embedding model to all images\n",
        "    all_embeddings = SimpleTimeDistributed(embedding_model, name=\"Image_Embeddings\")(input_all_images)\n",
        "\n",
        "    # Calculate importance weights for calibration points\n",
        "    calibration_weights = keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\"sigmoid\",\n",
        "        name=\"Calibration_Weights\"\n",
        "    )(all_embeddings)\n",
        "\n",
        "    ridge = MaskedWeightedRidgeRegressionLayer(\n",
        "        RIDGE_REGULARIZATION,\n",
        "        name=\"Regression\"\n",
        "    )(\n",
        "        # Inputs to the regression layer\n",
        "        [\n",
        "            all_embeddings,           # Embeddings for all points\n",
        "            input_all_coords,         # Coordinates for all points\n",
        "            calibration_weights,      # Weights for calibration importance\n",
        "            input_cal_mask,            # Explicit calibration mask\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Create the full model with proper masking connections\n",
        "    full_model = keras.Model(\n",
        "        inputs=[\n",
        "            input_all_images,\n",
        "            input_all_coords,\n",
        "            input_cal_mask,\n",
        "        ],\n",
        "        outputs=ridge,\n",
        "        name=\"MaskedEyePredictionModel\"\n",
        "    )\n",
        "\n",
        "    return full_model"
      ],
      "metadata": {
        "id": "OeXnPUbC08k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdQbBaKR6vLN"
      },
      "source": [
        "# Model Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation"
      ],
      "metadata": {
        "id": "xes53rsAcBOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translation_matrix(tx, ty):\n",
        "    \"\"\"Creates a 3x3 translation matrix.\"\"\"\n",
        "    return tf.convert_to_tensor([\n",
        "        [1., 0., tx],\n",
        "        [0., 1., ty],\n",
        "        [0., 0., 1.]\n",
        "    ], dtype=tf.float32)\n",
        "\n",
        "def get_rotation_matrix(angle, center_x, center_y):\n",
        "    \"\"\"Creates a 3x3 rotation matrix around a center point.\"\"\"\n",
        "    center_x = tf.cast(center_x, tf.float32)\n",
        "    center_y = tf.cast(center_y, tf.float32)\n",
        "    cos_a = tf.cos(angle)\n",
        "    sin_a = tf.sin(angle)\n",
        "    m_rot = tf.convert_to_tensor([\n",
        "        [cos_a, -sin_a, 0.],\n",
        "        [sin_a,  cos_a, 0.],\n",
        "        [0.,     0.,    1.]\n",
        "    ], dtype=tf.float32)\n",
        "    m_trans1 = get_translation_matrix(-center_x, -center_y)\n",
        "    m_trans2 = get_translation_matrix(center_x, center_y)\n",
        "    return m_trans2 @ m_rot @ m_trans1\n",
        "\n",
        "def get_zoom_matrix(zx, zy, center_x, center_y):\n",
        "    \"\"\"Creates a 3x3 zoom matrix around a center point.\"\"\"\n",
        "    center_x = tf.cast(center_x, tf.float32)\n",
        "    center_y = tf.cast(center_y, tf.float32)\n",
        "    m_scale = tf.convert_to_tensor([\n",
        "        [zx, 0., 0.],\n",
        "        [0., zy, 0.],\n",
        "        [0., 0., 1.]\n",
        "    ], dtype=tf.float32)\n",
        "    m_trans1 = get_translation_matrix(-center_x, -center_y)\n",
        "    m_trans2 = get_translation_matrix(center_x, center_y)\n",
        "    return m_trans2 @ m_scale @ m_trans1\n",
        "\n",
        "def get_flip_matrix(horizontal, center_x, center_y):\n",
        "    \"\"\"Creates a 3x3 horizontal flip matrix around a center point.\"\"\"\n",
        "    if not horizontal:\n",
        "        return tf.eye(3, dtype=tf.float32)\n",
        "    center_x = tf.cast(center_x, tf.float32)\n",
        "    center_y = tf.cast(center_y, tf.float32)\n",
        "    m_flip = tf.convert_to_tensor([\n",
        "        [-1., 0., 0.],\n",
        "        [ 0., 1., 0.],\n",
        "        [ 0., 0., 1.]\n",
        "    ], dtype=tf.float32)\n",
        "    m_trans1 = get_translation_matrix(-center_x, -center_y)\n",
        "    m_trans2 = get_translation_matrix(center_x, center_y)\n",
        "    return m_trans2 @ m_flip @ m_trans1\n",
        "\n",
        "def matrix_to_affine_params(matrix):\n",
        "    \"\"\"Extracts the 8 parameters from a 3x3 affine matrix.\"\"\"\n",
        "    params = tf.stack([\n",
        "        matrix[0, 0], matrix[0, 1], matrix[0, 2],\n",
        "        matrix[1, 0], matrix[1, 1], matrix[1, 2],\n",
        "        matrix[2, 0], matrix[2, 1]\n",
        "    ])\n",
        "    return params"
      ],
      "metadata": {
        "id": "zsXuwWINgwb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_sequence_keras_ops_affine(all_inputs, targets, mask):\n",
        "    \"\"\"\n",
        "    Applies the same random AFFINE augmentations to all frames in a sequence\n",
        "    using keras.ops.image.affine_transform and tf.random.stateless_uniform.\n",
        "\n",
        "    Args:\n",
        "        sequence: A tensor of shape (timesteps, H, W, C).\n",
        "\n",
        "    Returns:\n",
        "        The augmented sequence tensor with the same shape.\n",
        "    \"\"\"\n",
        "\n",
        "    sequence = all_inputs[\"Input_All_Images\"]\n",
        "    input_cal_points = all_inputs[\"Input_All_Coords\"]\n",
        "    input_cal_mask = all_inputs[\"Input_Calibration_Mask\"]\n",
        "\n",
        "    sequence_shape = keras.ops.shape(sequence)\n",
        "    img_height = sequence_shape[1]\n",
        "    img_width = sequence_shape[2]\n",
        "    center_x = tf.cast(img_width, tf.float32) / 2.0\n",
        "    center_y = tf.cast(img_height, tf.float32) / 2.0\n",
        "\n",
        "    # --- Generate ONE base seed pair for this sequence ---\n",
        "    # Use tf.random.uniform (stateful) to get a UNIQUE starting point\n",
        "    # for EACH sequence processed by the map function. This works reliably.\n",
        "    base_seed = tf.random.uniform([2], minval=0, maxval=tf.int32.max, dtype=tf.int32)\n",
        "\n",
        "    # --- Generate Random Parameters using tf.random.stateless_uniform ---\n",
        "    # Note: tf.random.stateless_uniform requires shape as the first arg.\n",
        "    # 1. Flip Decision\n",
        "    flip_seed = base_seed + [0, 1] # Derive seed\n",
        "    # Use tf.random.stateless_uniform directly\n",
        "    random_flip_val = tf.random.stateless_uniform(shape=(), seed=flip_seed, minval=0.0, maxval=1.0)\n",
        "    apply_flip = random_flip_val < 0.5\n",
        "\n",
        "    # 2. Rotation Angle\n",
        "    rotation_factor = 0.05\n",
        "    max_angle = rotation_factor * math.pi\n",
        "    rotation_seed = base_seed + [0, 2] # Derive seed\n",
        "    # Use tf.random.stateless_uniform directly\n",
        "    rotation_angle = tf.random.stateless_uniform(shape=(), seed=rotation_seed, minval=-max_angle, maxval=max_angle)\n",
        "\n",
        "    # 3. Zoom Factors\n",
        "    zoom_factor_range = (0.8, 1.2)\n",
        "    zoom_h_seed = base_seed + [0, 3] # Derive seed H\n",
        "    zoom_w_seed = base_seed + [0, 4] # Derive seed W\n",
        "    # Use tf.random.stateless_uniform directly\n",
        "    zoom_height_factor = tf.random.stateless_uniform(shape=(), seed=zoom_h_seed, minval=zoom_factor_range[0], maxval=zoom_factor_range[1])\n",
        "    zoom_width_factor = tf.random.stateless_uniform(shape=(), seed=zoom_w_seed, minval=zoom_factor_range[0], maxval=zoom_factor_range[1])\n",
        "\n",
        "    # 4. Translation Shifts\n",
        "    translation_height_factor = 0.1\n",
        "    translation_width_factor = 0.1\n",
        "    shift_h_seed = base_seed + [0, 5] # Derive seed H\n",
        "    shift_w_seed = base_seed + [0, 6] # Derive seed W\n",
        "    # Use tf.random.stateless_uniform directly\n",
        "    random_shift_h = tf.random.stateless_uniform(shape=(), seed=shift_h_seed, minval=-translation_height_factor, maxval=translation_height_factor)\n",
        "    random_shift_w = tf.random.stateless_uniform(shape=(), seed=shift_w_seed, minval=-translation_width_factor, maxval=translation_width_factor)\n",
        "    shift_height = random_shift_h * tf.cast(img_height, tf.float32)\n",
        "    shift_width = random_shift_w * tf.cast(img_width, tf.float32)\n",
        "\n",
        "    # --- Build Combined Affine Matrix (Same as before) ---\n",
        "    combined_matrix = tf.eye(3, dtype=tf.float32)\n",
        "    flip_matrix = get_flip_matrix(apply_flip, center_x, center_y)\n",
        "    zoom_matrix = get_zoom_matrix(zoom_width_factor, zoom_height_factor, center_x, center_y)\n",
        "    rotation_matrix = get_rotation_matrix(rotation_angle, center_x, center_y)\n",
        "    translation_matrix = get_translation_matrix(shift_width, shift_height)\n",
        "    combined_matrix = translation_matrix @ rotation_matrix @ zoom_matrix @ flip_matrix\n",
        "\n",
        "    # --- Extract 8 parameters for Keras Op (Same as before) ---\n",
        "    affine_params = matrix_to_affine_params(combined_matrix)\n",
        "\n",
        "    # --- Apply the transformation using Keras Ops (Same as before) ---\n",
        "    # keras.ops.image.affine_transform uses the backend implementation which\n",
        "    # should handle the transformation correctly based on the calculated params.\n",
        "    augmented_sequence = keras.ops.image.affine_transform(\n",
        "        sequence,\n",
        "        affine_params,\n",
        "        interpolation=\"bilinear\",\n",
        "        fill_mode=\"constant\"\n",
        "    )\n",
        "\n",
        "    # --- Optional Color Augmentations (using tf.random.stateless_*) ---\n",
        "    # Example:\n",
        "    # brightness_seed = base_seed + [0, 7]\n",
        "    # brightness_delta = tf.random.stateless_uniform((), seed=brightness_seed, minval=-0.2, maxval=0.2)\n",
        "    # augmented_sequence = augmented_sequence + brightness_delta\n",
        "    # augmented_sequence = keras.ops.clip(augmented_sequence, 0.0, 1.0)\n",
        "\n",
        "    # contrast_seed = base_seed + [0, 8]\n",
        "    # contrast_factor = tf.random.stateless_uniform((), seed=contrast_seed, minval=0.7, maxval=1.3)\n",
        "    # pixel_mean = keras.ops.mean(augmented_sequence, axis=[1, 2], keepdims=True)\n",
        "    # augmented_sequence = (augmented_sequence - pixel_mean) * contrast_factor + pixel_mean\n",
        "    # augmented_sequence = keras.ops.clip(augmented_sequence, 0.0, 1.0)\n",
        "\n",
        "    inputs = {\n",
        "        \"Input_All_Images\": augmented_sequence,\n",
        "        \"Input_All_Coords\": input_cal_points,\n",
        "        \"Input_Calibration_Mask\": input_cal_mask,\n",
        "    }\n",
        "\n",
        "    return inputs, targets, mask"
      ],
      "metadata": {
        "id": "roV0qSaLcT4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if AUGMENTATION:\n",
        "  train_ds_for_model = train_ds_for_model.map(\n",
        "    augment_sequence_keras_ops_affine,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        "  ).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "4aif6VhRcx3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing augmentation"
      ],
      "metadata": {
        "id": "uQqSdhHwdQe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sequence_grid(sequence, title=\"Image Sequence Timesteps\", figsize=None, cmap='gray'):\n",
        "    \"\"\"\n",
        "    Plots all frames (timesteps) of an image sequence in a grid layout.\n",
        "\n",
        "    This helps visualize if the same augmentation was applied consistently\n",
        "    across the time dimension.\n",
        "\n",
        "    Args:\n",
        "        sequence: A NumPy array or TensorFlow tensor representing the image\n",
        "                  sequence. Expected shape: (timesteps, H, W, C).\n",
        "        title (str): The overall title for the plot.\n",
        "        figsize (tuple, optional): Desired figure size (width, height) in inches.\n",
        "                                   If None, a default size is calculated based\n",
        "                                   on the number of timesteps.\n",
        "        cmap (str): The colormap to use for grayscale images (when C=1).\n",
        "                    Defaults to 'viridis'. Ignored for RGB images (C=3).\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Conversion ---\n",
        "    if hasattr(sequence, 'numpy'): # Check if it's a TF tensor\n",
        "        sequence = sequence.numpy()\n",
        "\n",
        "    if not isinstance(sequence, np.ndarray):\n",
        "        raise TypeError(\"Input 'sequence' must be a NumPy array or TensorFlow tensor.\")\n",
        "\n",
        "    if sequence.ndim != 4:\n",
        "        raise ValueError(f\"Input 'sequence' must be 4-dimensional (timesteps, H, W, C), got shape {sequence.shape}\")\n",
        "\n",
        "    num_timesteps, height, width, channels = sequence.shape\n",
        "\n",
        "    if num_timesteps == 0:\n",
        "        print(\"Sequence has 0 timesteps, nothing to plot.\")\n",
        "        return\n",
        "\n",
        "    # --- Determine Grid Size ---\n",
        "    # Calculate a reasonable grid size (aiming for roughly square or slightly wider)\n",
        "    # Ignore the 12x12 request as it's usually too large for typical timesteps;\n",
        "    # a dynamic grid is more practical.\n",
        "    ncols = int(math.ceil(math.sqrt(num_timesteps)))\n",
        "    # Try to make it slightly wider if not square - adjust ncols calculation if needed\n",
        "    # A simpler approach: use a max number of columns\n",
        "    # ncols = min(num_timesteps, 6) # Example: Limit to 6 columns max\n",
        "    nrows = int(math.ceil(num_timesteps / float(ncols)))\n",
        "\n",
        "    # --- Determine Figure Size ---\n",
        "    if figsize is None:\n",
        "        # Default figsize: scale based on grid size\n",
        "        # Adjust the multipliers (2.5, 2) as needed for your image aspect ratio/preference\n",
        "        figsize = (ncols * 2.5, nrows * 2)\n",
        "\n",
        "    # --- Create Subplots ---\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "\n",
        "    # Flatten axes array for easy iteration, handle cases with single row/col\n",
        "    if isinstance(axes, plt.Axes): # Single subplot case (nrows=1, ncols=1)\n",
        "        axes_flat = [axes]\n",
        "    else:\n",
        "        axes_flat = axes.flat # Flatten the 2D array of axes\n",
        "\n",
        "    # --- Prepare Data for Plotting ---\n",
        "    plot_cmap = None\n",
        "    if channels == 1:\n",
        "        plot_cmap = cmap # Use provided cmap for grayscale\n",
        "        # Remove channel dim for imshow if it exists and is 1\n",
        "        sequence_to_plot = np.squeeze(sequence, axis=-1)\n",
        "    elif channels == 3:\n",
        "         sequence_to_plot = sequence # imshow handles RGB (expects float 0-1 or int 0-255)\n",
        "         # Ensure data range is suitable for imshow if it's float\n",
        "         if sequence_to_plot.dtype == np.float32 or sequence_to_plot.dtype == np.float64:\n",
        "              sequence_to_plot = np.clip(sequence_to_plot, 0, 1)\n",
        "    else:\n",
        "        # Handle other channel numbers? Plot the first channel as grayscale.\n",
        "        print(f\"Warning: Sequence has {channels} channels. Plotting the first channel as grayscale.\")\n",
        "        plot_cmap = cmap\n",
        "        sequence_to_plot = sequence[..., 0] # Take the first channel\n",
        "\n",
        "    # Determine global vmin/vmax for consistent coloring across grayscale timesteps\n",
        "    vmin = np.min(sequence_to_plot)\n",
        "    vmax = np.max(sequence_to_plot)\n",
        "\n",
        "    # --- Plot Each Timestep ---\n",
        "    for t in range(num_timesteps):\n",
        "        ax = axes_flat[t] # Get the current subplot axis\n",
        "        ax.imshow(sequence_to_plot[t], cmap=plot_cmap, vmin=vmin, vmax=vmax)\n",
        "        ax.set_title(f\"t={t}\")\n",
        "        ax.axis('off') # Hide axes ticks and labels\n",
        "\n",
        "    # --- Hide Unused Subplots ---\n",
        "    # If nrows * ncols > num_timesteps, hide the remaining axes\n",
        "    for t in range(num_timesteps, len(axes_flat)):\n",
        "         axes_flat[t].axis('off')\n",
        "\n",
        "    # --- Final Touches ---\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "    # Adjust layout to prevent titles/labels overlapping and make space for suptitle\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CB0BDowFdTBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for element in train_ds_for_model.take(1):\n",
        "  plot_sequence_grid(element[0][\"Input_All_Images\"])"
      ],
      "metadata": {
        "id": "LMKcBXCThvCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model"
      ],
      "metadata": {
        "id": "Lptb4zfQsywN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_scheduler = keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate = INITIAL_LEARNING_RATE,\n",
        "    decay_steps = INDIVIDUALS // BATCH_SIZE * DECAY_EPOCHS,\n",
        "    alpha = DECAY_ALPHA,\n",
        "    warmup_target = LEARNING_RATE,\n",
        "    warmup_steps = INDIVIDUALS // BATCH_SIZE * WARMUP_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "i9e6VPgoi6pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model = create_masked_model()\n",
        "mask_model.compile(\n",
        "    optimizer=keras.optimizers.AdamW(learning_rate=learning_rate_scheduler),\n",
        "    loss=normalized_weighted_euc_dist,\n",
        "    jit_compile=True,\n",
        ")\n",
        "mask_model.summary()"
      ],
      "metadata": {
        "id": "fPT1PjTv4tNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_model.fit(train_ds_for_model.batch(BATCH_SIZE), epochs=TRAIN_EPOCHS, callbacks=[WandbMetricsLogger()])"
      ],
      "metadata": {
        "id": "9QWxiUiBMmue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzxl5IX0ycoR"
      },
      "source": [
        "# Save and export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVk3s8vQRFpl"
      },
      "outputs": [],
      "source": [
        "mask_model.save('full_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9SocMbzC5p"
      },
      "outputs": [],
      "source": [
        "mask_model.save_weights('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZy4iASiRJvi"
      },
      "outputs": [],
      "source": [
        "wandb.save('full_model.keras')\n",
        "wandb.save('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_dataset = prepare_masked_dataset(train_data_rescaled, calibration_points=scaled_cal_points)"
      ],
      "metadata": {
        "id": "Nx0NflgbDi1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = masked_dataset.map(\n",
        "    prepare_model_inputs,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "hCx68NkMCzql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds.element_spec"
      ],
      "metadata": {
        "id": "yGFrhP80C0XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_OUDhetR5LdO"
      },
      "outputs": [],
      "source": [
        "predictions = mask_model.predict(test_ds.batch(1))\n",
        "\n",
        "y_true = np.zeros(predictions.shape, dtype=np.float32)\n",
        "\n",
        "i = 0\n",
        "for e in test_ds.as_numpy_iterator():\n",
        "  y_true[i,:,:] = e[1]\n",
        "  i = i+1\n",
        "# y_true = np.array(y_true).reshape(-1, 144, 2)\n",
        "\n",
        "# # this step is slower than expected. not sure what's going on.\n",
        "# predictions = full_model.predict(test_ds.)\n",
        "\n",
        "batch_losses = np.zeros((predictions.shape[0], predictions.shape[1]))\n",
        "for i in range(len(predictions)):\n",
        "  loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
        "  batch_losses[i,:] = loss\n",
        "\n",
        "# Get mean per subject\n",
        "batch_losses = np.mean(batch_losses, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvKzpmrI9PBO"
      },
      "outputs": [],
      "source": [
        "table = wandb.Table(data=[[s] for s in batch_losses], columns=[\"Loss\"])\n",
        "final_loss_hist = wandb.plot.histogram(table, value=\"Loss\", title=\"Normalized Euclidean Distance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9iK_69wdrZ0"
      },
      "outputs": [],
      "source": [
        "final_loss_mean = np.mean(batch_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk8z0iV4DV2d"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"final_val_loss_hist\": final_loss_hist, \"final_loss_mean\": final_loss_mean})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blgWDH3E5Swd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(batch_losses, bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Batch Losses')\n",
        "plt.xlabel('Loss')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXAdVnFKMjg7"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "zuePQAbwTyeS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}