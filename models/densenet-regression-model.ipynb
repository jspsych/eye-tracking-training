{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LAJHmSmF84"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW4fMCshWBrD"
      },
      "outputs": [],
      "source": [
        "!pip install osfclient --quiet\n",
        "!pip install git+https://github.com/jspsych/eyetracking-utils.git --quiet\n",
        "!pip install keras_cv --quiet\n",
        "!pip install plotnine --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install albumentations --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQRQZ3D5Vwqc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import tensorflow.keras as keras\n",
        "import keras_cv\n",
        "from plotnine import ggplot, geom_point, aes, geom_line, scale_y_reverse, theme_void, scale_color_manual\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "import albumentations as A\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "\n",
        "import et_util.dataset_utils as dataset_utils\n",
        "import et_util.embedding_preprocessing as embed_pre\n",
        "import et_util.model_layers as model_layers\n",
        "from et_util import experiment_utils\n",
        "from et_util.custom_loss import normalized_weighted_euc_dist\n",
        "from et_util.model_analysis import plot_model_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BUzHdPEc-ef"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
        "os.environ['OSF_USERNAME'] = userdata.get('osfusername')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3M9X95jV8du"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9v9bxBcHc0"
      },
      "source": [
        "# Configure W&B experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SPGBdleCfbe"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWWKsFf6p1HJ"
      },
      "outputs": [],
      "source": [
        "# Fixed constants\n",
        "MAX_TARGETS = 144\n",
        "\n",
        "# Config constants\n",
        "EMBEDDING_DIM = 200\n",
        "RIDGE_REGULARIZATION = 0.1\n",
        "TRAIN_EPOCHS = 30\n",
        "MIN_CAL_POINTS = 8\n",
        "MAX_CAL_POINTS = 40\n",
        "DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "LEARNING_RATE = 0.001\n",
        "AUGMENTATION = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkHFv5kHfaWR"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"ridge_regularization\": RIDGE_REGULARIZATION,\n",
        "    \"train_epochs\": TRAIN_EPOCHS,\n",
        "    \"min_cal_points\": MIN_CAL_POINTS,\n",
        "    \"max_cal_points\": MAX_CAL_POINTS,\n",
        "    \"dense_net_stackwise_num_repeats\": DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"augmentation\": AUGMENTATION,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjDFWXNEddE9"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project='eye-tracking-dense-full-data-set-single-eye',\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeHGiFxtvo0"
      },
      "source": [
        "# Download dataset from OSF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tswV6s38WbtO"
      },
      "outputs": [],
      "source": [
        "!osf -p 6b5cd fetch single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgWOhWTt4iD"
      },
      "source": [
        "# Process raw data records into TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4h8QhGvWhZf"
      },
      "outputs": [],
      "source": [
        "!mkdir single_eye_tfrecords\n",
        "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZRcQgPadqtI"
      },
      "outputs": [],
      "source": [
        "def parse(element):\n",
        "    \"\"\"Process function that parses a tfr element in a raw dataset for process_tfr_to_tfds function.\n",
        "    Gets mediapipe landmarks, raw image, image width, image height, subject id, and xy labels.\n",
        "    Use for data generated with make_single_example_landmarks_and_jpg (i.e. data in\n",
        "    jpg_landmarks_tfrecords.tar.gz)\n",
        "\n",
        "    :param element: tfr element in raw dataset\n",
        "    :return: image, label(x,y), landmarks, subject_id\n",
        "    \"\"\"\n",
        "\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    landmarks = content['landmarks']\n",
        "    raw_image = content['eye_img']\n",
        "    width = content['img_width']\n",
        "    height = content['img_height']\n",
        "    depth = 3\n",
        "    label = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n",
        "    landmarks = tf.reshape(landmarks, shape=(478, 3))\n",
        "\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, landmarks, label, subject_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWv57eb5XHVP"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data, test_data = dataset_utils.process_tfr_to_tfds(\n",
        "    'single_eye_tfrecords/',\n",
        "    parse,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, landmarks, coords, z: z\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FULm58Cxu9oa"
      },
      "source": [
        "## Rescale the `x,y` coordinates to be 0-1 instead of 0-100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apcLK9klHLQV"
      },
      "outputs": [],
      "source": [
        "def rescale_coords_map(eyes, mesh, coords, id):\n",
        "  return eyes, mesh, tf.divide(coords, tf.constant([100.])), id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b47CtWKqHg0B"
      },
      "outputs": [],
      "source": [
        "train_data_rescaled = train_data.map(rescale_coords_map)\n",
        "validation_data_rescaled = validation_data.map(rescale_coords_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdQbBaKR6vLN"
      },
      "source": [
        "# Generate dataset that has calibration points, target point, and target output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W60NFqYpDDem"
      },
      "outputs": [],
      "source": [
        "def pad_to_fixed_size(tensor, target_shape, pad_value=0):\n",
        "    \"\"\"Pad tensor to target shape.\"\"\"\n",
        "    current_shape = tf.shape(tensor)\n",
        "    paddings = tf.maximum(0, target_shape - current_shape)\n",
        "    padded = tf.pad(tensor, [[0, paddings[0]], [0, paddings[1]], [0, paddings[2]], [0, paddings[3]]])\n",
        "    # Ensure fixed shape\n",
        "    padded = tf.ensure_shape(padded, [target_shape[0], target_shape[1], target_shape[2], target_shape[3]])\n",
        "    return padded\n",
        "\n",
        "def create_padding_mask(tensor, max_len):\n",
        "    \"\"\"Create a mask indicating which elements are padding.\"\"\"\n",
        "    actual_len = tf.shape(tensor)[0]\n",
        "    mask = tf.range(max_len) < actual_len\n",
        "    return tf.cast(mask, tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCSmeKFBH5YQ"
      },
      "source": [
        "Cols = 5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95\n",
        "\n",
        "Rows = 5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP0_I2DUuEKo"
      },
      "source": [
        "Fixed Points as calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuCld98e7vAZ"
      },
      "outputs": [],
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))\n",
        "\n",
        "def filter_cal_points(image, mesh, coords, id):\n",
        "\n",
        "  return tf.reduce_any(tf.reduce_all(tf.equal(coords, scaled_cal_points), axis=1))\n",
        "\n",
        "def filter_non_cal_points(image, mesh, coords, id):\n",
        "\n",
        "  return tf.reduce_all(tf.reduce_any(tf.not_equal(coords, scaled_cal_points), axis=1))\n",
        "\n",
        "def filter_subjects_missing_cal_points(cal, target):\n",
        "\n",
        "  return tf.equal(tf.shape(cal[0])[0], tf.shape(cal_points)[0])\n",
        "\n",
        "def map_for_calibration_pts(image, mesh, coords, id):\n",
        "  img = tf.reshape(image, (-1, 36, 144, 1))\n",
        "\n",
        "  # No padding here as we'll handle padding in the merged function\n",
        "  return img, coords\n",
        "\n",
        "def map_for_non_calibration_pts(image, mesh, coords, id):\n",
        "  img = tf.reshape(image, (-1, 36, 144, 1))\n",
        "\n",
        "  # No padding here as we'll handle padding in the merged function\n",
        "  return img, coords\n",
        "\n",
        "def map_for_merged(cal, non_cal):\n",
        "  # Unpack the inputs\n",
        "  cal_imgs, cal_coords = cal\n",
        "  target_imgs, target_coords = non_cal\n",
        "\n",
        "  # Get the sizes\n",
        "  cal_size = tf.shape(cal_imgs)[0]\n",
        "  target_size = tf.shape(target_imgs)[0]\n",
        "\n",
        "  # Create masks\n",
        "  cal_mask = create_padding_mask(cal_imgs, MAX_CAL_POINTS)\n",
        "  target_mask = create_padding_mask(target_imgs, MAX_TARGETS)  # Assuming max target points is 144\n",
        "\n",
        "  # Pad images and coordinates\n",
        "  padded_cal_imgs = tf.pad(cal_imgs, [[0, MAX_CAL_POINTS - cal_size], [0, 0], [0, 0], [0, 0]])\n",
        "  padded_cal_coords = tf.pad(cal_coords, [[0, MAX_CAL_POINTS - cal_size], [0, 0]])\n",
        "  padded_target_imgs = tf.pad(target_imgs, [[0, MAX_TARGETS - target_size], [0, 0], [0, 0], [0, 0]])\n",
        "  padded_target_coords = tf.pad(target_coords, [[0, MAX_TARGETS - target_size], [0, 0]])\n",
        "\n",
        "  # Ensure fixed shapes\n",
        "  padded_cal_imgs = tf.ensure_shape(padded_cal_imgs, [MAX_CAL_POINTS, 36, 144, 1])\n",
        "  padded_cal_coords = tf.ensure_shape(padded_cal_coords, [MAX_CAL_POINTS, 2])\n",
        "  padded_target_imgs = tf.ensure_shape(padded_target_imgs, [MAX_TARGETS, 36, 144, 1])\n",
        "  padded_target_coords = tf.ensure_shape(padded_target_coords, [MAX_TARGETS, 2])\n",
        "\n",
        "  # Return with masks\n",
        "  return (padded_cal_imgs, padded_cal_coords, cal_mask, padded_target_imgs, target_mask), padded_target_coords\n",
        "\n",
        "def map_for_merged_with_id(cal, non_cal, id):\n",
        "  # Unpack the inputs\n",
        "  cal_imgs, cal_coords = cal\n",
        "  target_imgs, target_coords = non_cal\n",
        "\n",
        "  # Get the sizes\n",
        "  cal_size = tf.shape(cal_imgs)[0]\n",
        "  target_size = tf.shape(target_imgs)[0]\n",
        "\n",
        "  # Create masks\n",
        "  cal_mask = create_padding_mask(cal_imgs, MAX_CAL_POINTS)\n",
        "  target_mask = create_padding_mask(target_imgs, MAX_TARGETS)  # Assuming max target points is 144\n",
        "\n",
        "  # Pad images and coordinates\n",
        "  padded_cal_imgs = tf.pad(cal_imgs, [[0, MAX_CAL_POINTS - cal_size], [0, 0], [0, 0], [0, 0]])\n",
        "  padded_cal_coords = tf.pad(cal_coords, [[0, MAX_CAL_POINTS - cal_size], [0, 0]])\n",
        "  padded_target_imgs = tf.pad(target_imgs, [[0, MAX_TARGETS - target_size], [0, 0], [0, 0], [0, 0]])\n",
        "  padded_target_coords = tf.pad(target_coords, [[0, MAX_TARGETS - target_size], [0, 0]])\n",
        "\n",
        "  # Ensure fixed shapes\n",
        "  padded_cal_imgs = tf.ensure_shape(padded_cal_imgs, [MAX_CAL_POINTS, 36, 144, 1])\n",
        "  padded_cal_coords = tf.ensure_shape(padded_cal_coords, [MAX_CAL_POINTS, 2])\n",
        "  padded_target_imgs = tf.ensure_shape(padded_target_imgs, [MAX_TARGETS, 36, 144, 1])\n",
        "  padded_target_coords = tf.ensure_shape(padded_target_coords, [MAX_TARGETS, 2])\n",
        "\n",
        "  # Return with masks\n",
        "  return (padded_cal_imgs, padded_cal_coords, cal_mask, padded_target_imgs, target_mask), padded_target_coords, id\n",
        "\n",
        "def reducer_function(k, ds):\n",
        "  ds_random = ds.shuffle(MAX_TARGETS)\n",
        "\n",
        "  n_cal_points = tf.random.uniform(shape=[], minval=MIN_CAL_POINTS, maxval=MAX_CAL_POINTS, dtype=tf.int64)\n",
        "\n",
        "  calibration_points = ds_random.take(n_cal_points).batch(n_cal_points).map(map_for_calibration_pts).repeat()\n",
        "\n",
        "  non_calibration_points = ds.batch(MAX_TARGETS).map(map_for_non_calibration_pts)\n",
        "\n",
        "  merged = tf.data.Dataset.zip(calibration_points, non_calibration_points)\n",
        "  return merged.map(map_for_merged, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# group data by subject id, create datasets with calibration points\n",
        "def reducer_function_fixed_pts(subject_id, ds):\n",
        "  non_cal_points = ds.batch(MAX_TARGETS, drop_remainder=True).map(map_for_non_calibration_pts)\n",
        "\n",
        "  points = ds.filter(filter_cal_points).batch(len(cal_points), drop_remainder=True).map(map_for_calibration_pts).repeat()\n",
        "\n",
        "  merged = tf.data.Dataset.zip(points, non_cal_points)\n",
        "  return merged.map(map_for_merged)\n",
        "\n",
        "def reducer_function_fixed_pts_with_id(subject_id, ds):\n",
        "  non_cal_points = ds.batch(MAX_TARGETS, drop_remainder=True).map(map_for_non_calibration_pts)\n",
        "\n",
        "  points = ds.filter(filter_cal_points).batch(len(cal_points)).map(map_for_calibration_pts).repeat()\n",
        "\n",
        "  merged = tf.data.Dataset.zip(points, non_cal_points)\n",
        "\n",
        "  return merged.map(lambda x, y: map_for_merged_with_id(x, y, subject_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFQ-O5idYhS3"
      },
      "source": [
        "## Create augmentation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Xkt4gQYhS3"
      },
      "outputs": [],
      "source": [
        "augmentation_layers = [\n",
        "    #keras.layers.RandomBrightness(factor=0.1),\n",
        "    #keras.layers.RandomContrast(factor=0.1),\n",
        "    keras.layers.RandomRotation(factor=.02, fill_mode='constant', fill_value=0),\n",
        "    #keras.layers.RandomTranslation(height_factor=0.05, width_factor=0.05)\n",
        "]\n",
        "\n",
        "augmentation_model = keras.Sequential(augmentation_layers)\n",
        "\n",
        "@tf.function\n",
        "def apply_consistent_augmentations(inputs, augmentation_model):\n",
        "    \"\"\"Apply consistent augmentations to calibration and target images.\n",
        "\n",
        "    Args:\n",
        "        inputs: Tuple of (cal_imgs, cal_coords, cal_mask, target_imgs, target_mask)\n",
        "        augmentation_fn: Function created by create_augmentation_fn\n",
        "\n",
        "    Returns:\n",
        "        Tuple with augmented images in the same structure\n",
        "    \"\"\"\n",
        "    cal_imgs, cal_coords, cal_mask, target_imgs, target_mask = inputs\n",
        "\n",
        "    merged_cal_images = tf.transpose(cal_imgs, perm=[3, 1, 2, 0])\n",
        "    merged_target_images = tf.transpose(target_imgs, perm=[3, 1, 2, 0])\n",
        "\n",
        "    merged_cal_imgs_aug = augmentation_model(merged_cal_images)\n",
        "    merged_target_imgs_aug = augmentation_model(merged_target_images)\n",
        "\n",
        "    cal_imgs_aug = tf.transpose(merged_cal_imgs_aug, perm=[3, 1, 2, 0])\n",
        "    target_imgs_aug = tf.transpose(merged_target_imgs_aug, perm=[3, 1, 2, 0])\n",
        "\n",
        "    return cal_imgs_aug, cal_coords, cal_mask, target_imgs_aug, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz_L2j0duJDw"
      },
      "outputs": [],
      "source": [
        "t0 = train_data_rescaled.cache()\n",
        "\n",
        "t1 = t0.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function,\n",
        "    window_size = 200\n",
        ")\n",
        "\n",
        "t2 = t0.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ")\n",
        "\n",
        "# Apply to dataset - ensure consistent shapes\n",
        "train_ds = t1.map(\n",
        "    lambda x, y: (apply_consistent_augmentations(x, augmentation_model), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Update the batching step to use a custom padded_batch function\n",
        "def prepare_batch_for_model(features, labels):\n",
        "    \"\"\"Ensure inputs are correctly formatted for the model\"\"\"\n",
        "    cal_imgs, cal_coords, cal_mask, target_imgs, target_mask = features\n",
        "\n",
        "    # Prepare inputs as a dictionary matching the model's expected inputs\n",
        "    inputs = {\n",
        "        \"Input_Calibration_Eyes\": cal_imgs,\n",
        "        \"Input_Calibration_Points\": cal_coords,\n",
        "        \"Input_Calibration_Mask\": cal_mask,\n",
        "        \"Input_Target_Eyes\": target_imgs,\n",
        "        \"Input_Target_Mask\": target_mask\n",
        "    }\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "# Use the updated version with proper input preparation\n",
        "train_ds_for_model = train_ds.map(prepare_batch_for_model).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.element_spec"
      ],
      "metadata": {
        "id": "RKeCKJ8jliC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORk_yuzTYhS3"
      },
      "source": [
        "## Visualize augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeTXbm3mYhS3"
      },
      "outputs": [],
      "source": [
        "def visualize_augmentations(dataset, num_examples=5):\n",
        "    \"\"\"Visualize augmentations from a tf.data.Dataset.\"\"\"\n",
        "\n",
        "    # Get a sample element from the dataset\n",
        "    for element in dataset.take(1):\n",
        "        # Access the calibration images from the element\n",
        "        images = element[0][0]  # Access all calibration images\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    num_images = images.shape[0]  # Get the number of images in the batch\n",
        "    grid_cols = 5  # Number of columns in the grid\n",
        "    grid_rows = (num_images + grid_cols - 1) // grid_cols  # Calculate number of rows\n",
        "\n",
        "    # Create a figure to display results\n",
        "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(15, 3 * grid_rows))  # Create subplots in a grid\n",
        "\n",
        "    # Display all images\n",
        "    for i in range(num_images):\n",
        "        row = i // grid_cols\n",
        "        col = i % grid_cols\n",
        "        axes[row, col].imshow(images[i].numpy().squeeze(), cmap='gray')  # Display image\n",
        "        axes[row, col].set_title(f\"Image {i + 1}\")  # Set title for each subplot\n",
        "        axes[row, col].axis('off')  # Turn off axis\n",
        "\n",
        "    # Hide empty subplots if any\n",
        "    for i in range(num_images, grid_rows * grid_cols):\n",
        "        row = i // grid_cols\n",
        "        col = i % grid_cols\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Use this to check your augmentations\n",
        "visualize_augmentations(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19yxxx_Ts8H"
      },
      "source": [
        "# Construct Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuZobSgkgbW"
      },
      "source": [
        "## Eye image processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnKs983UsyDj"
      },
      "outputs": [],
      "source": [
        "def create_embedding_model():\n",
        "    input_eyes = keras.layers.Input(shape=(36,144,1))\n",
        "\n",
        "    # Continue with the backbone\n",
        "    backbone = keras_cv.models.DenseNetBackbone(\n",
        "        include_rescaling=False,\n",
        "        input_shape=(36,144,1),\n",
        "        stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS\n",
        "    )\n",
        "    backbone_encoder = backbone(input_eyes)\n",
        "\n",
        "    flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "    eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "    embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "\n",
        "    return embedding_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62VuVwq6lvd"
      },
      "source": [
        "## Regression Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGk0aRz_ebqk"
      },
      "outputs": [],
      "source": [
        "class WeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    def __init__(self, lambda_ridge, embedding_dim, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        self.embedding_dim = embedding_dim  # Required static embedding dimension\n",
        "        super(WeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "        # Create identity matrix constant during initialization\n",
        "        # This ensures it's created only once and is available immediately\n",
        "        self._identity = tf.eye(embedding_dim, dtype=tf.float32)\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, inputs):\n",
        "        unknown_embeddings, calibration_embeddings, calibration_coords, weights, cal_mask, target_mask = inputs\n",
        "\n",
        "        # Explicitly set shapes where possible (assuming batch_size can remain dynamic)\n",
        "        batch_size = tf.shape(calibration_embeddings)[0]\n",
        "\n",
        "        # Force shapes to match the static embedding dimension\n",
        "        calibration_embeddings = tf.ensure_shape(\n",
        "            calibration_embeddings, [None, None, self.embedding_dim]\n",
        "        )\n",
        "        unknown_embeddings = tf.ensure_shape(\n",
        "            unknown_embeddings, [None, None, self.embedding_dim]\n",
        "        )\n",
        "\n",
        "        # Cast all inputs to the same precision\n",
        "        X = tf.cast(calibration_embeddings, tf.float32)  # (batch_size, n_calibration, embedding_dim)\n",
        "        y = tf.cast(calibration_coords, tf.float32)      # (batch_size, n_calibration, 2)\n",
        "        w = tf.cast(weights, tf.float32)                 # (batch_size, n_calibration)\n",
        "        cal_mask = tf.cast(cal_mask, tf.float32)         # (batch_size, n_calibration)\n",
        "        target_mask = tf.cast(target_mask, tf.float32)   # (batch_size, n_target)\n",
        "\n",
        "        # Ensure shapes are consistent\n",
        "        n_calibration = tf.shape(X)[1]\n",
        "        n_target = tf.shape(unknown_embeddings)[1]\n",
        "\n",
        "        # Broadcast the identity matrix to batch size\n",
        "        I = tf.broadcast_to(self._identity, [batch_size, self.embedding_dim, self.embedding_dim])\n",
        "\n",
        "        # Apply mask to weights - ensure this is broadcastable\n",
        "        w = w * cal_mask\n",
        "\n",
        "        # Apply weights to X and y with explicit reshaping\n",
        "        w_sqrt = tf.sqrt(w)\n",
        "        w_sqrt = tf.reshape(w_sqrt, [batch_size, n_calibration, 1])  # Reshape for broadcasting\n",
        "\n",
        "        X_weighted = X * w_sqrt  # (batch_size, n_calibration, embedding_dim)\n",
        "        y_weighted = y * w_sqrt  # (batch_size, n_calibration, 2)\n",
        "\n",
        "        # Matrix multiplication with explicit transpose\n",
        "        X_t = tf.transpose(X_weighted, perm=[0, 2, 1])  # (batch_size, embedding_dim, n_calibration)\n",
        "        X_t_X = tf.matmul(X_t, X_weighted)  # (batch_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Add regularization with stable epsilon\n",
        "        ridge_term = tf.multiply(I, self.lambda_ridge + self.epsilon)\n",
        "        lhs = tf.add(X_t_X, ridge_term)  # (batch_size, embedding_dim, embedding_dim)\n",
        "\n",
        "        # Compute right-hand side\n",
        "        rhs = tf.matmul(X_t, y_weighted)  # (batch_size, embedding_dim, 2)\n",
        "\n",
        "        # Solve linear system\n",
        "        # Add stability check for better XLA compatibility\n",
        "        is_singular = tf.math.reduce_any(tf.math.is_nan(lhs))\n",
        "        kernel = tf.cond(\n",
        "            is_singular,\n",
        "            lambda: tf.zeros([batch_size, self.embedding_dim, 2], dtype=tf.float32),\n",
        "            lambda: tf.linalg.solve(lhs, rhs)\n",
        "        )  # Shape: [batch_size, embedding_dim, 2]\n",
        "\n",
        "        # Apply regression to unknown point with explicit masking\n",
        "        unknown_embeddings = tf.cast(unknown_embeddings, tf.float32)  # Shape: [batch_size, n_target, embedding_dim]\n",
        "        target_mask_expanded = tf.reshape(target_mask, [batch_size, n_target, 1])\n",
        "        unknown_embeddings_masked = unknown_embeddings * target_mask_expanded\n",
        "\n",
        "        # Matrix multiplication with proper dimensions:\n",
        "        # unknown_embeddings_masked: [batch_size, n_target, embedding_dim]\n",
        "        # kernel:                    [batch_size, embedding_dim, 2]\n",
        "        # Result:                    [batch_size, n_target, 2]\n",
        "        output = tf.matmul(unknown_embeddings_masked, kernel)  # Shape: [batch_size, n_target, 2]\n",
        "\n",
        "        # Apply final mask\n",
        "        output = output * target_mask_expanded\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        unknown_embeddings_shape, _, _, _, _, _ = input_shapes\n",
        "        return (unknown_embeddings_shape[0], unknown_embeddings_shape[1], 2)\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        # This method is called once before the first call() to build the layer\n",
        "        # We already handle initialization in __init__, but this is a good place to validate shapes\n",
        "        unknown_shape, cal_shape, coords_shape, weights_shape, cal_mask_shape, target_mask_shape = input_shapes\n",
        "\n",
        "        if unknown_shape[-1] != self.embedding_dim:\n",
        "            raise ValueError(f\"Unknown embeddings dimension {unknown_shape[-1]} doesn't match specified embedding_dim {self.embedding_dim}\")\n",
        "\n",
        "        if cal_shape[-1] != self.embedding_dim:\n",
        "            raise ValueError(f\"Calibration embeddings dimension {cal_shape[-1]} doesn't match specified embedding_dim {self.embedding_dim}\")\n",
        "\n",
        "        # We don't need to create any weights in build() since we don't have trainable parameters\n",
        "        super(WeightedRidgeRegressionLayer, self).build(input_shapes)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(WeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"lambda_ridge\": self.lambda_ridge,\n",
        "            \"epsilon\": self.epsilon,\n",
        "            \"embedding_dim\": self.embedding_dim\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JOA5GNqTUW-"
      },
      "outputs": [],
      "source": [
        "def create_full_model():\n",
        "\n",
        "  embedding_model = create_embedding_model()\n",
        "\n",
        "  input_calibration_eyes = keras.layers.Input(shape=(MAX_CAL_POINTS, 36, 144, 1), name=\"Input_Calibration_Eyes\")\n",
        "  input_calibration_points = keras.layers.Input(shape=(MAX_CAL_POINTS, 2), name=\"Input_Calibration_Points\")\n",
        "  input_calibration_mask = keras.layers.Input(shape=(MAX_CAL_POINTS,), name=\"Input_Calibration_Mask\")\n",
        "\n",
        "  input_target_eyes = keras.layers.Input(shape=(144, 36, 144, 1), name=\"Input_Target_Eyes\")\n",
        "  input_target_mask = keras.layers.Input(shape=(144,), name=\"Input_Target_Mask\")\n",
        "\n",
        "  # Apply the embedding model using masking\n",
        "  target_embedding = keras.layers.TimeDistributed(embedding_model)(input_target_eyes)\n",
        "  calibration_embeddings = keras.layers.TimeDistributed(embedding_model)(input_calibration_eyes)\n",
        "\n",
        "  calibration_weights = keras.layers.Dense(1, activation=\"sigmoid\", name=\"Calibration_Weights\")(calibration_embeddings)\n",
        "  calibration_weights_reshaped = keras.layers.Reshape((-1,), name=\"Calibration_Weights_Reshaped\")(calibration_weights)\n",
        "\n",
        "  ridge = WeightedRidgeRegressionLayer(RIDGE_REGULARIZATION, embedding_dim=EMBEDDING_DIM)([\n",
        "    target_embedding,\n",
        "    calibration_embeddings,\n",
        "    input_calibration_points,\n",
        "    calibration_weights_reshaped,\n",
        "    input_calibration_mask,\n",
        "    input_target_mask\n",
        "  ])\n",
        "\n",
        "  full_model = keras.Model(inputs=[\n",
        "    input_calibration_eyes,\n",
        "    input_calibration_points,\n",
        "    input_calibration_mask,\n",
        "    input_target_eyes,\n",
        "    input_target_mask\n",
        "  ], outputs=ridge, name=\"FullEyePredictionModel\")\n",
        "\n",
        "  return full_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtm4EUSZwXDC"
      },
      "source": [
        "## Full trainable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xAFsV9x41BZ"
      },
      "outputs": [],
      "source": [
        "full_model = create_full_model()\n",
        "\n",
        "full_model.summary()\n",
        "\n",
        "def masked_normalized_weighted_euc_dist(y_true, y_pred):\n",
        "    \"\"\"Custom loss function that calculates a weighted Euclidean distance between two sets of points,\n",
        "    respecting masks for padded sequences.\n",
        "\n",
        "    Weighting multiplies the x-coordinate of each input by 1.778 (derived from the 16:9 aspect ratio of most laptops)\n",
        "    to match the scale of the y-axis. For interpretability, the distances are normalized to the diagonal\n",
        "    such that the maximum distance between two points (corner to corner) is 100.\n",
        "\n",
        "    :param y_true (tensor): A tensor of shape (batch_size, seq_len, 2) containing ground-truth x- and y- coordinates\n",
        "    :param y_pred (tensor): A tensor of shape (batch_size, seq_len, 2) containing predicted x- and y- coordinates\n",
        "\n",
        "    :returns: A tensor of shape (1,) with the weighted and normalized euclidean distances between valid points.\n",
        "    \"\"\"\n",
        "    # Extract shape information\n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    seq_len = tf.shape(y_true)[1]\n",
        "\n",
        "    # Create a mask based on zero-padding in y_pred\n",
        "    # Assuming padded values are exactly 0 in both dimensions\n",
        "    mask = tf.reduce_any(tf.not_equal(y_pred, 0), axis=-1)\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Weighting treats y-axis as unit-scale and creates a rectangle that's 177.8x100 units.\n",
        "    x_weight = tf.constant([1.778, 1.0], dtype=tf.float32)\n",
        "\n",
        "    # Multiply x-coordinate by 16/9 = 1.778\n",
        "    y_true_weighted = tf.math.multiply(x_weight, y_true)\n",
        "    y_pred_weighted = tf.math.multiply(x_weight, y_pred)\n",
        "\n",
        "    # Calculate Euclidean distance with weighted coordinates\n",
        "    squared_diff = tf.math.square(y_pred_weighted - y_true_weighted)\n",
        "    squared_dist = tf.math.reduce_sum(squared_diff, axis=-1)\n",
        "    dist = tf.math.sqrt(squared_dist)\n",
        "\n",
        "    # Euclidean Distance from [0,0] to [177.8, 100] = 203.992\n",
        "    norm_scale = tf.constant(203.992, dtype=tf.float32)\n",
        "\n",
        "    # Normalizes loss values to the diagonal-- makes loss easier to interpret\n",
        "    normalized_dist = (dist / norm_scale) * 100\n",
        "\n",
        "    # Apply mask to only include valid points in the loss calculation\n",
        "    masked_dist = normalized_dist * mask\n",
        "\n",
        "    # Sum the losses and divide by the number of non-padded elements\n",
        "    num_valid = tf.maximum(tf.reduce_sum(mask), 1.0)  # Avoid division by zero\n",
        "    loss = tf.reduce_sum(masked_dist) / num_valid\n",
        "\n",
        "    return loss\n",
        "\n",
        "full_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=masked_normalized_weighted_euc_dist,\n",
        "    metrics=[normalized_weighted_euc_dist],  # Keep the original for comparison if needed\n",
        "    jit_compile=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FnXA0JueNHm"
      },
      "outputs": [],
      "source": [
        "def lr_schedule(epoch):\n",
        "  if epoch < 15:\n",
        "    return LEARNING_RATE\n",
        "  else:\n",
        "    return LEARNING_RATE * 0.1\n",
        "\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzxl5IX0ycoR"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t15bFcCV4_yw"
      },
      "outputs": [],
      "source": [
        "train_history = full_model.fit(\n",
        "    train_ds_for_model.batch(64),\n",
        "    epochs=TRAIN_EPOCHS,\n",
        "    callbacks=[WandbMetricsLogger(), lr_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVk3s8vQRFpl"
      },
      "outputs": [],
      "source": [
        "full_model.save('full_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9SocMbzC5p"
      },
      "outputs": [],
      "source": [
        "full_model.save_weights('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZy4iASiRJvi"
      },
      "outputs": [],
      "source": [
        "wandb.save('full_model.keras')\n",
        "wandb.save('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_OUDhetR5LdO"
      },
      "outputs": [],
      "source": [
        "# Update the prediction code\n",
        "t2_processed = t2.map(\n",
        "    lambda x, y, id: (prepare_batch_for_model((x[0], x[1], x[2], x[3], x[4]), y)[0], y, id)\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Initialize a list to store the batch losses\n",
        "batch_losses = []\n",
        "subject_ids = []\n",
        "y_true = []\n",
        "\n",
        "for e in t2_processed.batch(1).as_numpy_iterator():\n",
        "    inputs = e[0]\n",
        "    y_true.append(e[1])\n",
        "    subject_ids.append(e[2][0])\n",
        "\n",
        "y_true = np.array(y_true).reshape(-1, 144, 2)\n",
        "\n",
        "# this step is slower than expected. not sure what's going on.\n",
        "predictions = full_model.predict(t2_processed.batch(1))\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
        "  batch_losses.append(loss)\n",
        "\n",
        "batch_losses = np.array(batch_losses)\n",
        "\n",
        "# Get mean per subject\n",
        "batch_losses = np.mean(batch_losses, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FdB2Anc88Qz"
      },
      "outputs": [],
      "source": [
        "final_loss_table = wandb.Table(data=[[s, l] for (s, l) in zip(subject_ids, batch_losses)], columns=[\"subject\", \"scores\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvKzpmrI9PBO"
      },
      "outputs": [],
      "source": [
        "final_loss_hist = wandb.plot.histogram(final_loss_table, \"scores\", title=\"Normalized Euclidean Distance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9iK_69wdrZ0"
      },
      "outputs": [],
      "source": [
        "final_loss_mean = np.mean(batch_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk8z0iV4DV2d"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"final_val_loss_table\": final_loss_table, \"final_val_loss_hist\": final_loss_hist, \"final_loss_mean\": final_loss_mean})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blgWDH3E5Swd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(batch_losses, bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Batch Losses')\n",
        "plt.xlabel('Loss')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXAdVnFKMjg7"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}