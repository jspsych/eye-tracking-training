{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LAJHmSmF84"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW4fMCshWBrD",
        "outputId": "25b9653d-da21-4adf-e8b4-253b69071a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for et_util (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install osfclient --quiet\n",
        "!pip install git+https://github.com/jspsych/eyetracking-utils.git --quiet\n",
        "!pip install keras_cv --quiet\n",
        "!pip install plotnine --quiet\n",
        "!pip install wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SQRQZ3D5Vwqc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import tensorflow.keras as keras\n",
        "import keras_cv\n",
        "from plotnine import ggplot, geom_point, aes, geom_line, scale_y_reverse, theme_void, scale_color_manual\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "import et_util.dataset_utils as dataset_utils\n",
        "import et_util.embedding_preprocessing as embed_pre\n",
        "import et_util.model_layers as model_layers\n",
        "from et_util import experiment_utils\n",
        "from et_util.custom_loss import normalized_weighted_euc_dist\n",
        "from et_util.model_analysis import plot_model_performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8BUzHdPEc-ef"
      },
      "outputs": [],
      "source": [
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "os.environ['OSF_TOKEN'] = userdata.get('osftoken')\n",
        "os.environ['OSF_USERNAME'] = userdata.get('osfusername')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZEsXfla-cg8G",
        "outputId": "f729c460-9188-4564-8af0-83657d689f7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cvv9AY4YAA43Lb17ua0cbUr0G4OAv5mvFmegj7RK8e3k4vJZt5H7AihU6nvJxVQdkdxeW3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "userdata.get('osftoken')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k3M9X95jV8du"
      },
      "outputs": [],
      "source": [
        "keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er9v9bxBcHc0"
      },
      "source": [
        "# Configure W&B experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SPGBdleCfbe",
        "outputId": "e55f66c4-357c-4a8d-d890-40c78dbf14fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoshdeleeuw\u001b[0m (\u001b[33meye-tracking\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rWWKsFf6p1HJ"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 200\n",
        "RIDGE_REGULARIZATION = 0.1\n",
        "TRAIN_EPOCHS = 30\n",
        "MIN_CAL_POINTS = 8\n",
        "MAX_CAL_POINTS = 40\n",
        "DENSE_NET_STACKWISE_NUM_REPEATS = [4,4,4]\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BkHFv5kHfaWR"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"embedding_dim\": EMBEDDING_DIM,\n",
        "    \"ridge_regularization\": RIDGE_REGULARIZATION,\n",
        "    \"train_epochs\": TRAIN_EPOCHS,\n",
        "    \"min_cal_points\": MIN_CAL_POINTS,\n",
        "    \"max_cal_points\": MAX_CAL_POINTS,\n",
        "    \"dense_net_stackwise_num_repeats\": DENSE_NET_STACKWISE_NUM_REPEATS,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"augmentation\": True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "wjDFWXNEddE9",
        "outputId": "2f834de9-b775-4784-9759-b0a28d753fa3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250314_151744-sam6als8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye/runs/sam6als8' target=\"_blank\">berry-brownie-7</a></strong> to <a href='https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye' target=\"_blank\">https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye/runs/sam6als8' target=\"_blank\">https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye/runs/sam6als8</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    project='eye-tracking-dense-full-data-set-single-eye',\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeHGiFxtvo0"
      },
      "source": [
        "# Download dataset from OSF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tswV6s38WbtO",
        "outputId": "1469acc4-f0d8-40e4-8b13-a2582ac3b005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 2.50G/2.50G [02:11<00:00, 19.0Mbytes/s]\n"
          ]
        }
      ],
      "source": [
        "!osf -p 6b5cd fetch single_eye_tfrecords.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFgWOhWTt4iD"
      },
      "source": [
        "# Process raw data records into TF Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G4h8QhGvWhZf"
      },
      "outputs": [],
      "source": [
        "!mkdir single_eye_tfrecords\n",
        "!tar -xf single_eye_tfrecords.tar.gz -C single_eye_tfrecords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dZRcQgPadqtI"
      },
      "outputs": [],
      "source": [
        "def parse(element):\n",
        "    \"\"\"Process function that parses a tfr element in a raw dataset for process_tfr_to_tfds function.\n",
        "    Gets mediapipe landmarks, raw image, image width, image height, subject id, and xy labels.\n",
        "    Use for data generated with make_single_example_landmarks_and_jpg (i.e. data in\n",
        "    jpg_landmarks_tfrecords.tar.gz)\n",
        "\n",
        "    :param element: tfr element in raw dataset\n",
        "    :return: image, label(x,y), landmarks, subject_id\n",
        "    \"\"\"\n",
        "\n",
        "    data_structure = {\n",
        "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
        "        'img_width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'img_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'eye_img': tf.io.FixedLenFeature([], tf.string),\n",
        "        'subject_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(element, data_structure)\n",
        "\n",
        "    landmarks = content['landmarks']\n",
        "    raw_image = content['eye_img']\n",
        "    width = content['img_width']\n",
        "    height = content['img_height']\n",
        "    depth = 3\n",
        "    label = [content['x'], content['y']]\n",
        "    subject_id = content['subject_id']\n",
        "\n",
        "    landmarks = tf.io.parse_tensor(landmarks, out_type=tf.float32)\n",
        "    landmarks = tf.reshape(landmarks, shape=(478, 3))\n",
        "\n",
        "    image = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n",
        "\n",
        "    return image, landmarks, label, subject_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JWv57eb5XHVP"
      },
      "outputs": [],
      "source": [
        "train_data, validation_data, test_data = dataset_utils.process_tfr_to_tfds(\n",
        "    'single_eye_tfrecords/',\n",
        "    parse,\n",
        "    train_split=1.0,\n",
        "    val_split=0.0,\n",
        "    test_split=0.0,\n",
        "    random_seed=12604,\n",
        "    group_function=lambda img, landmarks, coords, z: z\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FULm58Cxu9oa"
      },
      "source": [
        "## Rescale the `x,y` coordinates to be 0-1 instead of 0-100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "apcLK9klHLQV"
      },
      "outputs": [],
      "source": [
        "def rescale_coords_map(eyes, mesh, coords, id):\n",
        "  return eyes, mesh, tf.divide(coords, tf.constant([100.])), id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b47CtWKqHg0B"
      },
      "outputs": [],
      "source": [
        "train_data_rescaled = train_data.map(rescale_coords_map)\n",
        "validation_data_rescaled = validation_data.map(rescale_coords_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdQbBaKR6vLN"
      },
      "source": [
        "# Generate dataset that has calibration points, target point, and target output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCSmeKFBH5YQ"
      },
      "source": [
        "Cols = 5, 11, 17, 23, 29, 35, 41, 47, 53, 59, 65, 71, 77, 83, 89, 95\n",
        "\n",
        "Rows = 5, 16.25, 27.5, 38.75, 50, 61.25, 72.5, 83.75, 95  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP0_I2DUuEKo"
      },
      "source": [
        "Fixed Points as calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wuCld98e7vAZ"
      },
      "outputs": [],
      "source": [
        "cal_points = tf.constant([\n",
        "    [5, 5],\n",
        "    [5, 27.5],\n",
        "    [5, 50],\n",
        "    [5, 72.5],\n",
        "    [5, 95],\n",
        "    [35, 5],\n",
        "    [35, 27.5],\n",
        "    [35, 50],\n",
        "    [35, 72.5],\n",
        "    [35, 95],\n",
        "    [65, 5],\n",
        "    [65, 27.5],\n",
        "    [65, 50],\n",
        "    [65, 72.5],\n",
        "    [65, 95],\n",
        "    [95, 5],\n",
        "    [95, 27.5],\n",
        "    [95, 50],\n",
        "    [95, 72.5],\n",
        "    [95, 95],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "scaled_cal_points = tf.divide(cal_points, tf.constant([100.]))\n",
        "\n",
        "def filter_cal_points(image, mesh, coords, id):\n",
        "\n",
        "  return tf.reduce_any(tf.reduce_all(tf.equal(coords, scaled_cal_points), axis=1))\n",
        "\n",
        "def filter_non_cal_points(image, mesh, coords, id):\n",
        "\n",
        "  return tf.reduce_all(tf.reduce_any(tf.not_equal(coords, scaled_cal_points), axis=1))\n",
        "\n",
        "def filter_subjects_missing_cal_points(cal, target):\n",
        "\n",
        "  return tf.equal(tf.shape(cal[0])[0], tf.shape(cal_points)[0])\n",
        "\n",
        "def map_for_calibration_pts(image, mesh, coords, id):\n",
        "\n",
        "  img = tf.reshape(image, (-1, 36, 144, 1))\n",
        "\n",
        "  return img, coords\n",
        "\n",
        "def map_for_non_calibration_pts(image, mesh, coords, id):\n",
        "\n",
        "  img = tf.reshape(image, (-1, 36, 144, 1))\n",
        "\n",
        "  return img, coords\n",
        "\n",
        "def map_for_merged(cal, non_cal):\n",
        "  target_input, target_output = non_cal\n",
        "\n",
        "  return (*cal, target_input), target_output\n",
        "\n",
        "def map_for_merged_with_id(cal, non_cal, id):\n",
        "  target_input, target_output = non_cal\n",
        "\n",
        "  return (*cal, target_input), target_output, id\n",
        "\n",
        "def reducer_function(k, ds):\n",
        "\n",
        "  ds_random = ds.shuffle(144)\n",
        "\n",
        "  n_cal_points = tf.random.uniform(shape=[], minval=MIN_CAL_POINTS, maxval=MAX_CAL_POINTS, dtype=tf.int64)\n",
        "\n",
        "  calibration_points = ds_random.take(n_cal_points).batch(n_cal_points).map(map_for_calibration_pts).repeat()\n",
        "\n",
        "  non_calibration_points = ds.batch(144).map(map_for_non_calibration_pts)\n",
        "\n",
        "  merged = tf.data.Dataset.zip(calibration_points, non_calibration_points)\n",
        "  #return non_calibration_points\n",
        "  #return merged\n",
        "  return merged.map(map_for_merged)\n",
        "\n",
        "# group data by subject id, create datasets with calibration points\n",
        "def reducer_function_fixed_pts(subject_id, ds):\n",
        "\n",
        "  non_cal_points = ds.batch(144, drop_remainder=True).map(map_for_non_calibration_pts)\n",
        "\n",
        "  points = ds.filter(filter_cal_points).batch(len(cal_points), drop_remainder=True).map(map_for_calibration_pts).repeat()\n",
        "\n",
        "  merged = tf.data.Dataset.zip(points, non_cal_points)\n",
        "  #return merged\n",
        "  return merged.map(map_for_merged)\n",
        "\n",
        "def reducer_function_fixed_pts_with_id(subject_id, ds):\n",
        "\n",
        "  non_cal_points = ds.batch(144, drop_remainder=True).map(map_for_non_calibration_pts)\n",
        "\n",
        "  points = ds.filter(filter_cal_points).batch(len(cal_points)).map(map_for_calibration_pts).repeat()\n",
        "\n",
        "  merged = tf.data.Dataset.zip(points, non_cal_points)\n",
        "\n",
        "  #return merged\n",
        "  return merged.map(lambda x, y: map_for_merged_with_id(x, y, subject_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sz_L2j0duJDw"
      },
      "outputs": [],
      "source": [
        "t0 = train_data_rescaled.cache()\n",
        "\n",
        "t1 = t0.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function,\n",
        "    window_size = 200\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "t2 = t0.group_by_window(\n",
        "    key_func = lambda img, m, c, z: z,\n",
        "    reduce_func = reducer_function_fixed_pts_with_id,\n",
        "    window_size = 200\n",
        ")\n",
        "\n",
        "# v = validation_data_rescaled.group_by_window(\n",
        "#     key_func = lambda img, m, c, z: z,\n",
        "#     reduce_func = reducer_function_fixed_pts_with_id,\n",
        "#     window_size = 200\n",
        "# ).cache().prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19yxxx_Ts8H"
      },
      "source": [
        "# Construct Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxuZobSgkgbW"
      },
      "source": [
        "## Eye image processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GnKs983UsyDj"
      },
      "outputs": [],
      "source": [
        "def create_embedding_model():\n",
        "    input_eyes = keras.layers.Input(shape=(36,144,1))\n",
        "\n",
        "    # Apply rescaling first (0-255 to 0-1)\n",
        "    x = keras.layers.Rescaling(1.0/255)(input_eyes)\n",
        "\n",
        "    # Create an augmentation model that only applies during training\n",
        "    augmentation_model = keras.Sequential([\n",
        "        # Small brightness variations\n",
        "        keras.layers.RandomBrightness(factor=0.1),\n",
        "\n",
        "        # Adjust contrast slightly\n",
        "        keras.layers.RandomContrast(factor=0.1),\n",
        "\n",
        "        # Small random translations\n",
        "        keras.layers.RandomTranslation(\n",
        "            height_factor=0.05,\n",
        "            width_factor=0.05,\n",
        "            fill_mode='constant'\n",
        "        ),\n",
        "\n",
        "        # Apply slight rotation (-3 to 3 degrees)\n",
        "        keras.layers.RandomRotation(\n",
        "            factor=0.05,  # About 3 degrees max\n",
        "            fill_mode='constant'\n",
        "        ),\n",
        "\n",
        "        # Apply slight zoom (0.95x to 1.05x)\n",
        "        keras.layers.RandomZoom(\n",
        "            height_factor=(-0.05, 0.05),\n",
        "            width_factor=(-0.05, 0.05),\n",
        "            fill_mode='constant'\n",
        "        ),\n",
        "\n",
        "        # Add slight Gaussian noise\n",
        "        keras.layers.GaussianNoise(0.01)\n",
        "    ], name=\"eye_augmentation\")\n",
        "\n",
        "    # Apply augmentation only during training\n",
        "    x = augmentation_model(x)\n",
        "\n",
        "    # Continue with the backbone\n",
        "    backbone = keras_cv.models.DenseNetBackbone(\n",
        "        include_rescaling=False,\n",
        "        input_shape=(36,144,1),\n",
        "        stackwise_num_repeats=DENSE_NET_STACKWISE_NUM_REPEATS\n",
        "    )\n",
        "    backbone_encoder = backbone(x)\n",
        "\n",
        "    flatten_compress = keras.layers.Flatten()(backbone_encoder)\n",
        "    eye_embedding = keras.layers.Dense(units=EMBEDDING_DIM, activation=\"tanh\")(flatten_compress)\n",
        "\n",
        "    embedding_model = keras.Model(inputs=input_eyes, outputs=eye_embedding, name=\"Eye_Image_Embedding\")\n",
        "\n",
        "    return embedding_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62VuVwq6lvd"
      },
      "source": [
        "## Regression Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jGk0aRz_ebqk"
      },
      "outputs": [],
      "source": [
        "class WeightedRidgeRegressionLayer(keras.layers.Layer):\n",
        "    def __init__(self, lambda_ridge, epsilon=1e-6, **kwargs):\n",
        "        self.lambda_ridge = lambda_ridge\n",
        "        self.epsilon = epsilon\n",
        "        super(WeightedRidgeRegressionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, inputs):\n",
        "        unknown_embeddings, calibration_embeddings, calibration_coords, weights = inputs\n",
        "\n",
        "        X = tf.cast(calibration_embeddings, tf.float32)  # (batch_size, n_calibration, 20)\n",
        "        y = tf.cast(calibration_coords, tf.float32)  # (batch_size, n_calibration, 2)\n",
        "        w = tf.cast(weights, tf.float32)  # (batch_size, n_calibration)\n",
        "\n",
        "        # Add assertions\n",
        "        # tf.debugging.assert_rank(X, 3, \"calibration_embeddings should be rank 3\")\n",
        "        # tf.debugging.assert_rank(y, 3, \"calibration_coords should be rank 3\")\n",
        "        # tf.debugging.assert_rank(w, 2, \"weights should be rank 2\")\n",
        "        # tf.debugging.assert_non_negative(w, \"weights should be non-negative\")\n",
        "        # tf.debugging.assert_less_equal(w, 1.0, \"weights should be less than or equal to 1\")\n",
        "\n",
        "        # Compute weighted ridge regression coefficients\n",
        "        n, p = X.shape[-2:]  # n_calibration, 20\n",
        "        I = tf.eye(p, dtype=X.dtype)\n",
        "\n",
        "        # Apply weights to X and y\n",
        "        w_sqrt = tf.sqrt(w)\n",
        "        X_weighted = X * tf.expand_dims(w_sqrt, axis=-1)  # (batch_size, n_calibration, 20)\n",
        "        y_weighted = y * tf.expand_dims(w_sqrt, axis=-1)  # (batch_size, n_calibration, 2)\n",
        "\n",
        "        X_t = tf.transpose(X_weighted, perm=[0, 2, 1])  # (batch_size, 20, n_calibration)\n",
        "        X_t_X = tf.linalg.matmul(X_t, X_weighted)  # (batch_size, 20, 20)\n",
        "        lhs = X_t_X + (self.lambda_ridge + self.epsilon) * I  # (batch_size, 20, 20)\n",
        "        rhs = tf.linalg.matmul(X_t, y_weighted)  # (batch_size, 20, 2)\n",
        "\n",
        "        kernel = tf.linalg.solve(lhs, rhs)  # (batch_size, 20, 2)\n",
        "\n",
        "        # Apply regression to unknown point\n",
        "        unknown_embeddings = tf.cast(unknown_embeddings, tf.float32)\n",
        "        output = tf.linalg.matmul(unknown_embeddings, tf.transpose(kernel, perm=[0, 2, 1]), adjoint_b=True)  # (batch_size, ?, 2)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        unknown_embeddings_shape, _, _, _ = input_shapes\n",
        "        return unknown_embeddings_shape[:-1] + (2,)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(WeightedRidgeRegressionLayer, self).get_config()\n",
        "        config.update({\"lambda_ridge\": self.lambda_ridge, \"epsilon\": self.epsilon})\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6JOA5GNqTUW-"
      },
      "outputs": [],
      "source": [
        "def create_full_model():\n",
        "\n",
        "  embedding_model = create_embedding_model()\n",
        "\n",
        "  input_calibration_eyes = keras.layers.Input(shape=(None,36,144,1), name=\"Input_Calibration_Eyes\")\n",
        "  input_calibration_points = keras.layers.Input(shape=(None,2,), name=\"Input_Calibration_Points\")\n",
        "\n",
        "  input_target_eyes = keras.layers.Input(shape=(None,36,144,1), name=\"Input_Target_Eyes\")\n",
        "\n",
        "  target_embedding = keras.layers.TimeDistributed(embedding_model)(input_target_eyes)\n",
        "  calibration_embeddings = keras.layers.TimeDistributed(embedding_model)(input_calibration_eyes)\n",
        "\n",
        "  calibration_weights = keras.layers.Dense(1, activation=\"sigmoid\", name=\"Calibration_Weights\")(calibration_embeddings)\n",
        "  calibration_weights_reshaped = keras.layers.Reshape((-1,), name=\"Calibration_Weights_Reshaped\")(calibration_weights)\n",
        "\n",
        "  ridge = WeightedRidgeRegressionLayer(RIDGE_REGULARIZATION)([target_embedding, calibration_embeddings, input_calibration_points, calibration_weights_reshaped])\n",
        "\n",
        "  full_model = keras.Model(inputs=[\n",
        "    input_calibration_eyes,\n",
        "    input_calibration_points,\n",
        "    input_target_eyes\n",
        "  ], outputs=ridge, name=\"FullEyePredictionModel\")\n",
        "\n",
        "  return full_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtm4EUSZwXDC"
      },
      "source": [
        "## Full trainable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "7xAFsV9x41BZ",
        "outputId": "9f322694-343b-46dd-814d-a1c9776d1ff8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"FullEyePredictionModel\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"FullEyePredictionModel\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_Calibration_Eyes    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m144\u001b[0m,  │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │ \u001b[38;5;34m1\u001b[0m)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cast_4 (\u001b[38;5;33mCast\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m144\u001b[0m,  │              \u001b[38;5;34m0\u001b[0m │ Input_Calibration_Eye… │\n",
              "│                           │ \u001b[38;5;34m1\u001b[0m)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_Target_Eyes         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m144\u001b[0m,  │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │ \u001b[38;5;34m1\u001b[0m)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │      \u001b[38;5;34m1,581,896\u001b[0m │ cast_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cast_3 (\u001b[38;5;33mCast\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m144\u001b[0m,  │              \u001b[38;5;34m0\u001b[0m │ Input_Target_Eyes[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │ \u001b[38;5;34m1\u001b[0m)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_Calibration_Points  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Calibration_Weights       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │            \u001b[38;5;34m201\u001b[0m │ time_distributed_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)      │      \u001b[38;5;34m1,581,896\u001b[0m │ cast_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cast_5 (\u001b[38;5;33mCast\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ Input_Calibration_Poi… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Calibration_Weights_Resh… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ Calibration_Weights[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mReshape\u001b[0m)                 │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ weighted_ridge_regressio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ time_distributed[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mWeightedRidgeRegression…\u001b[0m │                        │                │ time_distributed_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                           │                        │                │ cast_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ Calibration_Weights_R… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ Input_Calibration_Eyes    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>,  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cast_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>,  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Input_Calibration_Eye… │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_Target_Eyes         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>,  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,581,896</span> │ cast_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cast_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>,  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Input_Target_Eyes[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Input_Calibration_Points  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Calibration_Weights       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">201</span> │ time_distributed_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,581,896</span> │ cast_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cast_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Input_Calibration_Poi… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ Calibration_Weights_Resh… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Calibration_Weights[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                 │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ weighted_ridge_regressio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ time_distributed[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">WeightedRidgeRegression…</span> │                        │                │ time_distributed_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                           │                        │                │ cast_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ Calibration_Weights_R… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,582,097\u001b[0m (6.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,582,097</span> (6.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,574,257\u001b[0m (6.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,257</span> (6.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,840\u001b[0m (30.62 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,840</span> (30.62 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "full_model = create_full_model()\n",
        "\n",
        "full_model.summary()\n",
        "\n",
        "full_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=normalized_weighted_euc_dist,\n",
        "    metrics=[normalized_weighted_euc_dist],\n",
        "    jit_compile = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5FnXA0JueNHm"
      },
      "outputs": [],
      "source": [
        "def lr_schedule(epoch):\n",
        "  if epoch < 15:\n",
        "    return LEARNING_RATE\n",
        "  else:\n",
        "    return LEARNING_RATE * 0.1\n",
        "\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzxl5IX0ycoR"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "t15bFcCV4_yw",
        "outputId": "681eedeb-ee61-41cf-afbf-29eb89e11403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1726/1726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 369ms/step - loss: 0.2726 - normalized_weighted_euc_dist: 0.2726 - learning_rate: 0.0010\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1726/1726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 270ms/step - loss: 0.2631 - normalized_weighted_euc_dist: 0.2631 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m 529/1726\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:05\u001b[0m 255ms/step - loss: 0.2633 - normalized_weighted_euc_dist: 0.2633"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-368403f4c39d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWandbMetricsLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#full_model.fit(t1.batch(1), epochs=TRAIN_EPOCHS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_history = full_model.fit(t1.batch(1), epochs=TRAIN_EPOCHS, callbacks=[WandbMetricsLogger(), lr_scheduler])\n",
        "#full_model.fit(t1.batch(1), epochs=TRAIN_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVk3s8vQRFpl"
      },
      "outputs": [],
      "source": [
        "full_model.save('full_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9SocMbzC5p"
      },
      "outputs": [],
      "source": [
        "full_model.save_weights('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZy4iASiRJvi",
        "outputId": "06b52841-04d9-4799-b797-ba1d2fc63367"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/wandb/run-20241221_135150-2rlxy5dx/files/full_model.weights.h5']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.save('full_model.keras')\n",
        "wandb.save('full_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_OUDhetR5LdO",
        "outputId": "51a6252a-2ea1-4d47-806f-08a20a6f3d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1564/1564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 421ms/step\n"
          ]
        }
      ],
      "source": [
        "# Initialize a list to store the batch losses\n",
        "batch_losses = []\n",
        "subject_ids = []\n",
        "y_true = []\n",
        "\n",
        "for e in t2.batch(1).as_numpy_iterator():\n",
        "\n",
        "    y_true.append(e[1])\n",
        "    subject_ids.append(e[2][0])\n",
        "\n",
        "y_true = np.array(y_true).reshape(-1, 144, 2)\n",
        "\n",
        "# this step is slower than expected. not sure what's going on.\n",
        "predictions = full_model.predict(t2.batch(1))\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  loss = normalized_weighted_euc_dist(y_true[i], predictions[i]).numpy()\n",
        "  batch_losses.append(loss)\n",
        "\n",
        "batch_losses = np.array(batch_losses)\n",
        "\n",
        "# Get mean per subject\n",
        "batch_losses = np.mean(batch_losses, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FdB2Anc88Qz"
      },
      "outputs": [],
      "source": [
        "final_loss_table = wandb.Table(data=[[s, l] for (s, l) in zip(subject_ids, batch_losses)], columns=[\"subject\", \"scores\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvKzpmrI9PBO"
      },
      "outputs": [],
      "source": [
        "final_loss_hist = wandb.plot.histogram(final_loss_table, \"scores\", title=\"Normalized Euclidean Distance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9iK_69wdrZ0"
      },
      "outputs": [],
      "source": [
        "final_loss_mean = np.mean(batch_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk8z0iV4DV2d"
      },
      "outputs": [],
      "source": [
        "wandb.log({\"final_val_loss_table\": final_loss_table, \"final_val_loss_hist\": final_loss_hist, \"final_loss_mean\": final_loss_mean})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blgWDH3E5Swd",
        "outputId": "f99d4dea-8b6d-431b-e653-fec7a49caa73"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(batch_losses, bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Batch Losses')\n",
        "plt.xlabel('Loss')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "xXAdVnFKMjg7",
        "outputId": "f270a33d-3e8d-4824-b4a6-55936f6c1a33"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>███████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▆▅▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/normalized_weighted_euc_dist</td><td>█▆▅▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>final_loss_mean</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/epoch</td><td>29</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.04859</td></tr><tr><td>epoch/normalized_weighted_euc_dist</td><td>0.04859</td></tr><tr><td>final_loss_mean</td><td>0.05181</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devout-meadow-6</strong> at: <a href='https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye/runs/2rlxy5dx' target=\"_blank\">https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye/runs/2rlxy5dx</a><br> View project at: <a href='https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye' target=\"_blank\">https://wandb.ai/eye-tracking/eye-tracking-dense-full-data-set-single-eye</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241221_135150-2rlxy5dx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}